Log file created at: 2018/04/28 17:38:39
Running on machine: HZQSY-PC
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0428 17:38:39.734721  7412 caffe.cpp:219] Using GPUs 0
I0428 17:38:39.782724  7412 caffe.cpp:224] GPU 0: GeForce GTX 980
I0428 17:38:40.091742  7412 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 300000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 500
snapshot_prefix: "D:/_test/train/caffenet_train/"
solver_mode: GPU
device_id: 0
net: "D:/_test/train/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 40000
stepvalue: 80000
I0428 17:38:40.093742  7412 solver.cpp:87] Creating training net from net file: D:/_test/train/train_val.prototxt
I0428 17:38:40.094743  7412 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0428 17:38:40.094743  7412 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 17:38:40.095742  7412 net.cpp:51] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "D:/_test/train/img_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "conv2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv5"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv5"
  type: "Scale"
  bottom: "conv5"
  top: "conv5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "conv5"
  top: "conv6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv6"
  type: "BatchNorm"
  bottom: "conv6"
  top: "conv6"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv6"
  type: "Scale"
  bottom: "conv6"
  top: "conv6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "conv6"
  top: "conv6"
}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "conv6"
  top: "conv7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv7"
  type: "BatchNorm"
  bottom: "conv7"
  top: "conv7"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv7"
  type: "Scale"
  bottom: "conv7"
  top: "conv7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "conv7"
  top: "conv8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv8"
  type: "BatchNorm"
  bottom: "conv8"
  top: "conv8"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv8"
  type: "Scale"
  bottom: "conv8"
  top: "conv8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "conv8"
  top: "conv9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv9"
  type: "BatchNorm"
  bottom: "conv9"
  top: "conv9"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv9"
  type: "Scale"
  bottom: "conv9"
  top: "conv9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "conv9"
  top: "conv10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv10"
  type: "BatchNorm"
  bottom: "conv10"
  top: "conv10"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv10"
  type: "Scale"
  bottom: "conv10"
  top: "conv10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv10"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "conv13"
  type: "Convolution"
  bottom: "conv12"
  top: "conv13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv13"
  type: "BatchNorm"
  bottom: "conv13"
  top: "conv13"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv13"
  type: "Scale"
  bottom: "conv13"
  top: "conv13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu13"
  type: "ReLU"
  bottom: "conv13"
  top: "conv13"
}
layer {
  name: "conv14"
  type: "Convolution"
  bottom: "conv13"
  top: "conv14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv14"
  type: "BatchNorm"
  bottom: "conv14"
  top: "conv14"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv14"
  type: "Scale"
  bottom: "conv14"
  top: "conv14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu14"
  type: "ReLU"
  bottom: "conv14"
  top: "conv14"
}
layer {
  name: "conv15"
  type: "Convolution"
  bottom: "conv14"
  top: "conv15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv15"
  type: "BatchNorm"
  bottom: "conv15"
  top: "conv15"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv15"
  type: "Scale"
  bottom: "conv15"
  top: "conv15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu15"
  type: "ReLU"
  bottom: "conv15"
  top: "conv15"
}
layer {
  name: "conv16"
  type: "Convolution"
  bottom: "conv15"
  top: "conv16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv16"
  type: "BatchNorm"
  bottom: "conv16"
  top: "conv16"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv16"
  type: "Scale"
  bottom: "conv16"
  top: "conv16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu16"
  type: "ReLU"
  bottom: "conv16"
  top: "conv16"
}
layer {
  name: "conv17"
  type: "Convolution"
  bottom: "conv16"
  top: "conv17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv17"
  type: "BatchNorm"
  bottom: "conv17"
  top: "conv17"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv17"
  type: "Scale"
  bottom: "conv17"
  top: "conv17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu17"
  type: "ReLU"
  bottom: "conv17"
  top: "conv17"
}
layer {
  name: "conv18"
  type: "Convolution"
  bottom: "conv17"
  top: "conv18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv17"
  type: "BatchNorm"
  bottom: "conv18"
  top: "conv18"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv18"
  type: "Scale"
  bottom: "conv18"
  top: "conv18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu18"
  type: "ReLU"
  bottom: "conv18"
  top: "conv18"
}
layer {
  name: "conv19"
  type: "Convolution"
  bottom: "conv18"
  top: "conv19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv19"
  type: "BatchNorm"
  bottom: "conv19"
  top: "conv19"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv19"
  type: "Scale"
  bottom: "conv19"
  top: "conv19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu19"
  type: "ReLU"
  bottom: "conv19"
  top: "conv19"
}
layer {
  name: "conv20"
  type: "Convolution"
  bottom: "conv19"
  top: "conv20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv20"
  type: "BatchNorm"
  bottom: "conv20"
  top: "conv20"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv20"
  type: "Scale"
  bottom: "conv20"
  top: "conv20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu20"
  type: "ReLU"
  bottom: "conv20"
  top: "conv20"
}
layer {
  name: "pool21"
  type: "Pooling"
  bottom: "conv20"
  top: "pool21"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc22"
  type: "InnerProduct"
  bottom: "pool21"
  top: "fc22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc22"
  bottom: "label"
  top: "loss"
}
I0428 17:38:40.137744  7412 layer_factory.hpp:77] Creating layer data
I0428 17:38:40.138744  7412 db_lmdb.cpp:40] Opened lmdb D:/_test/train/img_train_lmdb
I0428 17:38:40.138744  7412 net.cpp:84] Creating Layer data
I0428 17:38:40.138744  7412 net.cpp:380] data -> data
I0428 17:38:40.138744  7412 net.cpp:380] data -> label
I0428 17:38:40.139744  7412 data_layer.cpp:45] output data size: 100,3,32,32
I0428 17:38:40.143745  7412 net.cpp:122] Setting up data
I0428 17:38:40.143745  7412 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0428 17:38:40.144745  7412 net.cpp:129] Top shape: 100 (100)
I0428 17:38:40.144745  7412 net.cpp:137] Memory required for data: 1229200
I0428 17:38:40.144745  7412 layer_factory.hpp:77] Creating layer conv1
I0428 17:38:40.144745  7412 net.cpp:84] Creating Layer conv1
I0428 17:38:40.144745  7412 net.cpp:406] conv1 <- data
I0428 17:38:40.144745  7412 net.cpp:380] conv1 -> conv1
I0428 17:38:40.145745  7412 net.cpp:122] Setting up conv1
I0428 17:38:40.145745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.145745  7412 net.cpp:137] Memory required for data: 7782800
I0428 17:38:40.145745  7412 layer_factory.hpp:77] Creating layer bn_conv1
I0428 17:38:40.146745  7412 net.cpp:84] Creating Layer bn_conv1
I0428 17:38:40.146745  7412 net.cpp:406] bn_conv1 <- conv1
I0428 17:38:40.146745  7412 net.cpp:367] bn_conv1 -> conv1 (in-place)
I0428 17:38:40.146745  7412 net.cpp:122] Setting up bn_conv1
I0428 17:38:40.146745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.146745  7412 net.cpp:137] Memory required for data: 14336400
I0428 17:38:40.146745  7412 layer_factory.hpp:77] Creating layer scale_conv1
I0428 17:38:40.147745  7412 net.cpp:84] Creating Layer scale_conv1
I0428 17:38:40.147745  7412 net.cpp:406] scale_conv1 <- conv1
I0428 17:38:40.147745  7412 net.cpp:367] scale_conv1 -> conv1 (in-place)
I0428 17:38:40.147745  7412 layer_factory.hpp:77] Creating layer scale_conv1
I0428 17:38:40.147745  7412 net.cpp:122] Setting up scale_conv1
I0428 17:38:40.147745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.147745  7412 net.cpp:137] Memory required for data: 20890000
I0428 17:38:40.147745  7412 layer_factory.hpp:77] Creating layer relu1
I0428 17:38:40.147745  7412 net.cpp:84] Creating Layer relu1
I0428 17:38:40.147745  7412 net.cpp:406] relu1 <- conv1
I0428 17:38:40.148746  7412 net.cpp:367] relu1 -> conv1 (in-place)
I0428 17:38:40.148746  7412 net.cpp:122] Setting up relu1
I0428 17:38:40.148746  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.148746  7412 net.cpp:137] Memory required for data: 27443600
I0428 17:38:40.148746  7412 layer_factory.hpp:77] Creating layer conv2
I0428 17:38:40.148746  7412 net.cpp:84] Creating Layer conv2
I0428 17:38:40.148746  7412 net.cpp:406] conv2 <- conv1
I0428 17:38:40.148746  7412 net.cpp:380] conv2 -> conv2
I0428 17:38:40.149745  7412 net.cpp:122] Setting up conv2
I0428 17:38:40.149745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.149745  7412 net.cpp:137] Memory required for data: 33997200
I0428 17:38:40.149745  7412 layer_factory.hpp:77] Creating layer bn_conv2
I0428 17:38:40.149745  7412 net.cpp:84] Creating Layer bn_conv2
I0428 17:38:40.149745  7412 net.cpp:406] bn_conv2 <- conv2
I0428 17:38:40.149745  7412 net.cpp:367] bn_conv2 -> conv2 (in-place)
I0428 17:38:40.149745  7412 net.cpp:122] Setting up bn_conv2
I0428 17:38:40.150745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.150745  7412 net.cpp:137] Memory required for data: 40550800
I0428 17:38:40.150745  7412 layer_factory.hpp:77] Creating layer scale_conv2
I0428 17:38:40.150745  7412 net.cpp:84] Creating Layer scale_conv2
I0428 17:38:40.150745  7412 net.cpp:406] scale_conv2 <- conv2
I0428 17:38:40.150745  7412 net.cpp:367] scale_conv2 -> conv2 (in-place)
I0428 17:38:40.150745  7412 layer_factory.hpp:77] Creating layer scale_conv2
I0428 17:38:40.150745  7412 net.cpp:122] Setting up scale_conv2
I0428 17:38:40.150745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.151746  7412 net.cpp:137] Memory required for data: 47104400
I0428 17:38:40.151746  7412 layer_factory.hpp:77] Creating layer relu2
I0428 17:38:40.151746  7412 net.cpp:84] Creating Layer relu2
I0428 17:38:40.151746  7412 net.cpp:406] relu2 <- conv2
I0428 17:38:40.151746  7412 net.cpp:367] relu2 -> conv2 (in-place)
I0428 17:38:40.151746  7412 net.cpp:122] Setting up relu2
I0428 17:38:40.151746  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.151746  7412 net.cpp:137] Memory required for data: 53658000
I0428 17:38:40.151746  7412 layer_factory.hpp:77] Creating layer conv3
I0428 17:38:40.151746  7412 net.cpp:84] Creating Layer conv3
I0428 17:38:40.151746  7412 net.cpp:406] conv3 <- conv2
I0428 17:38:40.152745  7412 net.cpp:380] conv3 -> conv3
I0428 17:38:40.152745  7412 net.cpp:122] Setting up conv3
I0428 17:38:40.152745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.152745  7412 net.cpp:137] Memory required for data: 60211600
I0428 17:38:40.152745  7412 layer_factory.hpp:77] Creating layer bn_conv3
I0428 17:38:40.152745  7412 net.cpp:84] Creating Layer bn_conv3
I0428 17:38:40.152745  7412 net.cpp:406] bn_conv3 <- conv3
I0428 17:38:40.152745  7412 net.cpp:367] bn_conv3 -> conv3 (in-place)
I0428 17:38:40.153745  7412 net.cpp:122] Setting up bn_conv3
I0428 17:38:40.153745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.153745  7412 net.cpp:137] Memory required for data: 66765200
I0428 17:38:40.153745  7412 layer_factory.hpp:77] Creating layer scale_conv3
I0428 17:38:40.153745  7412 net.cpp:84] Creating Layer scale_conv3
I0428 17:38:40.153745  7412 net.cpp:406] scale_conv3 <- conv3
I0428 17:38:40.153745  7412 net.cpp:367] scale_conv3 -> conv3 (in-place)
I0428 17:38:40.153745  7412 layer_factory.hpp:77] Creating layer scale_conv3
I0428 17:38:40.153745  7412 net.cpp:122] Setting up scale_conv3
I0428 17:38:40.153745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.154745  7412 net.cpp:137] Memory required for data: 73318800
I0428 17:38:40.154745  7412 layer_factory.hpp:77] Creating layer relu3
I0428 17:38:40.154745  7412 net.cpp:84] Creating Layer relu3
I0428 17:38:40.154745  7412 net.cpp:406] relu3 <- conv3
I0428 17:38:40.154745  7412 net.cpp:367] relu3 -> conv3 (in-place)
I0428 17:38:40.154745  7412 net.cpp:122] Setting up relu3
I0428 17:38:40.154745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.154745  7412 net.cpp:137] Memory required for data: 79872400
I0428 17:38:40.154745  7412 layer_factory.hpp:77] Creating layer conv4
I0428 17:38:40.154745  7412 net.cpp:84] Creating Layer conv4
I0428 17:38:40.154745  7412 net.cpp:406] conv4 <- conv3
I0428 17:38:40.155745  7412 net.cpp:380] conv4 -> conv4
I0428 17:38:40.155745  7412 net.cpp:122] Setting up conv4
I0428 17:38:40.155745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.155745  7412 net.cpp:137] Memory required for data: 86426000
I0428 17:38:40.155745  7412 layer_factory.hpp:77] Creating layer bn_conv4
I0428 17:38:40.155745  7412 net.cpp:84] Creating Layer bn_conv4
I0428 17:38:40.155745  7412 net.cpp:406] bn_conv4 <- conv4
I0428 17:38:40.156745  7412 net.cpp:367] bn_conv4 -> conv4 (in-place)
I0428 17:38:40.156745  7412 net.cpp:122] Setting up bn_conv4
I0428 17:38:40.156745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.156745  7412 net.cpp:137] Memory required for data: 92979600
I0428 17:38:40.156745  7412 layer_factory.hpp:77] Creating layer scale_conv4
I0428 17:38:40.156745  7412 net.cpp:84] Creating Layer scale_conv4
I0428 17:38:40.156745  7412 net.cpp:406] scale_conv4 <- conv4
I0428 17:38:40.156745  7412 net.cpp:367] scale_conv4 -> conv4 (in-place)
I0428 17:38:40.156745  7412 layer_factory.hpp:77] Creating layer scale_conv4
I0428 17:38:40.157745  7412 net.cpp:122] Setting up scale_conv4
I0428 17:38:40.157745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.157745  7412 net.cpp:137] Memory required for data: 99533200
I0428 17:38:40.157745  7412 layer_factory.hpp:77] Creating layer relu4
I0428 17:38:40.157745  7412 net.cpp:84] Creating Layer relu4
I0428 17:38:40.157745  7412 net.cpp:406] relu4 <- conv4
I0428 17:38:40.157745  7412 net.cpp:367] relu4 -> conv4 (in-place)
I0428 17:38:40.157745  7412 net.cpp:122] Setting up relu4
I0428 17:38:40.157745  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.157745  7412 net.cpp:137] Memory required for data: 106086800
I0428 17:38:40.158746  7412 layer_factory.hpp:77] Creating layer conv5
I0428 17:38:40.158746  7412 net.cpp:84] Creating Layer conv5
I0428 17:38:40.158746  7412 net.cpp:406] conv5 <- conv4
I0428 17:38:40.158746  7412 net.cpp:380] conv5 -> conv5
I0428 17:38:40.158746  7412 net.cpp:122] Setting up conv5
I0428 17:38:40.158746  7412 net.cpp:129] Top shape: 100 16 16 16 (409600)
I0428 17:38:40.158746  7412 net.cpp:137] Memory required for data: 107725200
I0428 17:38:40.158746  7412 layer_factory.hpp:77] Creating layer bn_conv5
I0428 17:38:40.158746  7412 net.cpp:84] Creating Layer bn_conv5
I0428 17:38:40.159745  7412 net.cpp:406] bn_conv5 <- conv5
I0428 17:38:40.159745  7412 net.cpp:367] bn_conv5 -> conv5 (in-place)
I0428 17:38:40.159745  7412 net.cpp:122] Setting up bn_conv5
I0428 17:38:40.159745  7412 net.cpp:129] Top shape: 100 16 16 16 (409600)
I0428 17:38:40.159745  7412 net.cpp:137] Memory required for data: 109363600
I0428 17:38:40.159745  7412 layer_factory.hpp:77] Creating layer scale_conv5
I0428 17:38:40.160745  7412 net.cpp:84] Creating Layer scale_conv5
I0428 17:38:40.160745  7412 net.cpp:406] scale_conv5 <- conv5
I0428 17:38:40.160745  7412 net.cpp:367] scale_conv5 -> conv5 (in-place)
I0428 17:38:40.160745  7412 layer_factory.hpp:77] Creating layer scale_conv5
I0428 17:38:40.160745  7412 net.cpp:122] Setting up scale_conv5
I0428 17:38:40.160745  7412 net.cpp:129] Top shape: 100 16 16 16 (409600)
I0428 17:38:40.160745  7412 net.cpp:137] Memory required for data: 111002000
I0428 17:38:40.160745  7412 layer_factory.hpp:77] Creating layer relu5
I0428 17:38:40.160745  7412 net.cpp:84] Creating Layer relu5
I0428 17:38:40.161746  7412 net.cpp:406] relu5 <- conv5
I0428 17:38:40.161746  7412 net.cpp:367] relu5 -> conv5 (in-place)
I0428 17:38:40.161746  7412 net.cpp:122] Setting up relu5
I0428 17:38:40.161746  7412 net.cpp:129] Top shape: 100 16 16 16 (409600)
I0428 17:38:40.161746  7412 net.cpp:137] Memory required for data: 112640400
I0428 17:38:40.161746  7412 layer_factory.hpp:77] Creating layer conv6
I0428 17:38:40.161746  7412 net.cpp:84] Creating Layer conv6
I0428 17:38:40.161746  7412 net.cpp:406] conv6 <- conv5
I0428 17:38:40.161746  7412 net.cpp:380] conv6 -> conv6
I0428 17:38:40.162746  7412 net.cpp:122] Setting up conv6
I0428 17:38:40.162746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.162746  7412 net.cpp:137] Memory required for data: 115917200
I0428 17:38:40.162746  7412 layer_factory.hpp:77] Creating layer bn_conv6
I0428 17:38:40.162746  7412 net.cpp:84] Creating Layer bn_conv6
I0428 17:38:40.163746  7412 net.cpp:406] bn_conv6 <- conv6
I0428 17:38:40.163746  7412 net.cpp:367] bn_conv6 -> conv6 (in-place)
I0428 17:38:40.163746  7412 net.cpp:122] Setting up bn_conv6
I0428 17:38:40.163746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.163746  7412 net.cpp:137] Memory required for data: 119194000
I0428 17:38:40.163746  7412 layer_factory.hpp:77] Creating layer scale_conv6
I0428 17:38:40.163746  7412 net.cpp:84] Creating Layer scale_conv6
I0428 17:38:40.163746  7412 net.cpp:406] scale_conv6 <- conv6
I0428 17:38:40.163746  7412 net.cpp:367] scale_conv6 -> conv6 (in-place)
I0428 17:38:40.163746  7412 layer_factory.hpp:77] Creating layer scale_conv6
I0428 17:38:40.164746  7412 net.cpp:122] Setting up scale_conv6
I0428 17:38:40.164746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.164746  7412 net.cpp:137] Memory required for data: 122470800
I0428 17:38:40.164746  7412 layer_factory.hpp:77] Creating layer relu6
I0428 17:38:40.164746  7412 net.cpp:84] Creating Layer relu6
I0428 17:38:40.164746  7412 net.cpp:406] relu6 <- conv6
I0428 17:38:40.164746  7412 net.cpp:367] relu6 -> conv6 (in-place)
I0428 17:38:40.164746  7412 net.cpp:122] Setting up relu6
I0428 17:38:40.164746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.164746  7412 net.cpp:137] Memory required for data: 125747600
I0428 17:38:40.165746  7412 layer_factory.hpp:77] Creating layer conv7
I0428 17:38:40.165746  7412 net.cpp:84] Creating Layer conv7
I0428 17:38:40.165746  7412 net.cpp:406] conv7 <- conv6
I0428 17:38:40.165746  7412 net.cpp:380] conv7 -> conv7
I0428 17:38:40.165746  7412 net.cpp:122] Setting up conv7
I0428 17:38:40.165746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.165746  7412 net.cpp:137] Memory required for data: 129024400
I0428 17:38:40.165746  7412 layer_factory.hpp:77] Creating layer bn_conv7
I0428 17:38:40.165746  7412 net.cpp:84] Creating Layer bn_conv7
I0428 17:38:40.166746  7412 net.cpp:406] bn_conv7 <- conv7
I0428 17:38:40.166746  7412 net.cpp:367] bn_conv7 -> conv7 (in-place)
I0428 17:38:40.166746  7412 net.cpp:122] Setting up bn_conv7
I0428 17:38:40.166746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.166746  7412 net.cpp:137] Memory required for data: 132301200
I0428 17:38:40.166746  7412 layer_factory.hpp:77] Creating layer scale_conv7
I0428 17:38:40.166746  7412 net.cpp:84] Creating Layer scale_conv7
I0428 17:38:40.166746  7412 net.cpp:406] scale_conv7 <- conv7
I0428 17:38:40.166746  7412 net.cpp:367] scale_conv7 -> conv7 (in-place)
I0428 17:38:40.167747  7412 layer_factory.hpp:77] Creating layer scale_conv7
I0428 17:38:40.167747  7412 net.cpp:122] Setting up scale_conv7
I0428 17:38:40.167747  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.167747  7412 net.cpp:137] Memory required for data: 135578000
I0428 17:38:40.167747  7412 layer_factory.hpp:77] Creating layer relu7
I0428 17:38:40.167747  7412 net.cpp:84] Creating Layer relu7
I0428 17:38:40.167747  7412 net.cpp:406] relu7 <- conv7
I0428 17:38:40.167747  7412 net.cpp:367] relu7 -> conv7 (in-place)
I0428 17:38:40.167747  7412 net.cpp:122] Setting up relu7
I0428 17:38:40.167747  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.168746  7412 net.cpp:137] Memory required for data: 138854800
I0428 17:38:40.168746  7412 layer_factory.hpp:77] Creating layer conv8
I0428 17:38:40.168746  7412 net.cpp:84] Creating Layer conv8
I0428 17:38:40.168746  7412 net.cpp:406] conv8 <- conv7
I0428 17:38:40.168746  7412 net.cpp:380] conv8 -> conv8
I0428 17:38:40.168746  7412 net.cpp:122] Setting up conv8
I0428 17:38:40.168746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.168746  7412 net.cpp:137] Memory required for data: 142131600
I0428 17:38:40.168746  7412 layer_factory.hpp:77] Creating layer bn_conv8
I0428 17:38:40.169746  7412 net.cpp:84] Creating Layer bn_conv8
I0428 17:38:40.169746  7412 net.cpp:406] bn_conv8 <- conv8
I0428 17:38:40.169746  7412 net.cpp:367] bn_conv8 -> conv8 (in-place)
I0428 17:38:40.169746  7412 net.cpp:122] Setting up bn_conv8
I0428 17:38:40.169746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.169746  7412 net.cpp:137] Memory required for data: 145408400
I0428 17:38:40.169746  7412 layer_factory.hpp:77] Creating layer scale_conv8
I0428 17:38:40.169746  7412 net.cpp:84] Creating Layer scale_conv8
I0428 17:38:40.169746  7412 net.cpp:406] scale_conv8 <- conv8
I0428 17:38:40.170747  7412 net.cpp:367] scale_conv8 -> conv8 (in-place)
I0428 17:38:40.170747  7412 layer_factory.hpp:77] Creating layer scale_conv8
I0428 17:38:40.170747  7412 net.cpp:122] Setting up scale_conv8
I0428 17:38:40.170747  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.170747  7412 net.cpp:137] Memory required for data: 148685200
I0428 17:38:40.170747  7412 layer_factory.hpp:77] Creating layer relu8
I0428 17:38:40.170747  7412 net.cpp:84] Creating Layer relu8
I0428 17:38:40.170747  7412 net.cpp:406] relu8 <- conv8
I0428 17:38:40.170747  7412 net.cpp:367] relu8 -> conv8 (in-place)
I0428 17:38:40.171746  7412 net.cpp:122] Setting up relu8
I0428 17:38:40.171746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.171746  7412 net.cpp:137] Memory required for data: 151962000
I0428 17:38:40.171746  7412 layer_factory.hpp:77] Creating layer conv9
I0428 17:38:40.171746  7412 net.cpp:84] Creating Layer conv9
I0428 17:38:40.171746  7412 net.cpp:406] conv9 <- conv8
I0428 17:38:40.171746  7412 net.cpp:380] conv9 -> conv9
I0428 17:38:40.171746  7412 net.cpp:122] Setting up conv9
I0428 17:38:40.172746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.172746  7412 net.cpp:137] Memory required for data: 155238800
I0428 17:38:40.172746  7412 layer_factory.hpp:77] Creating layer bn_conv9
I0428 17:38:40.172746  7412 net.cpp:84] Creating Layer bn_conv9
I0428 17:38:40.172746  7412 net.cpp:406] bn_conv9 <- conv9
I0428 17:38:40.172746  7412 net.cpp:367] bn_conv9 -> conv9 (in-place)
I0428 17:38:40.172746  7412 net.cpp:122] Setting up bn_conv9
I0428 17:38:40.172746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.172746  7412 net.cpp:137] Memory required for data: 158515600
I0428 17:38:40.173746  7412 layer_factory.hpp:77] Creating layer scale_conv9
I0428 17:38:40.173746  7412 net.cpp:84] Creating Layer scale_conv9
I0428 17:38:40.173746  7412 net.cpp:406] scale_conv9 <- conv9
I0428 17:38:40.173746  7412 net.cpp:367] scale_conv9 -> conv9 (in-place)
I0428 17:38:40.173746  7412 layer_factory.hpp:77] Creating layer scale_conv9
I0428 17:38:40.173746  7412 net.cpp:122] Setting up scale_conv9
I0428 17:38:40.173746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.173746  7412 net.cpp:137] Memory required for data: 161792400
I0428 17:38:40.173746  7412 layer_factory.hpp:77] Creating layer relu9
I0428 17:38:40.173746  7412 net.cpp:84] Creating Layer relu9
I0428 17:38:40.174746  7412 net.cpp:406] relu9 <- conv9
I0428 17:38:40.174746  7412 net.cpp:367] relu9 -> conv9 (in-place)
I0428 17:38:40.174746  7412 net.cpp:122] Setting up relu9
I0428 17:38:40.174746  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.174746  7412 net.cpp:137] Memory required for data: 165069200
I0428 17:38:40.174746  7412 layer_factory.hpp:77] Creating layer conv10
I0428 17:38:40.174746  7412 net.cpp:84] Creating Layer conv10
I0428 17:38:40.174746  7412 net.cpp:406] conv10 <- conv9
I0428 17:38:40.174746  7412 net.cpp:380] conv10 -> conv10
I0428 17:38:40.175746  7412 net.cpp:122] Setting up conv10
I0428 17:38:40.175746  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.175746  7412 net.cpp:137] Memory required for data: 165888400
I0428 17:38:40.175746  7412 layer_factory.hpp:77] Creating layer bn_conv10
I0428 17:38:40.175746  7412 net.cpp:84] Creating Layer bn_conv10
I0428 17:38:40.175746  7412 net.cpp:406] bn_conv10 <- conv10
I0428 17:38:40.175746  7412 net.cpp:367] bn_conv10 -> conv10 (in-place)
I0428 17:38:40.175746  7412 net.cpp:122] Setting up bn_conv10
I0428 17:38:40.176746  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.176746  7412 net.cpp:137] Memory required for data: 166707600
I0428 17:38:40.176746  7412 layer_factory.hpp:77] Creating layer scale_conv10
I0428 17:38:40.176746  7412 net.cpp:84] Creating Layer scale_conv10
I0428 17:38:40.176746  7412 net.cpp:406] scale_conv10 <- conv10
I0428 17:38:40.176746  7412 net.cpp:367] scale_conv10 -> conv10 (in-place)
I0428 17:38:40.176746  7412 layer_factory.hpp:77] Creating layer scale_conv10
I0428 17:38:40.176746  7412 net.cpp:122] Setting up scale_conv10
I0428 17:38:40.176746  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.176746  7412 net.cpp:137] Memory required for data: 167526800
I0428 17:38:40.177747  7412 layer_factory.hpp:77] Creating layer relu10
I0428 17:38:40.177747  7412 net.cpp:84] Creating Layer relu10
I0428 17:38:40.177747  7412 net.cpp:406] relu10 <- conv10
I0428 17:38:40.177747  7412 net.cpp:367] relu10 -> conv10 (in-place)
I0428 17:38:40.177747  7412 net.cpp:122] Setting up relu10
I0428 17:38:40.177747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.177747  7412 net.cpp:137] Memory required for data: 168346000
I0428 17:38:40.177747  7412 layer_factory.hpp:77] Creating layer conv11
I0428 17:38:40.177747  7412 net.cpp:84] Creating Layer conv11
I0428 17:38:40.177747  7412 net.cpp:406] conv11 <- conv10
I0428 17:38:40.178747  7412 net.cpp:380] conv11 -> conv11
I0428 17:38:40.178747  7412 net.cpp:122] Setting up conv11
I0428 17:38:40.178747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.178747  7412 net.cpp:137] Memory required for data: 169165200
I0428 17:38:40.178747  7412 layer_factory.hpp:77] Creating layer bn_conv11
I0428 17:38:40.178747  7412 net.cpp:84] Creating Layer bn_conv11
I0428 17:38:40.178747  7412 net.cpp:406] bn_conv11 <- conv11
I0428 17:38:40.178747  7412 net.cpp:367] bn_conv11 -> conv11 (in-place)
I0428 17:38:40.179747  7412 net.cpp:122] Setting up bn_conv11
I0428 17:38:40.179747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.179747  7412 net.cpp:137] Memory required for data: 169984400
I0428 17:38:40.179747  7412 layer_factory.hpp:77] Creating layer scale_conv11
I0428 17:38:40.179747  7412 net.cpp:84] Creating Layer scale_conv11
I0428 17:38:40.179747  7412 net.cpp:406] scale_conv11 <- conv11
I0428 17:38:40.179747  7412 net.cpp:367] scale_conv11 -> conv11 (in-place)
I0428 17:38:40.179747  7412 layer_factory.hpp:77] Creating layer scale_conv11
I0428 17:38:40.179747  7412 net.cpp:122] Setting up scale_conv11
I0428 17:38:40.179747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.180747  7412 net.cpp:137] Memory required for data: 170803600
I0428 17:38:40.180747  7412 layer_factory.hpp:77] Creating layer relu11
I0428 17:38:40.180747  7412 net.cpp:84] Creating Layer relu11
I0428 17:38:40.180747  7412 net.cpp:406] relu11 <- conv11
I0428 17:38:40.180747  7412 net.cpp:367] relu11 -> conv11 (in-place)
I0428 17:38:40.180747  7412 net.cpp:122] Setting up relu11
I0428 17:38:40.180747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.180747  7412 net.cpp:137] Memory required for data: 171622800
I0428 17:38:40.180747  7412 layer_factory.hpp:77] Creating layer conv12
I0428 17:38:40.180747  7412 net.cpp:84] Creating Layer conv12
I0428 17:38:40.181747  7412 net.cpp:406] conv12 <- conv11
I0428 17:38:40.181747  7412 net.cpp:380] conv12 -> conv12
I0428 17:38:40.181747  7412 net.cpp:122] Setting up conv12
I0428 17:38:40.181747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.181747  7412 net.cpp:137] Memory required for data: 172442000
I0428 17:38:40.181747  7412 layer_factory.hpp:77] Creating layer bn_conv12
I0428 17:38:40.181747  7412 net.cpp:84] Creating Layer bn_conv12
I0428 17:38:40.181747  7412 net.cpp:406] bn_conv12 <- conv12
I0428 17:38:40.181747  7412 net.cpp:367] bn_conv12 -> conv12 (in-place)
I0428 17:38:40.182747  7412 net.cpp:122] Setting up bn_conv12
I0428 17:38:40.182747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.182747  7412 net.cpp:137] Memory required for data: 173261200
I0428 17:38:40.182747  7412 layer_factory.hpp:77] Creating layer scale_conv12
I0428 17:38:40.182747  7412 net.cpp:84] Creating Layer scale_conv12
I0428 17:38:40.182747  7412 net.cpp:406] scale_conv12 <- conv12
I0428 17:38:40.182747  7412 net.cpp:367] scale_conv12 -> conv12 (in-place)
I0428 17:38:40.182747  7412 layer_factory.hpp:77] Creating layer scale_conv12
I0428 17:38:40.182747  7412 net.cpp:122] Setting up scale_conv12
I0428 17:38:40.183748  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.183748  7412 net.cpp:137] Memory required for data: 174080400
I0428 17:38:40.183748  7412 layer_factory.hpp:77] Creating layer relu12
I0428 17:38:40.183748  7412 net.cpp:84] Creating Layer relu12
I0428 17:38:40.183748  7412 net.cpp:406] relu12 <- conv12
I0428 17:38:40.183748  7412 net.cpp:367] relu12 -> conv12 (in-place)
I0428 17:38:40.183748  7412 net.cpp:122] Setting up relu12
I0428 17:38:40.183748  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.184747  7412 net.cpp:137] Memory required for data: 174899600
I0428 17:38:40.184747  7412 layer_factory.hpp:77] Creating layer conv13
I0428 17:38:40.184747  7412 net.cpp:84] Creating Layer conv13
I0428 17:38:40.184747  7412 net.cpp:406] conv13 <- conv12
I0428 17:38:40.184747  7412 net.cpp:380] conv13 -> conv13
I0428 17:38:40.184747  7412 net.cpp:122] Setting up conv13
I0428 17:38:40.184747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.184747  7412 net.cpp:137] Memory required for data: 175718800
I0428 17:38:40.185747  7412 layer_factory.hpp:77] Creating layer bn_conv13
I0428 17:38:40.185747  7412 net.cpp:84] Creating Layer bn_conv13
I0428 17:38:40.185747  7412 net.cpp:406] bn_conv13 <- conv13
I0428 17:38:40.185747  7412 net.cpp:367] bn_conv13 -> conv13 (in-place)
I0428 17:38:40.185747  7412 net.cpp:122] Setting up bn_conv13
I0428 17:38:40.185747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.185747  7412 net.cpp:137] Memory required for data: 176538000
I0428 17:38:40.185747  7412 layer_factory.hpp:77] Creating layer scale_conv13
I0428 17:38:40.185747  7412 net.cpp:84] Creating Layer scale_conv13
I0428 17:38:40.185747  7412 net.cpp:406] scale_conv13 <- conv13
I0428 17:38:40.186748  7412 net.cpp:367] scale_conv13 -> conv13 (in-place)
I0428 17:38:40.186748  7412 layer_factory.hpp:77] Creating layer scale_conv13
I0428 17:38:40.186748  7412 net.cpp:122] Setting up scale_conv13
I0428 17:38:40.186748  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.186748  7412 net.cpp:137] Memory required for data: 177357200
I0428 17:38:40.186748  7412 layer_factory.hpp:77] Creating layer relu13
I0428 17:38:40.186748  7412 net.cpp:84] Creating Layer relu13
I0428 17:38:40.186748  7412 net.cpp:406] relu13 <- conv13
I0428 17:38:40.186748  7412 net.cpp:367] relu13 -> conv13 (in-place)
I0428 17:38:40.187747  7412 net.cpp:122] Setting up relu13
I0428 17:38:40.187747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.187747  7412 net.cpp:137] Memory required for data: 178176400
I0428 17:38:40.187747  7412 layer_factory.hpp:77] Creating layer conv14
I0428 17:38:40.187747  7412 net.cpp:84] Creating Layer conv14
I0428 17:38:40.187747  7412 net.cpp:406] conv14 <- conv13
I0428 17:38:40.187747  7412 net.cpp:380] conv14 -> conv14
I0428 17:38:40.187747  7412 net.cpp:122] Setting up conv14
I0428 17:38:40.187747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.188747  7412 net.cpp:137] Memory required for data: 178995600
I0428 17:38:40.188747  7412 layer_factory.hpp:77] Creating layer bn_conv14
I0428 17:38:40.188747  7412 net.cpp:84] Creating Layer bn_conv14
I0428 17:38:40.188747  7412 net.cpp:406] bn_conv14 <- conv14
I0428 17:38:40.188747  7412 net.cpp:367] bn_conv14 -> conv14 (in-place)
I0428 17:38:40.188747  7412 net.cpp:122] Setting up bn_conv14
I0428 17:38:40.188747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.188747  7412 net.cpp:137] Memory required for data: 179814800
I0428 17:38:40.189748  7412 layer_factory.hpp:77] Creating layer scale_conv14
I0428 17:38:40.189748  7412 net.cpp:84] Creating Layer scale_conv14
I0428 17:38:40.189748  7412 net.cpp:406] scale_conv14 <- conv14
I0428 17:38:40.189748  7412 net.cpp:367] scale_conv14 -> conv14 (in-place)
I0428 17:38:40.189748  7412 layer_factory.hpp:77] Creating layer scale_conv14
I0428 17:38:40.189748  7412 net.cpp:122] Setting up scale_conv14
I0428 17:38:40.189748  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.189748  7412 net.cpp:137] Memory required for data: 180634000
I0428 17:38:40.189748  7412 layer_factory.hpp:77] Creating layer relu14
I0428 17:38:40.189748  7412 net.cpp:84] Creating Layer relu14
I0428 17:38:40.190747  7412 net.cpp:406] relu14 <- conv14
I0428 17:38:40.190747  7412 net.cpp:367] relu14 -> conv14 (in-place)
I0428 17:38:40.190747  7412 net.cpp:122] Setting up relu14
I0428 17:38:40.190747  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.190747  7412 net.cpp:137] Memory required for data: 181453200
I0428 17:38:40.190747  7412 layer_factory.hpp:77] Creating layer conv15
I0428 17:38:40.190747  7412 net.cpp:84] Creating Layer conv15
I0428 17:38:40.190747  7412 net.cpp:406] conv15 <- conv14
I0428 17:38:40.190747  7412 net.cpp:380] conv15 -> conv15
I0428 17:38:40.191747  7412 net.cpp:122] Setting up conv15
I0428 17:38:40.191747  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.191747  7412 net.cpp:137] Memory required for data: 181658000
I0428 17:38:40.191747  7412 layer_factory.hpp:77] Creating layer bn_conv15
I0428 17:38:40.191747  7412 net.cpp:84] Creating Layer bn_conv15
I0428 17:38:40.191747  7412 net.cpp:406] bn_conv15 <- conv15
I0428 17:38:40.191747  7412 net.cpp:367] bn_conv15 -> conv15 (in-place)
I0428 17:38:40.191747  7412 net.cpp:122] Setting up bn_conv15
I0428 17:38:40.192747  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.192747  7412 net.cpp:137] Memory required for data: 181862800
I0428 17:38:40.192747  7412 layer_factory.hpp:77] Creating layer scale_conv15
I0428 17:38:40.192747  7412 net.cpp:84] Creating Layer scale_conv15
I0428 17:38:40.192747  7412 net.cpp:406] scale_conv15 <- conv15
I0428 17:38:40.192747  7412 net.cpp:367] scale_conv15 -> conv15 (in-place)
I0428 17:38:40.192747  7412 layer_factory.hpp:77] Creating layer scale_conv15
I0428 17:38:40.192747  7412 net.cpp:122] Setting up scale_conv15
I0428 17:38:40.192747  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.192747  7412 net.cpp:137] Memory required for data: 182067600
I0428 17:38:40.193747  7412 layer_factory.hpp:77] Creating layer relu15
I0428 17:38:40.193747  7412 net.cpp:84] Creating Layer relu15
I0428 17:38:40.193747  7412 net.cpp:406] relu15 <- conv15
I0428 17:38:40.193747  7412 net.cpp:367] relu15 -> conv15 (in-place)
I0428 17:38:40.193747  7412 net.cpp:122] Setting up relu15
I0428 17:38:40.193747  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.193747  7412 net.cpp:137] Memory required for data: 182272400
I0428 17:38:40.193747  7412 layer_factory.hpp:77] Creating layer conv16
I0428 17:38:40.194747  7412 net.cpp:84] Creating Layer conv16
I0428 17:38:40.194747  7412 net.cpp:406] conv16 <- conv15
I0428 17:38:40.194747  7412 net.cpp:380] conv16 -> conv16
I0428 17:38:40.194747  7412 net.cpp:122] Setting up conv16
I0428 17:38:40.194747  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.194747  7412 net.cpp:137] Memory required for data: 182477200
I0428 17:38:40.194747  7412 layer_factory.hpp:77] Creating layer bn_conv16
I0428 17:38:40.194747  7412 net.cpp:84] Creating Layer bn_conv16
I0428 17:38:40.195747  7412 net.cpp:406] bn_conv16 <- conv16
I0428 17:38:40.195747  7412 net.cpp:367] bn_conv16 -> conv16 (in-place)
I0428 17:38:40.195747  7412 net.cpp:122] Setting up bn_conv16
I0428 17:38:40.195747  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.195747  7412 net.cpp:137] Memory required for data: 182682000
I0428 17:38:40.195747  7412 layer_factory.hpp:77] Creating layer scale_conv16
I0428 17:38:40.195747  7412 net.cpp:84] Creating Layer scale_conv16
I0428 17:38:40.195747  7412 net.cpp:406] scale_conv16 <- conv16
I0428 17:38:40.195747  7412 net.cpp:367] scale_conv16 -> conv16 (in-place)
I0428 17:38:40.196748  7412 layer_factory.hpp:77] Creating layer scale_conv16
I0428 17:38:40.196748  7412 net.cpp:122] Setting up scale_conv16
I0428 17:38:40.196748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.196748  7412 net.cpp:137] Memory required for data: 182886800
I0428 17:38:40.196748  7412 layer_factory.hpp:77] Creating layer relu16
I0428 17:38:40.196748  7412 net.cpp:84] Creating Layer relu16
I0428 17:38:40.196748  7412 net.cpp:406] relu16 <- conv16
I0428 17:38:40.196748  7412 net.cpp:367] relu16 -> conv16 (in-place)
I0428 17:38:40.196748  7412 net.cpp:122] Setting up relu16
I0428 17:38:40.197748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.197748  7412 net.cpp:137] Memory required for data: 183091600
I0428 17:38:40.197748  7412 layer_factory.hpp:77] Creating layer conv17
I0428 17:38:40.197748  7412 net.cpp:84] Creating Layer conv17
I0428 17:38:40.197748  7412 net.cpp:406] conv17 <- conv16
I0428 17:38:40.197748  7412 net.cpp:380] conv17 -> conv17
I0428 17:38:40.197748  7412 net.cpp:122] Setting up conv17
I0428 17:38:40.197748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.197748  7412 net.cpp:137] Memory required for data: 183296400
I0428 17:38:40.198748  7412 layer_factory.hpp:77] Creating layer bn_conv17
I0428 17:38:40.198748  7412 net.cpp:84] Creating Layer bn_conv17
I0428 17:38:40.198748  7412 net.cpp:406] bn_conv17 <- conv17
I0428 17:38:40.198748  7412 net.cpp:367] bn_conv17 -> conv17 (in-place)
I0428 17:38:40.198748  7412 net.cpp:122] Setting up bn_conv17
I0428 17:38:40.198748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.198748  7412 net.cpp:137] Memory required for data: 183501200
I0428 17:38:40.198748  7412 layer_factory.hpp:77] Creating layer scale_conv17
I0428 17:38:40.199748  7412 net.cpp:84] Creating Layer scale_conv17
I0428 17:38:40.199748  7412 net.cpp:406] scale_conv17 <- conv17
I0428 17:38:40.199748  7412 net.cpp:367] scale_conv17 -> conv17 (in-place)
I0428 17:38:40.199748  7412 layer_factory.hpp:77] Creating layer scale_conv17
I0428 17:38:40.199748  7412 net.cpp:122] Setting up scale_conv17
I0428 17:38:40.199748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.199748  7412 net.cpp:137] Memory required for data: 183706000
I0428 17:38:40.199748  7412 layer_factory.hpp:77] Creating layer relu17
I0428 17:38:40.199748  7412 net.cpp:84] Creating Layer relu17
I0428 17:38:40.200748  7412 net.cpp:406] relu17 <- conv17
I0428 17:38:40.200748  7412 net.cpp:367] relu17 -> conv17 (in-place)
I0428 17:38:40.200748  7412 net.cpp:122] Setting up relu17
I0428 17:38:40.200748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.200748  7412 net.cpp:137] Memory required for data: 183910800
I0428 17:38:40.200748  7412 layer_factory.hpp:77] Creating layer conv18
I0428 17:38:40.200748  7412 net.cpp:84] Creating Layer conv18
I0428 17:38:40.200748  7412 net.cpp:406] conv18 <- conv17
I0428 17:38:40.200748  7412 net.cpp:380] conv18 -> conv18
I0428 17:38:40.201748  7412 net.cpp:122] Setting up conv18
I0428 17:38:40.201748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.201748  7412 net.cpp:137] Memory required for data: 184115600
I0428 17:38:40.201748  7412 layer_factory.hpp:77] Creating layer bn_conv17
I0428 17:38:40.201748  7412 net.cpp:84] Creating Layer bn_conv17
I0428 17:38:40.201748  7412 net.cpp:406] bn_conv17 <- conv18
I0428 17:38:40.201748  7412 net.cpp:367] bn_conv17 -> conv18 (in-place)
I0428 17:38:40.201748  7412 net.cpp:122] Setting up bn_conv17
I0428 17:38:40.201748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.202749  7412 net.cpp:137] Memory required for data: 184320400
I0428 17:38:40.202749  7412 layer_factory.hpp:77] Creating layer scale_conv18
I0428 17:38:40.202749  7412 net.cpp:84] Creating Layer scale_conv18
I0428 17:38:40.202749  7412 net.cpp:406] scale_conv18 <- conv18
I0428 17:38:40.202749  7412 net.cpp:367] scale_conv18 -> conv18 (in-place)
I0428 17:38:40.202749  7412 layer_factory.hpp:77] Creating layer scale_conv18
I0428 17:38:40.202749  7412 net.cpp:122] Setting up scale_conv18
I0428 17:38:40.202749  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.202749  7412 net.cpp:137] Memory required for data: 184525200
I0428 17:38:40.203748  7412 layer_factory.hpp:77] Creating layer relu18
I0428 17:38:40.203748  7412 net.cpp:84] Creating Layer relu18
I0428 17:38:40.203748  7412 net.cpp:406] relu18 <- conv18
I0428 17:38:40.203748  7412 net.cpp:367] relu18 -> conv18 (in-place)
I0428 17:38:40.203748  7412 net.cpp:122] Setting up relu18
I0428 17:38:40.203748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.203748  7412 net.cpp:137] Memory required for data: 184730000
I0428 17:38:40.203748  7412 layer_factory.hpp:77] Creating layer conv19
I0428 17:38:40.203748  7412 net.cpp:84] Creating Layer conv19
I0428 17:38:40.203748  7412 net.cpp:406] conv19 <- conv18
I0428 17:38:40.204748  7412 net.cpp:380] conv19 -> conv19
I0428 17:38:40.204748  7412 net.cpp:122] Setting up conv19
I0428 17:38:40.204748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.204748  7412 net.cpp:137] Memory required for data: 184934800
I0428 17:38:40.204748  7412 layer_factory.hpp:77] Creating layer bn_conv19
I0428 17:38:40.204748  7412 net.cpp:84] Creating Layer bn_conv19
I0428 17:38:40.204748  7412 net.cpp:406] bn_conv19 <- conv19
I0428 17:38:40.204748  7412 net.cpp:367] bn_conv19 -> conv19 (in-place)
I0428 17:38:40.205749  7412 net.cpp:122] Setting up bn_conv19
I0428 17:38:40.205749  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.205749  7412 net.cpp:137] Memory required for data: 185139600
I0428 17:38:40.205749  7412 layer_factory.hpp:77] Creating layer scale_conv19
I0428 17:38:40.205749  7412 net.cpp:84] Creating Layer scale_conv19
I0428 17:38:40.205749  7412 net.cpp:406] scale_conv19 <- conv19
I0428 17:38:40.205749  7412 net.cpp:367] scale_conv19 -> conv19 (in-place)
I0428 17:38:40.205749  7412 layer_factory.hpp:77] Creating layer scale_conv19
I0428 17:38:40.206748  7412 net.cpp:122] Setting up scale_conv19
I0428 17:38:40.206748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.206748  7412 net.cpp:137] Memory required for data: 185344400
I0428 17:38:40.206748  7412 layer_factory.hpp:77] Creating layer relu19
I0428 17:38:40.206748  7412 net.cpp:84] Creating Layer relu19
I0428 17:38:40.206748  7412 net.cpp:406] relu19 <- conv19
I0428 17:38:40.206748  7412 net.cpp:367] relu19 -> conv19 (in-place)
I0428 17:38:40.206748  7412 net.cpp:122] Setting up relu19
I0428 17:38:40.206748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.206748  7412 net.cpp:137] Memory required for data: 185549200
I0428 17:38:40.207748  7412 layer_factory.hpp:77] Creating layer conv20
I0428 17:38:40.207748  7412 net.cpp:84] Creating Layer conv20
I0428 17:38:40.207748  7412 net.cpp:406] conv20 <- conv19
I0428 17:38:40.207748  7412 net.cpp:380] conv20 -> conv20
I0428 17:38:40.207748  7412 net.cpp:122] Setting up conv20
I0428 17:38:40.207748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.207748  7412 net.cpp:137] Memory required for data: 185754000
I0428 17:38:40.207748  7412 layer_factory.hpp:77] Creating layer bn_conv20
I0428 17:38:40.207748  7412 net.cpp:84] Creating Layer bn_conv20
I0428 17:38:40.208748  7412 net.cpp:406] bn_conv20 <- conv20
I0428 17:38:40.208748  7412 net.cpp:367] bn_conv20 -> conv20 (in-place)
I0428 17:38:40.208748  7412 net.cpp:122] Setting up bn_conv20
I0428 17:38:40.208748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.208748  7412 net.cpp:137] Memory required for data: 185958800
I0428 17:38:40.208748  7412 layer_factory.hpp:77] Creating layer scale_conv20
I0428 17:38:40.208748  7412 net.cpp:84] Creating Layer scale_conv20
I0428 17:38:40.208748  7412 net.cpp:406] scale_conv20 <- conv20
I0428 17:38:40.208748  7412 net.cpp:367] scale_conv20 -> conv20 (in-place)
I0428 17:38:40.209748  7412 layer_factory.hpp:77] Creating layer scale_conv20
I0428 17:38:40.209748  7412 net.cpp:122] Setting up scale_conv20
I0428 17:38:40.209748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.209748  7412 net.cpp:137] Memory required for data: 186163600
I0428 17:38:40.209748  7412 layer_factory.hpp:77] Creating layer relu20
I0428 17:38:40.209748  7412 net.cpp:84] Creating Layer relu20
I0428 17:38:40.209748  7412 net.cpp:406] relu20 <- conv20
I0428 17:38:40.209748  7412 net.cpp:367] relu20 -> conv20 (in-place)
I0428 17:38:40.209748  7412 net.cpp:122] Setting up relu20
I0428 17:38:40.210748  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.210748  7412 net.cpp:137] Memory required for data: 186368400
I0428 17:38:40.210748  7412 layer_factory.hpp:77] Creating layer pool21
I0428 17:38:40.210748  7412 net.cpp:84] Creating Layer pool21
I0428 17:38:40.210748  7412 net.cpp:406] pool21 <- conv20
I0428 17:38:40.210748  7412 net.cpp:380] pool21 -> pool21
I0428 17:38:40.210748  7412 net.cpp:122] Setting up pool21
I0428 17:38:40.210748  7412 net.cpp:129] Top shape: 100 32 1 1 (3200)
I0428 17:38:40.210748  7412 net.cpp:137] Memory required for data: 186381200
I0428 17:38:40.210748  7412 layer_factory.hpp:77] Creating layer fc22
I0428 17:38:40.211748  7412 net.cpp:84] Creating Layer fc22
I0428 17:38:40.211748  7412 net.cpp:406] fc22 <- pool21
I0428 17:38:40.211748  7412 net.cpp:380] fc22 -> fc22
I0428 17:38:40.211748  7412 net.cpp:122] Setting up fc22
I0428 17:38:40.211748  7412 net.cpp:129] Top shape: 100 10 (1000)
I0428 17:38:40.211748  7412 net.cpp:137] Memory required for data: 186385200
I0428 17:38:40.211748  7412 layer_factory.hpp:77] Creating layer loss
I0428 17:38:40.211748  7412 net.cpp:84] Creating Layer loss
I0428 17:38:40.212749  7412 net.cpp:406] loss <- fc22
I0428 17:38:40.212749  7412 net.cpp:406] loss <- label
I0428 17:38:40.212749  7412 net.cpp:380] loss -> loss
I0428 17:38:40.212749  7412 layer_factory.hpp:77] Creating layer loss
I0428 17:38:40.212749  7412 net.cpp:122] Setting up loss
I0428 17:38:40.212749  7412 net.cpp:129] Top shape: (1)
I0428 17:38:40.212749  7412 net.cpp:132]     with loss weight 1
I0428 17:38:40.212749  7412 net.cpp:137] Memory required for data: 186385204
I0428 17:38:40.213749  7412 net.cpp:198] loss needs backward computation.
I0428 17:38:40.213749  7412 net.cpp:198] fc22 needs backward computation.
I0428 17:38:40.213749  7412 net.cpp:198] pool21 needs backward computation.
I0428 17:38:40.213749  7412 net.cpp:198] relu20 needs backward computation.
I0428 17:38:40.213749  7412 net.cpp:198] scale_conv20 needs backward computation.
I0428 17:38:40.213749  7412 net.cpp:198] bn_conv20 needs backward computation.
I0428 17:38:40.213749  7412 net.cpp:198] conv20 needs backward computation.
I0428 17:38:40.213749  7412 net.cpp:198] relu19 needs backward computation.
I0428 17:38:40.213749  7412 net.cpp:198] scale_conv19 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] bn_conv19 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] conv19 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] relu18 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] scale_conv18 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] bn_conv17 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] conv18 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] relu17 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] scale_conv17 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] bn_conv17 needs backward computation.
I0428 17:38:40.214749  7412 net.cpp:198] conv17 needs backward computation.
I0428 17:38:40.215749  7412 net.cpp:198] relu16 needs backward computation.
I0428 17:38:40.215749  7412 net.cpp:198] scale_conv16 needs backward computation.
I0428 17:38:40.215749  7412 net.cpp:198] bn_conv16 needs backward computation.
I0428 17:38:40.215749  7412 net.cpp:198] conv16 needs backward computation.
I0428 17:38:40.215749  7412 net.cpp:198] relu15 needs backward computation.
I0428 17:38:40.215749  7412 net.cpp:198] scale_conv15 needs backward computation.
I0428 17:38:40.215749  7412 net.cpp:198] bn_conv15 needs backward computation.
I0428 17:38:40.215749  7412 net.cpp:198] conv15 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] relu14 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] scale_conv14 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] bn_conv14 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] conv14 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] relu13 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] scale_conv13 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] bn_conv13 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] conv13 needs backward computation.
I0428 17:38:40.216749  7412 net.cpp:198] relu12 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] scale_conv12 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] bn_conv12 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] conv12 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] relu11 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] scale_conv11 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] bn_conv11 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] conv11 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] relu10 needs backward computation.
I0428 17:38:40.217749  7412 net.cpp:198] scale_conv10 needs backward computation.
I0428 17:38:40.218750  7412 net.cpp:198] bn_conv10 needs backward computation.
I0428 17:38:40.218750  7412 net.cpp:198] conv10 needs backward computation.
I0428 17:38:40.218750  7412 net.cpp:198] relu9 needs backward computation.
I0428 17:38:40.218750  7412 net.cpp:198] scale_conv9 needs backward computation.
I0428 17:38:40.218750  7412 net.cpp:198] bn_conv9 needs backward computation.
I0428 17:38:40.218750  7412 net.cpp:198] conv9 needs backward computation.
I0428 17:38:40.218750  7412 net.cpp:198] relu8 needs backward computation.
I0428 17:38:40.218750  7412 net.cpp:198] scale_conv8 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] bn_conv8 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] conv8 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] relu7 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] scale_conv7 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] bn_conv7 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] conv7 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] relu6 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] scale_conv6 needs backward computation.
I0428 17:38:40.219749  7412 net.cpp:198] bn_conv6 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] conv6 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] relu5 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] scale_conv5 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] bn_conv5 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] conv5 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] relu4 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] scale_conv4 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] bn_conv4 needs backward computation.
I0428 17:38:40.220749  7412 net.cpp:198] conv4 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] relu3 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] scale_conv3 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] bn_conv3 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] conv3 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] relu2 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] scale_conv2 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] bn_conv2 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] conv2 needs backward computation.
I0428 17:38:40.221750  7412 net.cpp:198] relu1 needs backward computation.
I0428 17:38:40.222749  7412 net.cpp:198] scale_conv1 needs backward computation.
I0428 17:38:40.222749  7412 net.cpp:198] bn_conv1 needs backward computation.
I0428 17:38:40.222749  7412 net.cpp:198] conv1 needs backward computation.
I0428 17:38:40.222749  7412 net.cpp:200] data does not need backward computation.
I0428 17:38:40.222749  7412 net.cpp:242] This network produces output loss
I0428 17:38:40.222749  7412 net.cpp:255] Network initialization done.
I0428 17:38:40.223749  7412 solver.cpp:172] Creating test net (#0) specified by net file: D:/_test/train/train_val.prototxt
I0428 17:38:40.223749  7412 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0428 17:38:40.224750  7412 net.cpp:51] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "D:/_test/train/img_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "conv2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv5"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv5"
  type: "Scale"
  bottom: "conv5"
  top: "conv5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "conv5"
  top: "conv6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv6"
  type: "BatchNorm"
  bottom: "conv6"
  top: "conv6"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv6"
  type: "Scale"
  bottom: "conv6"
  top: "conv6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "conv6"
  top: "conv6"
}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "conv6"
  top: "conv7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv7"
  type: "BatchNorm"
  bottom: "conv7"
  top: "conv7"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv7"
  type: "Scale"
  bottom: "conv7"
  top: "conv7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "conv7"
  top: "conv8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv8"
  type: "BatchNorm"
  bottom: "conv8"
  top: "conv8"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv8"
  type: "Scale"
  bottom: "conv8"
  top: "conv8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "conv8"
  top: "conv9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv9"
  type: "BatchNorm"
  bottom: "conv9"
  top: "conv9"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv9"
  type: "Scale"
  bottom: "conv9"
  top: "conv9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "conv9"
  top: "conv10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv10"
  type: "BatchNorm"
  bottom: "conv10"
  top: "conv10"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv10"
  type: "Scale"
  bottom: "conv10"
  top: "conv10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv10"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "conv13"
  type: "Convolution"
  bottom: "conv12"
  top: "conv13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv13"
  type: "BatchNorm"
  bottom: "conv13"
  top: "conv13"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv13"
  type: "Scale"
  bottom: "conv13"
  top: "conv13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu13"
  type: "ReLU"
  bottom: "conv13"
  top: "conv13"
}
layer {
  name: "conv14"
  type: "Convolution"
  bottom: "conv13"
  top: "conv14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv14"
  type: "BatchNorm"
  bottom: "conv14"
  top: "conv14"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv14"
  type: "Scale"
  bottom: "conv14"
  top: "conv14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu14"
  type: "ReLU"
  bottom: "conv14"
  top: "conv14"
}
layer {
  name: "conv15"
  type: "Convolution"
  bottom: "conv14"
  top: "conv15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv15"
  type: "BatchNorm"
  bottom: "conv15"
  top: "conv15"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv15"
  type: "Scale"
  bottom: "conv15"
  top: "conv15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu15"
  type: "ReLU"
  bottom: "conv15"
  top: "conv15"
}
layer {
  name: "conv16"
  type: "Convolution"
  bottom: "conv15"
  top: "conv16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv16"
  type: "BatchNorm"
  bottom: "conv16"
  top: "conv16"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv16"
  type: "Scale"
  bottom: "conv16"
  top: "conv16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu16"
  type: "ReLU"
  bottom: "conv16"
  top: "conv16"
}
layer {
  name: "conv17"
  type: "Convolution"
  bottom: "conv16"
  top: "conv17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv17"
  type: "BatchNorm"
  bottom: "conv17"
  top: "conv17"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv17"
  type: "Scale"
  bottom: "conv17"
  top: "conv17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu17"
  type: "ReLU"
  bottom: "conv17"
  top: "conv17"
}
layer {
  name: "conv18"
  type: "Convolution"
  bottom: "conv17"
  top: "conv18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv17"
  type: "BatchNorm"
  bottom: "conv18"
  top: "conv18"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv18"
  type: "Scale"
  bottom: "conv18"
  top: "conv18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu18"
  type: "ReLU"
  bottom: "conv18"
  top: "conv18"
}
layer {
  name: "conv19"
  type: "Convolution"
  bottom: "conv18"
  top: "conv19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv19"
  type: "BatchNorm"
  bottom: "conv19"
  top: "conv19"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv19"
  type: "Scale"
  bottom: "conv19"
  top: "conv19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu19"
  type: "ReLU"
  bottom: "conv19"
  top: "conv19"
}
layer {
  name: "conv20"
  type: "Convolution"
  bottom: "conv19"
  top: "conv20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn_conv20"
  type: "BatchNorm"
  bottom: "conv20"
  top: "conv20"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale_conv20"
  type: "Scale"
  bottom: "conv20"
  top: "conv20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu20"
  type: "ReLU"
  bottom: "conv20"
  top: "conv20"
}
layer {
  name: "pool21"
  type: "Pooling"
  bottom: "conv20"
  top: "pool21"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc22"
  type: "InnerProduct"
  bottom: "pool21"
  top: "fc22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
      variance_norm: FAN_IN
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc22"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc22"
  bottom: "label"
  top: "loss"
}
I0428 17:38:40.262751  7412 layer_factory.hpp:77] Creating layer data
I0428 17:38:40.263751  7412 db_lmdb.cpp:40] Opened lmdb D:/_test/train/img_test_lmdb
I0428 17:38:40.263751  7412 net.cpp:84] Creating Layer data
I0428 17:38:40.264751  7412 net.cpp:380] data -> data
I0428 17:38:40.264751  7412 net.cpp:380] data -> label
I0428 17:38:40.264751  7412 data_layer.cpp:45] output data size: 100,3,32,32
I0428 17:38:40.268752  7412 net.cpp:122] Setting up data
I0428 17:38:40.269752  7412 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0428 17:38:40.269752  7412 net.cpp:129] Top shape: 100 (100)
I0428 17:38:40.269752  7412 net.cpp:137] Memory required for data: 1229200
I0428 17:38:40.269752  7412 layer_factory.hpp:77] Creating layer label_data_1_split
I0428 17:38:40.270752  7412 net.cpp:84] Creating Layer label_data_1_split
I0428 17:38:40.270752  7412 net.cpp:406] label_data_1_split <- label
I0428 17:38:40.270752  7412 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0428 17:38:40.270752  7412 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0428 17:38:40.270752  7412 net.cpp:122] Setting up label_data_1_split
I0428 17:38:40.271752  7412 net.cpp:129] Top shape: 100 (100)
I0428 17:38:40.271752  7412 net.cpp:129] Top shape: 100 (100)
I0428 17:38:40.271752  7412 net.cpp:137] Memory required for data: 1230000
I0428 17:38:40.271752  7412 layer_factory.hpp:77] Creating layer conv1
I0428 17:38:40.271752  7412 net.cpp:84] Creating Layer conv1
I0428 17:38:40.271752  7412 net.cpp:406] conv1 <- data
I0428 17:38:40.271752  7412 net.cpp:380] conv1 -> conv1
I0428 17:38:40.271752  7412 net.cpp:122] Setting up conv1
I0428 17:38:40.272753  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.272753  7412 net.cpp:137] Memory required for data: 7783600
I0428 17:38:40.272753  7412 layer_factory.hpp:77] Creating layer bn_conv1
I0428 17:38:40.272753  7412 net.cpp:84] Creating Layer bn_conv1
I0428 17:38:40.272753  7412 net.cpp:406] bn_conv1 <- conv1
I0428 17:38:40.272753  7412 net.cpp:367] bn_conv1 -> conv1 (in-place)
I0428 17:38:40.272753  7412 net.cpp:122] Setting up bn_conv1
I0428 17:38:40.272753  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.273752  7412 net.cpp:137] Memory required for data: 14337200
I0428 17:38:40.273752  7412 layer_factory.hpp:77] Creating layer scale_conv1
I0428 17:38:40.273752  7412 net.cpp:84] Creating Layer scale_conv1
I0428 17:38:40.273752  7412 net.cpp:406] scale_conv1 <- conv1
I0428 17:38:40.273752  7412 net.cpp:367] scale_conv1 -> conv1 (in-place)
I0428 17:38:40.273752  7412 layer_factory.hpp:77] Creating layer scale_conv1
I0428 17:38:40.273752  7412 net.cpp:122] Setting up scale_conv1
I0428 17:38:40.273752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.273752  7412 net.cpp:137] Memory required for data: 20890800
I0428 17:38:40.273752  7412 layer_factory.hpp:77] Creating layer relu1
I0428 17:38:40.274752  7412 net.cpp:84] Creating Layer relu1
I0428 17:38:40.274752  7412 net.cpp:406] relu1 <- conv1
I0428 17:38:40.274752  7412 net.cpp:367] relu1 -> conv1 (in-place)
I0428 17:38:40.274752  7412 net.cpp:122] Setting up relu1
I0428 17:38:40.274752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.274752  7412 net.cpp:137] Memory required for data: 27444400
I0428 17:38:40.274752  7412 layer_factory.hpp:77] Creating layer conv2
I0428 17:38:40.274752  7412 net.cpp:84] Creating Layer conv2
I0428 17:38:40.274752  7412 net.cpp:406] conv2 <- conv1
I0428 17:38:40.274752  7412 net.cpp:380] conv2 -> conv2
I0428 17:38:40.275753  7412 net.cpp:122] Setting up conv2
I0428 17:38:40.275753  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.275753  7412 net.cpp:137] Memory required for data: 33998000
I0428 17:38:40.275753  7412 layer_factory.hpp:77] Creating layer bn_conv2
I0428 17:38:40.275753  7412 net.cpp:84] Creating Layer bn_conv2
I0428 17:38:40.275753  7412 net.cpp:406] bn_conv2 <- conv2
I0428 17:38:40.275753  7412 net.cpp:367] bn_conv2 -> conv2 (in-place)
I0428 17:38:40.276752  7412 net.cpp:122] Setting up bn_conv2
I0428 17:38:40.276752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.276752  7412 net.cpp:137] Memory required for data: 40551600
I0428 17:38:40.276752  7412 layer_factory.hpp:77] Creating layer scale_conv2
I0428 17:38:40.276752  7412 net.cpp:84] Creating Layer scale_conv2
I0428 17:38:40.276752  7412 net.cpp:406] scale_conv2 <- conv2
I0428 17:38:40.276752  7412 net.cpp:367] scale_conv2 -> conv2 (in-place)
I0428 17:38:40.276752  7412 layer_factory.hpp:77] Creating layer scale_conv2
I0428 17:38:40.276752  7412 net.cpp:122] Setting up scale_conv2
I0428 17:38:40.277752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.277752  7412 net.cpp:137] Memory required for data: 47105200
I0428 17:38:40.277752  7412 layer_factory.hpp:77] Creating layer relu2
I0428 17:38:40.277752  7412 net.cpp:84] Creating Layer relu2
I0428 17:38:40.277752  7412 net.cpp:406] relu2 <- conv2
I0428 17:38:40.277752  7412 net.cpp:367] relu2 -> conv2 (in-place)
I0428 17:38:40.277752  7412 net.cpp:122] Setting up relu2
I0428 17:38:40.277752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.277752  7412 net.cpp:137] Memory required for data: 53658800
I0428 17:38:40.277752  7412 layer_factory.hpp:77] Creating layer conv3
I0428 17:38:40.278753  7412 net.cpp:84] Creating Layer conv3
I0428 17:38:40.278753  7412 net.cpp:406] conv3 <- conv2
I0428 17:38:40.278753  7412 net.cpp:380] conv3 -> conv3
I0428 17:38:40.278753  7412 net.cpp:122] Setting up conv3
I0428 17:38:40.278753  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.278753  7412 net.cpp:137] Memory required for data: 60212400
I0428 17:38:40.278753  7412 layer_factory.hpp:77] Creating layer bn_conv3
I0428 17:38:40.278753  7412 net.cpp:84] Creating Layer bn_conv3
I0428 17:38:40.278753  7412 net.cpp:406] bn_conv3 <- conv3
I0428 17:38:40.279752  7412 net.cpp:367] bn_conv3 -> conv3 (in-place)
I0428 17:38:40.279752  7412 net.cpp:122] Setting up bn_conv3
I0428 17:38:40.279752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.279752  7412 net.cpp:137] Memory required for data: 66766000
I0428 17:38:40.279752  7412 layer_factory.hpp:77] Creating layer scale_conv3
I0428 17:38:40.279752  7412 net.cpp:84] Creating Layer scale_conv3
I0428 17:38:40.279752  7412 net.cpp:406] scale_conv3 <- conv3
I0428 17:38:40.279752  7412 net.cpp:367] scale_conv3 -> conv3 (in-place)
I0428 17:38:40.279752  7412 layer_factory.hpp:77] Creating layer scale_conv3
I0428 17:38:40.280752  7412 net.cpp:122] Setting up scale_conv3
I0428 17:38:40.280752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.280752  7412 net.cpp:137] Memory required for data: 73319600
I0428 17:38:40.280752  7412 layer_factory.hpp:77] Creating layer relu3
I0428 17:38:40.280752  7412 net.cpp:84] Creating Layer relu3
I0428 17:38:40.280752  7412 net.cpp:406] relu3 <- conv3
I0428 17:38:40.280752  7412 net.cpp:367] relu3 -> conv3 (in-place)
I0428 17:38:40.281752  7412 net.cpp:122] Setting up relu3
I0428 17:38:40.281752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.281752  7412 net.cpp:137] Memory required for data: 79873200
I0428 17:38:40.281752  7412 layer_factory.hpp:77] Creating layer conv4
I0428 17:38:40.281752  7412 net.cpp:84] Creating Layer conv4
I0428 17:38:40.281752  7412 net.cpp:406] conv4 <- conv3
I0428 17:38:40.281752  7412 net.cpp:380] conv4 -> conv4
I0428 17:38:40.281752  7412 net.cpp:122] Setting up conv4
I0428 17:38:40.281752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.282752  7412 net.cpp:137] Memory required for data: 86426800
I0428 17:38:40.282752  7412 layer_factory.hpp:77] Creating layer bn_conv4
I0428 17:38:40.282752  7412 net.cpp:84] Creating Layer bn_conv4
I0428 17:38:40.282752  7412 net.cpp:406] bn_conv4 <- conv4
I0428 17:38:40.282752  7412 net.cpp:367] bn_conv4 -> conv4 (in-place)
I0428 17:38:40.282752  7412 net.cpp:122] Setting up bn_conv4
I0428 17:38:40.282752  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.282752  7412 net.cpp:137] Memory required for data: 92980400
I0428 17:38:40.282752  7412 layer_factory.hpp:77] Creating layer scale_conv4
I0428 17:38:40.283753  7412 net.cpp:84] Creating Layer scale_conv4
I0428 17:38:40.283753  7412 net.cpp:406] scale_conv4 <- conv4
I0428 17:38:40.283753  7412 net.cpp:367] scale_conv4 -> conv4 (in-place)
I0428 17:38:40.283753  7412 layer_factory.hpp:77] Creating layer scale_conv4
I0428 17:38:40.283753  7412 net.cpp:122] Setting up scale_conv4
I0428 17:38:40.283753  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.283753  7412 net.cpp:137] Memory required for data: 99534000
I0428 17:38:40.283753  7412 layer_factory.hpp:77] Creating layer relu4
I0428 17:38:40.283753  7412 net.cpp:84] Creating Layer relu4
I0428 17:38:40.283753  7412 net.cpp:406] relu4 <- conv4
I0428 17:38:40.284754  7412 net.cpp:367] relu4 -> conv4 (in-place)
I0428 17:38:40.284754  7412 net.cpp:122] Setting up relu4
I0428 17:38:40.284754  7412 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0428 17:38:40.284754  7412 net.cpp:137] Memory required for data: 106087600
I0428 17:38:40.284754  7412 layer_factory.hpp:77] Creating layer conv5
I0428 17:38:40.284754  7412 net.cpp:84] Creating Layer conv5
I0428 17:38:40.284754  7412 net.cpp:406] conv5 <- conv4
I0428 17:38:40.284754  7412 net.cpp:380] conv5 -> conv5
I0428 17:38:40.284754  7412 net.cpp:122] Setting up conv5
I0428 17:38:40.285753  7412 net.cpp:129] Top shape: 100 16 16 16 (409600)
I0428 17:38:40.285753  7412 net.cpp:137] Memory required for data: 107726000
I0428 17:38:40.285753  7412 layer_factory.hpp:77] Creating layer bn_conv5
I0428 17:38:40.285753  7412 net.cpp:84] Creating Layer bn_conv5
I0428 17:38:40.285753  7412 net.cpp:406] bn_conv5 <- conv5
I0428 17:38:40.285753  7412 net.cpp:367] bn_conv5 -> conv5 (in-place)
I0428 17:38:40.285753  7412 net.cpp:122] Setting up bn_conv5
I0428 17:38:40.285753  7412 net.cpp:129] Top shape: 100 16 16 16 (409600)
I0428 17:38:40.285753  7412 net.cpp:137] Memory required for data: 109364400
I0428 17:38:40.286753  7412 layer_factory.hpp:77] Creating layer scale_conv5
I0428 17:38:40.286753  7412 net.cpp:84] Creating Layer scale_conv5
I0428 17:38:40.286753  7412 net.cpp:406] scale_conv5 <- conv5
I0428 17:38:40.286753  7412 net.cpp:367] scale_conv5 -> conv5 (in-place)
I0428 17:38:40.286753  7412 layer_factory.hpp:77] Creating layer scale_conv5
I0428 17:38:40.286753  7412 net.cpp:122] Setting up scale_conv5
I0428 17:38:40.286753  7412 net.cpp:129] Top shape: 100 16 16 16 (409600)
I0428 17:38:40.286753  7412 net.cpp:137] Memory required for data: 111002800
I0428 17:38:40.286753  7412 layer_factory.hpp:77] Creating layer relu5
I0428 17:38:40.286753  7412 net.cpp:84] Creating Layer relu5
I0428 17:38:40.287753  7412 net.cpp:406] relu5 <- conv5
I0428 17:38:40.287753  7412 net.cpp:367] relu5 -> conv5 (in-place)
I0428 17:38:40.287753  7412 net.cpp:122] Setting up relu5
I0428 17:38:40.287753  7412 net.cpp:129] Top shape: 100 16 16 16 (409600)
I0428 17:38:40.287753  7412 net.cpp:137] Memory required for data: 112641200
I0428 17:38:40.287753  7412 layer_factory.hpp:77] Creating layer conv6
I0428 17:38:40.287753  7412 net.cpp:84] Creating Layer conv6
I0428 17:38:40.287753  7412 net.cpp:406] conv6 <- conv5
I0428 17:38:40.287753  7412 net.cpp:380] conv6 -> conv6
I0428 17:38:40.288753  7412 net.cpp:122] Setting up conv6
I0428 17:38:40.288753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.288753  7412 net.cpp:137] Memory required for data: 115918000
I0428 17:38:40.288753  7412 layer_factory.hpp:77] Creating layer bn_conv6
I0428 17:38:40.288753  7412 net.cpp:84] Creating Layer bn_conv6
I0428 17:38:40.288753  7412 net.cpp:406] bn_conv6 <- conv6
I0428 17:38:40.288753  7412 net.cpp:367] bn_conv6 -> conv6 (in-place)
I0428 17:38:40.288753  7412 net.cpp:122] Setting up bn_conv6
I0428 17:38:40.288753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.289753  7412 net.cpp:137] Memory required for data: 119194800
I0428 17:38:40.289753  7412 layer_factory.hpp:77] Creating layer scale_conv6
I0428 17:38:40.289753  7412 net.cpp:84] Creating Layer scale_conv6
I0428 17:38:40.289753  7412 net.cpp:406] scale_conv6 <- conv6
I0428 17:38:40.289753  7412 net.cpp:367] scale_conv6 -> conv6 (in-place)
I0428 17:38:40.289753  7412 layer_factory.hpp:77] Creating layer scale_conv6
I0428 17:38:40.289753  7412 net.cpp:122] Setting up scale_conv6
I0428 17:38:40.289753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.289753  7412 net.cpp:137] Memory required for data: 122471600
I0428 17:38:40.289753  7412 layer_factory.hpp:77] Creating layer relu6
I0428 17:38:40.290753  7412 net.cpp:84] Creating Layer relu6
I0428 17:38:40.290753  7412 net.cpp:406] relu6 <- conv6
I0428 17:38:40.290753  7412 net.cpp:367] relu6 -> conv6 (in-place)
I0428 17:38:40.290753  7412 net.cpp:122] Setting up relu6
I0428 17:38:40.290753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.290753  7412 net.cpp:137] Memory required for data: 125748400
I0428 17:38:40.290753  7412 layer_factory.hpp:77] Creating layer conv7
I0428 17:38:40.290753  7412 net.cpp:84] Creating Layer conv7
I0428 17:38:40.290753  7412 net.cpp:406] conv7 <- conv6
I0428 17:38:40.290753  7412 net.cpp:380] conv7 -> conv7
I0428 17:38:40.291754  7412 net.cpp:122] Setting up conv7
I0428 17:38:40.291754  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.291754  7412 net.cpp:137] Memory required for data: 129025200
I0428 17:38:40.291754  7412 layer_factory.hpp:77] Creating layer bn_conv7
I0428 17:38:40.291754  7412 net.cpp:84] Creating Layer bn_conv7
I0428 17:38:40.291754  7412 net.cpp:406] bn_conv7 <- conv7
I0428 17:38:40.291754  7412 net.cpp:367] bn_conv7 -> conv7 (in-place)
I0428 17:38:40.291754  7412 net.cpp:122] Setting up bn_conv7
I0428 17:38:40.292753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.292753  7412 net.cpp:137] Memory required for data: 132302000
I0428 17:38:40.292753  7412 layer_factory.hpp:77] Creating layer scale_conv7
I0428 17:38:40.292753  7412 net.cpp:84] Creating Layer scale_conv7
I0428 17:38:40.292753  7412 net.cpp:406] scale_conv7 <- conv7
I0428 17:38:40.292753  7412 net.cpp:367] scale_conv7 -> conv7 (in-place)
I0428 17:38:40.292753  7412 layer_factory.hpp:77] Creating layer scale_conv7
I0428 17:38:40.292753  7412 net.cpp:122] Setting up scale_conv7
I0428 17:38:40.292753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.293753  7412 net.cpp:137] Memory required for data: 135578800
I0428 17:38:40.293753  7412 layer_factory.hpp:77] Creating layer relu7
I0428 17:38:40.293753  7412 net.cpp:84] Creating Layer relu7
I0428 17:38:40.293753  7412 net.cpp:406] relu7 <- conv7
I0428 17:38:40.293753  7412 net.cpp:367] relu7 -> conv7 (in-place)
I0428 17:38:40.293753  7412 net.cpp:122] Setting up relu7
I0428 17:38:40.293753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.293753  7412 net.cpp:137] Memory required for data: 138855600
I0428 17:38:40.293753  7412 layer_factory.hpp:77] Creating layer conv8
I0428 17:38:40.293753  7412 net.cpp:84] Creating Layer conv8
I0428 17:38:40.293753  7412 net.cpp:406] conv8 <- conv7
I0428 17:38:40.294754  7412 net.cpp:380] conv8 -> conv8
I0428 17:38:40.294754  7412 net.cpp:122] Setting up conv8
I0428 17:38:40.294754  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.294754  7412 net.cpp:137] Memory required for data: 142132400
I0428 17:38:40.294754  7412 layer_factory.hpp:77] Creating layer bn_conv8
I0428 17:38:40.294754  7412 net.cpp:84] Creating Layer bn_conv8
I0428 17:38:40.294754  7412 net.cpp:406] bn_conv8 <- conv8
I0428 17:38:40.294754  7412 net.cpp:367] bn_conv8 -> conv8 (in-place)
I0428 17:38:40.295753  7412 net.cpp:122] Setting up bn_conv8
I0428 17:38:40.295753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.295753  7412 net.cpp:137] Memory required for data: 145409200
I0428 17:38:40.295753  7412 layer_factory.hpp:77] Creating layer scale_conv8
I0428 17:38:40.295753  7412 net.cpp:84] Creating Layer scale_conv8
I0428 17:38:40.295753  7412 net.cpp:406] scale_conv8 <- conv8
I0428 17:38:40.295753  7412 net.cpp:367] scale_conv8 -> conv8 (in-place)
I0428 17:38:40.296753  7412 layer_factory.hpp:77] Creating layer scale_conv8
I0428 17:38:40.296753  7412 net.cpp:122] Setting up scale_conv8
I0428 17:38:40.296753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.296753  7412 net.cpp:137] Memory required for data: 148686000
I0428 17:38:40.296753  7412 layer_factory.hpp:77] Creating layer relu8
I0428 17:38:40.296753  7412 net.cpp:84] Creating Layer relu8
I0428 17:38:40.296753  7412 net.cpp:406] relu8 <- conv8
I0428 17:38:40.296753  7412 net.cpp:367] relu8 -> conv8 (in-place)
I0428 17:38:40.296753  7412 net.cpp:122] Setting up relu8
I0428 17:38:40.297754  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.297754  7412 net.cpp:137] Memory required for data: 151962800
I0428 17:38:40.297754  7412 layer_factory.hpp:77] Creating layer conv9
I0428 17:38:40.297754  7412 net.cpp:84] Creating Layer conv9
I0428 17:38:40.297754  7412 net.cpp:406] conv9 <- conv8
I0428 17:38:40.297754  7412 net.cpp:380] conv9 -> conv9
I0428 17:38:40.298753  7412 net.cpp:122] Setting up conv9
I0428 17:38:40.298753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.298753  7412 net.cpp:137] Memory required for data: 155239600
I0428 17:38:40.298753  7412 layer_factory.hpp:77] Creating layer bn_conv9
I0428 17:38:40.298753  7412 net.cpp:84] Creating Layer bn_conv9
I0428 17:38:40.298753  7412 net.cpp:406] bn_conv9 <- conv9
I0428 17:38:40.298753  7412 net.cpp:367] bn_conv9 -> conv9 (in-place)
I0428 17:38:40.298753  7412 net.cpp:122] Setting up bn_conv9
I0428 17:38:40.298753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.299753  7412 net.cpp:137] Memory required for data: 158516400
I0428 17:38:40.299753  7412 layer_factory.hpp:77] Creating layer scale_conv9
I0428 17:38:40.299753  7412 net.cpp:84] Creating Layer scale_conv9
I0428 17:38:40.299753  7412 net.cpp:406] scale_conv9 <- conv9
I0428 17:38:40.299753  7412 net.cpp:367] scale_conv9 -> conv9 (in-place)
I0428 17:38:40.299753  7412 layer_factory.hpp:77] Creating layer scale_conv9
I0428 17:38:40.299753  7412 net.cpp:122] Setting up scale_conv9
I0428 17:38:40.299753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.299753  7412 net.cpp:137] Memory required for data: 161793200
I0428 17:38:40.300753  7412 layer_factory.hpp:77] Creating layer relu9
I0428 17:38:40.300753  7412 net.cpp:84] Creating Layer relu9
I0428 17:38:40.300753  7412 net.cpp:406] relu9 <- conv9
I0428 17:38:40.300753  7412 net.cpp:367] relu9 -> conv9 (in-place)
I0428 17:38:40.300753  7412 net.cpp:122] Setting up relu9
I0428 17:38:40.300753  7412 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0428 17:38:40.300753  7412 net.cpp:137] Memory required for data: 165070000
I0428 17:38:40.300753  7412 layer_factory.hpp:77] Creating layer conv10
I0428 17:38:40.300753  7412 net.cpp:84] Creating Layer conv10
I0428 17:38:40.300753  7412 net.cpp:406] conv10 <- conv9
I0428 17:38:40.301754  7412 net.cpp:380] conv10 -> conv10
I0428 17:38:40.301754  7412 net.cpp:122] Setting up conv10
I0428 17:38:40.301754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.301754  7412 net.cpp:137] Memory required for data: 165889200
I0428 17:38:40.301754  7412 layer_factory.hpp:77] Creating layer bn_conv10
I0428 17:38:40.301754  7412 net.cpp:84] Creating Layer bn_conv10
I0428 17:38:40.301754  7412 net.cpp:406] bn_conv10 <- conv10
I0428 17:38:40.301754  7412 net.cpp:367] bn_conv10 -> conv10 (in-place)
I0428 17:38:40.302754  7412 net.cpp:122] Setting up bn_conv10
I0428 17:38:40.302754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.302754  7412 net.cpp:137] Memory required for data: 166708400
I0428 17:38:40.302754  7412 layer_factory.hpp:77] Creating layer scale_conv10
I0428 17:38:40.302754  7412 net.cpp:84] Creating Layer scale_conv10
I0428 17:38:40.302754  7412 net.cpp:406] scale_conv10 <- conv10
I0428 17:38:40.302754  7412 net.cpp:367] scale_conv10 -> conv10 (in-place)
I0428 17:38:40.303755  7412 layer_factory.hpp:77] Creating layer scale_conv10
I0428 17:38:40.303755  7412 net.cpp:122] Setting up scale_conv10
I0428 17:38:40.303755  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.303755  7412 net.cpp:137] Memory required for data: 167527600
I0428 17:38:40.303755  7412 layer_factory.hpp:77] Creating layer relu10
I0428 17:38:40.303755  7412 net.cpp:84] Creating Layer relu10
I0428 17:38:40.303755  7412 net.cpp:406] relu10 <- conv10
I0428 17:38:40.303755  7412 net.cpp:367] relu10 -> conv10 (in-place)
I0428 17:38:40.303755  7412 net.cpp:122] Setting up relu10
I0428 17:38:40.303755  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.304754  7412 net.cpp:137] Memory required for data: 168346800
I0428 17:38:40.304754  7412 layer_factory.hpp:77] Creating layer conv11
I0428 17:38:40.304754  7412 net.cpp:84] Creating Layer conv11
I0428 17:38:40.304754  7412 net.cpp:406] conv11 <- conv10
I0428 17:38:40.304754  7412 net.cpp:380] conv11 -> conv11
I0428 17:38:40.304754  7412 net.cpp:122] Setting up conv11
I0428 17:38:40.304754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.304754  7412 net.cpp:137] Memory required for data: 169166000
I0428 17:38:40.305754  7412 layer_factory.hpp:77] Creating layer bn_conv11
I0428 17:38:40.305754  7412 net.cpp:84] Creating Layer bn_conv11
I0428 17:38:40.305754  7412 net.cpp:406] bn_conv11 <- conv11
I0428 17:38:40.305754  7412 net.cpp:367] bn_conv11 -> conv11 (in-place)
I0428 17:38:40.305754  7412 net.cpp:122] Setting up bn_conv11
I0428 17:38:40.305754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.305754  7412 net.cpp:137] Memory required for data: 169985200
I0428 17:38:40.305754  7412 layer_factory.hpp:77] Creating layer scale_conv11
I0428 17:38:40.305754  7412 net.cpp:84] Creating Layer scale_conv11
I0428 17:38:40.305754  7412 net.cpp:406] scale_conv11 <- conv11
I0428 17:38:40.306754  7412 net.cpp:367] scale_conv11 -> conv11 (in-place)
I0428 17:38:40.306754  7412 layer_factory.hpp:77] Creating layer scale_conv11
I0428 17:38:40.306754  7412 net.cpp:122] Setting up scale_conv11
I0428 17:38:40.306754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.306754  7412 net.cpp:137] Memory required for data: 170804400
I0428 17:38:40.306754  7412 layer_factory.hpp:77] Creating layer relu11
I0428 17:38:40.306754  7412 net.cpp:84] Creating Layer relu11
I0428 17:38:40.306754  7412 net.cpp:406] relu11 <- conv11
I0428 17:38:40.306754  7412 net.cpp:367] relu11 -> conv11 (in-place)
I0428 17:38:40.306754  7412 net.cpp:122] Setting up relu11
I0428 17:38:40.307754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.307754  7412 net.cpp:137] Memory required for data: 171623600
I0428 17:38:40.307754  7412 layer_factory.hpp:77] Creating layer conv12
I0428 17:38:40.307754  7412 net.cpp:84] Creating Layer conv12
I0428 17:38:40.307754  7412 net.cpp:406] conv12 <- conv11
I0428 17:38:40.307754  7412 net.cpp:380] conv12 -> conv12
I0428 17:38:40.307754  7412 net.cpp:122] Setting up conv12
I0428 17:38:40.307754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.308754  7412 net.cpp:137] Memory required for data: 172442800
I0428 17:38:40.308754  7412 layer_factory.hpp:77] Creating layer bn_conv12
I0428 17:38:40.308754  7412 net.cpp:84] Creating Layer bn_conv12
I0428 17:38:40.308754  7412 net.cpp:406] bn_conv12 <- conv12
I0428 17:38:40.308754  7412 net.cpp:367] bn_conv12 -> conv12 (in-place)
I0428 17:38:40.308754  7412 net.cpp:122] Setting up bn_conv12
I0428 17:38:40.308754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.308754  7412 net.cpp:137] Memory required for data: 173262000
I0428 17:38:40.308754  7412 layer_factory.hpp:77] Creating layer scale_conv12
I0428 17:38:40.308754  7412 net.cpp:84] Creating Layer scale_conv12
I0428 17:38:40.309754  7412 net.cpp:406] scale_conv12 <- conv12
I0428 17:38:40.309754  7412 net.cpp:367] scale_conv12 -> conv12 (in-place)
I0428 17:38:40.309754  7412 layer_factory.hpp:77] Creating layer scale_conv12
I0428 17:38:40.309754  7412 net.cpp:122] Setting up scale_conv12
I0428 17:38:40.309754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.309754  7412 net.cpp:137] Memory required for data: 174081200
I0428 17:38:40.309754  7412 layer_factory.hpp:77] Creating layer relu12
I0428 17:38:40.309754  7412 net.cpp:84] Creating Layer relu12
I0428 17:38:40.309754  7412 net.cpp:406] relu12 <- conv12
I0428 17:38:40.310755  7412 net.cpp:367] relu12 -> conv12 (in-place)
I0428 17:38:40.310755  7412 net.cpp:122] Setting up relu12
I0428 17:38:40.310755  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.310755  7412 net.cpp:137] Memory required for data: 174900400
I0428 17:38:40.310755  7412 layer_factory.hpp:77] Creating layer conv13
I0428 17:38:40.310755  7412 net.cpp:84] Creating Layer conv13
I0428 17:38:40.310755  7412 net.cpp:406] conv13 <- conv12
I0428 17:38:40.310755  7412 net.cpp:380] conv13 -> conv13
I0428 17:38:40.311754  7412 net.cpp:122] Setting up conv13
I0428 17:38:40.311754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.311754  7412 net.cpp:137] Memory required for data: 175719600
I0428 17:38:40.311754  7412 layer_factory.hpp:77] Creating layer bn_conv13
I0428 17:38:40.311754  7412 net.cpp:84] Creating Layer bn_conv13
I0428 17:38:40.311754  7412 net.cpp:406] bn_conv13 <- conv13
I0428 17:38:40.311754  7412 net.cpp:367] bn_conv13 -> conv13 (in-place)
I0428 17:38:40.311754  7412 net.cpp:122] Setting up bn_conv13
I0428 17:38:40.311754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.312754  7412 net.cpp:137] Memory required for data: 176538800
I0428 17:38:40.312754  7412 layer_factory.hpp:77] Creating layer scale_conv13
I0428 17:38:40.312754  7412 net.cpp:84] Creating Layer scale_conv13
I0428 17:38:40.312754  7412 net.cpp:406] scale_conv13 <- conv13
I0428 17:38:40.312754  7412 net.cpp:367] scale_conv13 -> conv13 (in-place)
I0428 17:38:40.312754  7412 layer_factory.hpp:77] Creating layer scale_conv13
I0428 17:38:40.312754  7412 net.cpp:122] Setting up scale_conv13
I0428 17:38:40.312754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.312754  7412 net.cpp:137] Memory required for data: 177358000
I0428 17:38:40.312754  7412 layer_factory.hpp:77] Creating layer relu13
I0428 17:38:40.313755  7412 net.cpp:84] Creating Layer relu13
I0428 17:38:40.313755  7412 net.cpp:406] relu13 <- conv13
I0428 17:38:40.313755  7412 net.cpp:367] relu13 -> conv13 (in-place)
I0428 17:38:40.313755  7412 net.cpp:122] Setting up relu13
I0428 17:38:40.313755  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.313755  7412 net.cpp:137] Memory required for data: 178177200
I0428 17:38:40.313755  7412 layer_factory.hpp:77] Creating layer conv14
I0428 17:38:40.313755  7412 net.cpp:84] Creating Layer conv14
I0428 17:38:40.313755  7412 net.cpp:406] conv14 <- conv13
I0428 17:38:40.313755  7412 net.cpp:380] conv14 -> conv14
I0428 17:38:40.314754  7412 net.cpp:122] Setting up conv14
I0428 17:38:40.314754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.314754  7412 net.cpp:137] Memory required for data: 178996400
I0428 17:38:40.314754  7412 layer_factory.hpp:77] Creating layer bn_conv14
I0428 17:38:40.314754  7412 net.cpp:84] Creating Layer bn_conv14
I0428 17:38:40.314754  7412 net.cpp:406] bn_conv14 <- conv14
I0428 17:38:40.314754  7412 net.cpp:367] bn_conv14 -> conv14 (in-place)
I0428 17:38:40.314754  7412 net.cpp:122] Setting up bn_conv14
I0428 17:38:40.315754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.315754  7412 net.cpp:137] Memory required for data: 179815600
I0428 17:38:40.315754  7412 layer_factory.hpp:77] Creating layer scale_conv14
I0428 17:38:40.315754  7412 net.cpp:84] Creating Layer scale_conv14
I0428 17:38:40.315754  7412 net.cpp:406] scale_conv14 <- conv14
I0428 17:38:40.315754  7412 net.cpp:367] scale_conv14 -> conv14 (in-place)
I0428 17:38:40.315754  7412 layer_factory.hpp:77] Creating layer scale_conv14
I0428 17:38:40.315754  7412 net.cpp:122] Setting up scale_conv14
I0428 17:38:40.315754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.315754  7412 net.cpp:137] Memory required for data: 180634800
I0428 17:38:40.316754  7412 layer_factory.hpp:77] Creating layer relu14
I0428 17:38:40.316754  7412 net.cpp:84] Creating Layer relu14
I0428 17:38:40.316754  7412 net.cpp:406] relu14 <- conv14
I0428 17:38:40.316754  7412 net.cpp:367] relu14 -> conv14 (in-place)
I0428 17:38:40.316754  7412 net.cpp:122] Setting up relu14
I0428 17:38:40.316754  7412 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0428 17:38:40.316754  7412 net.cpp:137] Memory required for data: 181454000
I0428 17:38:40.316754  7412 layer_factory.hpp:77] Creating layer conv15
I0428 17:38:40.316754  7412 net.cpp:84] Creating Layer conv15
I0428 17:38:40.316754  7412 net.cpp:406] conv15 <- conv14
I0428 17:38:40.316754  7412 net.cpp:380] conv15 -> conv15
I0428 17:38:40.317754  7412 net.cpp:122] Setting up conv15
I0428 17:38:40.317754  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.317754  7412 net.cpp:137] Memory required for data: 181658800
I0428 17:38:40.317754  7412 layer_factory.hpp:77] Creating layer bn_conv15
I0428 17:38:40.317754  7412 net.cpp:84] Creating Layer bn_conv15
I0428 17:38:40.317754  7412 net.cpp:406] bn_conv15 <- conv15
I0428 17:38:40.317754  7412 net.cpp:367] bn_conv15 -> conv15 (in-place)
I0428 17:38:40.317754  7412 net.cpp:122] Setting up bn_conv15
I0428 17:38:40.318755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.318755  7412 net.cpp:137] Memory required for data: 181863600
I0428 17:38:40.318755  7412 layer_factory.hpp:77] Creating layer scale_conv15
I0428 17:38:40.318755  7412 net.cpp:84] Creating Layer scale_conv15
I0428 17:38:40.318755  7412 net.cpp:406] scale_conv15 <- conv15
I0428 17:38:40.318755  7412 net.cpp:367] scale_conv15 -> conv15 (in-place)
I0428 17:38:40.318755  7412 layer_factory.hpp:77] Creating layer scale_conv15
I0428 17:38:40.318755  7412 net.cpp:122] Setting up scale_conv15
I0428 17:38:40.318755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.319756  7412 net.cpp:137] Memory required for data: 182068400
I0428 17:38:40.319756  7412 layer_factory.hpp:77] Creating layer relu15
I0428 17:38:40.319756  7412 net.cpp:84] Creating Layer relu15
I0428 17:38:40.319756  7412 net.cpp:406] relu15 <- conv15
I0428 17:38:40.319756  7412 net.cpp:367] relu15 -> conv15 (in-place)
I0428 17:38:40.319756  7412 net.cpp:122] Setting up relu15
I0428 17:38:40.319756  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.319756  7412 net.cpp:137] Memory required for data: 182273200
I0428 17:38:40.319756  7412 layer_factory.hpp:77] Creating layer conv16
I0428 17:38:40.319756  7412 net.cpp:84] Creating Layer conv16
I0428 17:38:40.319756  7412 net.cpp:406] conv16 <- conv15
I0428 17:38:40.320755  7412 net.cpp:380] conv16 -> conv16
I0428 17:38:40.320755  7412 net.cpp:122] Setting up conv16
I0428 17:38:40.320755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.320755  7412 net.cpp:137] Memory required for data: 182478000
I0428 17:38:40.320755  7412 layer_factory.hpp:77] Creating layer bn_conv16
I0428 17:38:40.320755  7412 net.cpp:84] Creating Layer bn_conv16
I0428 17:38:40.320755  7412 net.cpp:406] bn_conv16 <- conv16
I0428 17:38:40.321755  7412 net.cpp:367] bn_conv16 -> conv16 (in-place)
I0428 17:38:40.321755  7412 net.cpp:122] Setting up bn_conv16
I0428 17:38:40.321755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.321755  7412 net.cpp:137] Memory required for data: 182682800
I0428 17:38:40.321755  7412 layer_factory.hpp:77] Creating layer scale_conv16
I0428 17:38:40.321755  7412 net.cpp:84] Creating Layer scale_conv16
I0428 17:38:40.321755  7412 net.cpp:406] scale_conv16 <- conv16
I0428 17:38:40.321755  7412 net.cpp:367] scale_conv16 -> conv16 (in-place)
I0428 17:38:40.322755  7412 layer_factory.hpp:77] Creating layer scale_conv16
I0428 17:38:40.322755  7412 net.cpp:122] Setting up scale_conv16
I0428 17:38:40.322755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.322755  7412 net.cpp:137] Memory required for data: 182887600
I0428 17:38:40.322755  7412 layer_factory.hpp:77] Creating layer relu16
I0428 17:38:40.322755  7412 net.cpp:84] Creating Layer relu16
I0428 17:38:40.322755  7412 net.cpp:406] relu16 <- conv16
I0428 17:38:40.322755  7412 net.cpp:367] relu16 -> conv16 (in-place)
I0428 17:38:40.322755  7412 net.cpp:122] Setting up relu16
I0428 17:38:40.322755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.323755  7412 net.cpp:137] Memory required for data: 183092400
I0428 17:38:40.323755  7412 layer_factory.hpp:77] Creating layer conv17
I0428 17:38:40.323755  7412 net.cpp:84] Creating Layer conv17
I0428 17:38:40.323755  7412 net.cpp:406] conv17 <- conv16
I0428 17:38:40.323755  7412 net.cpp:380] conv17 -> conv17
I0428 17:38:40.323755  7412 net.cpp:122] Setting up conv17
I0428 17:38:40.323755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.323755  7412 net.cpp:137] Memory required for data: 183297200
I0428 17:38:40.324755  7412 layer_factory.hpp:77] Creating layer bn_conv17
I0428 17:38:40.324755  7412 net.cpp:84] Creating Layer bn_conv17
I0428 17:38:40.324755  7412 net.cpp:406] bn_conv17 <- conv17
I0428 17:38:40.324755  7412 net.cpp:367] bn_conv17 -> conv17 (in-place)
I0428 17:38:40.324755  7412 net.cpp:122] Setting up bn_conv17
I0428 17:38:40.324755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.324755  7412 net.cpp:137] Memory required for data: 183502000
I0428 17:38:40.324755  7412 layer_factory.hpp:77] Creating layer scale_conv17
I0428 17:38:40.324755  7412 net.cpp:84] Creating Layer scale_conv17
I0428 17:38:40.325755  7412 net.cpp:406] scale_conv17 <- conv17
I0428 17:38:40.325755  7412 net.cpp:367] scale_conv17 -> conv17 (in-place)
I0428 17:38:40.325755  7412 layer_factory.hpp:77] Creating layer scale_conv17
I0428 17:38:40.325755  7412 net.cpp:122] Setting up scale_conv17
I0428 17:38:40.325755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.325755  7412 net.cpp:137] Memory required for data: 183706800
I0428 17:38:40.325755  7412 layer_factory.hpp:77] Creating layer relu17
I0428 17:38:40.326756  7412 net.cpp:84] Creating Layer relu17
I0428 17:38:40.326756  7412 net.cpp:406] relu17 <- conv17
I0428 17:38:40.326756  7412 net.cpp:367] relu17 -> conv17 (in-place)
I0428 17:38:40.326756  7412 net.cpp:122] Setting up relu17
I0428 17:38:40.326756  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.326756  7412 net.cpp:137] Memory required for data: 183911600
I0428 17:38:40.326756  7412 layer_factory.hpp:77] Creating layer conv18
I0428 17:38:40.326756  7412 net.cpp:84] Creating Layer conv18
I0428 17:38:40.326756  7412 net.cpp:406] conv18 <- conv17
I0428 17:38:40.326756  7412 net.cpp:380] conv18 -> conv18
I0428 17:38:40.327755  7412 net.cpp:122] Setting up conv18
I0428 17:38:40.327755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.327755  7412 net.cpp:137] Memory required for data: 184116400
I0428 17:38:40.327755  7412 layer_factory.hpp:77] Creating layer bn_conv17
I0428 17:38:40.327755  7412 net.cpp:84] Creating Layer bn_conv17
I0428 17:38:40.327755  7412 net.cpp:406] bn_conv17 <- conv18
I0428 17:38:40.327755  7412 net.cpp:367] bn_conv17 -> conv18 (in-place)
I0428 17:38:40.327755  7412 net.cpp:122] Setting up bn_conv17
I0428 17:38:40.328755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.328755  7412 net.cpp:137] Memory required for data: 184321200
I0428 17:38:40.328755  7412 layer_factory.hpp:77] Creating layer scale_conv18
I0428 17:38:40.328755  7412 net.cpp:84] Creating Layer scale_conv18
I0428 17:38:40.328755  7412 net.cpp:406] scale_conv18 <- conv18
I0428 17:38:40.328755  7412 net.cpp:367] scale_conv18 -> conv18 (in-place)
I0428 17:38:40.328755  7412 layer_factory.hpp:77] Creating layer scale_conv18
I0428 17:38:40.328755  7412 net.cpp:122] Setting up scale_conv18
I0428 17:38:40.328755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.329756  7412 net.cpp:137] Memory required for data: 184526000
I0428 17:38:40.329756  7412 layer_factory.hpp:77] Creating layer relu18
I0428 17:38:40.329756  7412 net.cpp:84] Creating Layer relu18
I0428 17:38:40.329756  7412 net.cpp:406] relu18 <- conv18
I0428 17:38:40.329756  7412 net.cpp:367] relu18 -> conv18 (in-place)
I0428 17:38:40.329756  7412 net.cpp:122] Setting up relu18
I0428 17:38:40.329756  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.329756  7412 net.cpp:137] Memory required for data: 184730800
I0428 17:38:40.329756  7412 layer_factory.hpp:77] Creating layer conv19
I0428 17:38:40.329756  7412 net.cpp:84] Creating Layer conv19
I0428 17:38:40.330755  7412 net.cpp:406] conv19 <- conv18
I0428 17:38:40.330755  7412 net.cpp:380] conv19 -> conv19
I0428 17:38:40.330755  7412 net.cpp:122] Setting up conv19
I0428 17:38:40.330755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.330755  7412 net.cpp:137] Memory required for data: 184935600
I0428 17:38:40.330755  7412 layer_factory.hpp:77] Creating layer bn_conv19
I0428 17:38:40.330755  7412 net.cpp:84] Creating Layer bn_conv19
I0428 17:38:40.330755  7412 net.cpp:406] bn_conv19 <- conv19
I0428 17:38:40.331755  7412 net.cpp:367] bn_conv19 -> conv19 (in-place)
I0428 17:38:40.331755  7412 net.cpp:122] Setting up bn_conv19
I0428 17:38:40.331755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.331755  7412 net.cpp:137] Memory required for data: 185140400
I0428 17:38:40.331755  7412 layer_factory.hpp:77] Creating layer scale_conv19
I0428 17:38:40.331755  7412 net.cpp:84] Creating Layer scale_conv19
I0428 17:38:40.331755  7412 net.cpp:406] scale_conv19 <- conv19
I0428 17:38:40.331755  7412 net.cpp:367] scale_conv19 -> conv19 (in-place)
I0428 17:38:40.331755  7412 layer_factory.hpp:77] Creating layer scale_conv19
I0428 17:38:40.332756  7412 net.cpp:122] Setting up scale_conv19
I0428 17:38:40.332756  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.332756  7412 net.cpp:137] Memory required for data: 185345200
I0428 17:38:40.332756  7412 layer_factory.hpp:77] Creating layer relu19
I0428 17:38:40.332756  7412 net.cpp:84] Creating Layer relu19
I0428 17:38:40.332756  7412 net.cpp:406] relu19 <- conv19
I0428 17:38:40.332756  7412 net.cpp:367] relu19 -> conv19 (in-place)
I0428 17:38:40.332756  7412 net.cpp:122] Setting up relu19
I0428 17:38:40.332756  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.332756  7412 net.cpp:137] Memory required for data: 185550000
I0428 17:38:40.333755  7412 layer_factory.hpp:77] Creating layer conv20
I0428 17:38:40.333755  7412 net.cpp:84] Creating Layer conv20
I0428 17:38:40.333755  7412 net.cpp:406] conv20 <- conv19
I0428 17:38:40.333755  7412 net.cpp:380] conv20 -> conv20
I0428 17:38:40.334755  7412 net.cpp:122] Setting up conv20
I0428 17:38:40.334755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.334755  7412 net.cpp:137] Memory required for data: 185754800
I0428 17:38:40.334755  7412 layer_factory.hpp:77] Creating layer bn_conv20
I0428 17:38:40.334755  7412 net.cpp:84] Creating Layer bn_conv20
I0428 17:38:40.334755  7412 net.cpp:406] bn_conv20 <- conv20
I0428 17:38:40.334755  7412 net.cpp:367] bn_conv20 -> conv20 (in-place)
I0428 17:38:40.334755  7412 net.cpp:122] Setting up bn_conv20
I0428 17:38:40.334755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.335755  7412 net.cpp:137] Memory required for data: 185959600
I0428 17:38:40.335755  7412 layer_factory.hpp:77] Creating layer scale_conv20
I0428 17:38:40.335755  7412 net.cpp:84] Creating Layer scale_conv20
I0428 17:38:40.335755  7412 net.cpp:406] scale_conv20 <- conv20
I0428 17:38:40.335755  7412 net.cpp:367] scale_conv20 -> conv20 (in-place)
I0428 17:38:40.335755  7412 layer_factory.hpp:77] Creating layer scale_conv20
I0428 17:38:40.335755  7412 net.cpp:122] Setting up scale_conv20
I0428 17:38:40.335755  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.335755  7412 net.cpp:137] Memory required for data: 186164400
I0428 17:38:40.335755  7412 layer_factory.hpp:77] Creating layer relu20
I0428 17:38:40.336756  7412 net.cpp:84] Creating Layer relu20
I0428 17:38:40.336756  7412 net.cpp:406] relu20 <- conv20
I0428 17:38:40.336756  7412 net.cpp:367] relu20 -> conv20 (in-place)
I0428 17:38:40.336756  7412 net.cpp:122] Setting up relu20
I0428 17:38:40.336756  7412 net.cpp:129] Top shape: 100 32 4 4 (51200)
I0428 17:38:40.336756  7412 net.cpp:137] Memory required for data: 186369200
I0428 17:38:40.336756  7412 layer_factory.hpp:77] Creating layer pool21
I0428 17:38:40.336756  7412 net.cpp:84] Creating Layer pool21
I0428 17:38:40.336756  7412 net.cpp:406] pool21 <- conv20
I0428 17:38:40.336756  7412 net.cpp:380] pool21 -> pool21
I0428 17:38:40.336756  7412 net.cpp:122] Setting up pool21
I0428 17:38:40.337756  7412 net.cpp:129] Top shape: 100 32 1 1 (3200)
I0428 17:38:40.337756  7412 net.cpp:137] Memory required for data: 186382000
I0428 17:38:40.337756  7412 layer_factory.hpp:77] Creating layer fc22
I0428 17:38:40.337756  7412 net.cpp:84] Creating Layer fc22
I0428 17:38:40.337756  7412 net.cpp:406] fc22 <- pool21
I0428 17:38:40.337756  7412 net.cpp:380] fc22 -> fc22
I0428 17:38:40.337756  7412 net.cpp:122] Setting up fc22
I0428 17:38:40.337756  7412 net.cpp:129] Top shape: 100 10 (1000)
I0428 17:38:40.337756  7412 net.cpp:137] Memory required for data: 186386000
I0428 17:38:40.338757  7412 layer_factory.hpp:77] Creating layer fc22_fc22_0_split
I0428 17:38:40.338757  7412 net.cpp:84] Creating Layer fc22_fc22_0_split
I0428 17:38:40.338757  7412 net.cpp:406] fc22_fc22_0_split <- fc22
I0428 17:38:40.338757  7412 net.cpp:380] fc22_fc22_0_split -> fc22_fc22_0_split_0
I0428 17:38:40.338757  7412 net.cpp:380] fc22_fc22_0_split -> fc22_fc22_0_split_1
I0428 17:38:40.338757  7412 net.cpp:122] Setting up fc22_fc22_0_split
I0428 17:38:40.338757  7412 net.cpp:129] Top shape: 100 10 (1000)
I0428 17:38:40.338757  7412 net.cpp:129] Top shape: 100 10 (1000)
I0428 17:38:40.339756  7412 net.cpp:137] Memory required for data: 186394000
I0428 17:38:40.339756  7412 layer_factory.hpp:77] Creating layer accuracy
I0428 17:38:40.339756  7412 net.cpp:84] Creating Layer accuracy
I0428 17:38:40.339756  7412 net.cpp:406] accuracy <- fc22_fc22_0_split_0
I0428 17:38:40.339756  7412 net.cpp:406] accuracy <- label_data_1_split_0
I0428 17:38:40.339756  7412 net.cpp:380] accuracy -> accuracy
I0428 17:38:40.339756  7412 net.cpp:122] Setting up accuracy
I0428 17:38:40.339756  7412 net.cpp:129] Top shape: (1)
I0428 17:38:40.339756  7412 net.cpp:137] Memory required for data: 186394004
I0428 17:38:40.339756  7412 layer_factory.hpp:77] Creating layer loss
I0428 17:38:40.339756  7412 net.cpp:84] Creating Layer loss
I0428 17:38:40.340756  7412 net.cpp:406] loss <- fc22_fc22_0_split_1
I0428 17:38:40.340756  7412 net.cpp:406] loss <- label_data_1_split_1
I0428 17:38:40.340756  7412 net.cpp:380] loss -> loss
I0428 17:38:40.340756  7412 layer_factory.hpp:77] Creating layer loss
I0428 17:38:40.340756  7412 net.cpp:122] Setting up loss
I0428 17:38:40.340756  7412 net.cpp:129] Top shape: (1)
I0428 17:38:40.340756  7412 net.cpp:132]     with loss weight 1
I0428 17:38:40.341756  7412 net.cpp:137] Memory required for data: 186394008
I0428 17:38:40.341756  7412 net.cpp:198] loss needs backward computation.
I0428 17:38:40.341756  7412 net.cpp:200] accuracy does not need backward computation.
I0428 17:38:40.341756  7412 net.cpp:198] fc22_fc22_0_split needs backward computation.
I0428 17:38:40.341756  7412 net.cpp:198] fc22 needs backward computation.
I0428 17:38:40.341756  7412 net.cpp:198] pool21 needs backward computation.
I0428 17:38:40.341756  7412 net.cpp:198] relu20 needs backward computation.
I0428 17:38:40.341756  7412 net.cpp:198] scale_conv20 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] bn_conv20 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] conv20 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] relu19 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] scale_conv19 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] bn_conv19 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] conv19 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] relu18 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] scale_conv18 needs backward computation.
I0428 17:38:40.342756  7412 net.cpp:198] bn_conv17 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] conv18 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] relu17 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] scale_conv17 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] bn_conv17 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] conv17 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] relu16 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] scale_conv16 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] bn_conv16 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] conv16 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] relu15 needs backward computation.
I0428 17:38:40.343756  7412 net.cpp:198] scale_conv15 needs backward computation.
I0428 17:38:40.344756  7412 net.cpp:198] bn_conv15 needs backward computation.
I0428 17:38:40.344756  7412 net.cpp:198] conv15 needs backward computation.
I0428 17:38:40.344756  7412 net.cpp:198] relu14 needs backward computation.
I0428 17:38:40.344756  7412 net.cpp:198] scale_conv14 needs backward computation.
I0428 17:38:40.344756  7412 net.cpp:198] bn_conv14 needs backward computation.
I0428 17:38:40.344756  7412 net.cpp:198] conv14 needs backward computation.
I0428 17:38:40.344756  7412 net.cpp:198] relu13 needs backward computation.
I0428 17:38:40.344756  7412 net.cpp:198] scale_conv13 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] bn_conv13 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] conv13 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] relu12 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] scale_conv12 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] bn_conv12 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] conv12 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] relu11 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] scale_conv11 needs backward computation.
I0428 17:38:40.345757  7412 net.cpp:198] bn_conv11 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] conv11 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] relu10 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] scale_conv10 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] bn_conv10 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] conv10 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] relu9 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] scale_conv9 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] bn_conv9 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] conv9 needs backward computation.
I0428 17:38:40.346756  7412 net.cpp:198] relu8 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] scale_conv8 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] bn_conv8 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] conv8 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] relu7 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] scale_conv7 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] bn_conv7 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] conv7 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] relu6 needs backward computation.
I0428 17:38:40.347756  7412 net.cpp:198] scale_conv6 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] bn_conv6 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] conv6 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] relu5 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] scale_conv5 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] bn_conv5 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] conv5 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] relu4 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] scale_conv4 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] bn_conv4 needs backward computation.
I0428 17:38:40.348757  7412 net.cpp:198] conv4 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] relu3 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] scale_conv3 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] bn_conv3 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] conv3 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] relu2 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] scale_conv2 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] bn_conv2 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] conv2 needs backward computation.
I0428 17:38:40.349756  7412 net.cpp:198] relu1 needs backward computation.
I0428 17:38:40.350756  7412 net.cpp:198] scale_conv1 needs backward computation.
I0428 17:38:40.350756  7412 net.cpp:198] bn_conv1 needs backward computation.
I0428 17:38:40.350756  7412 net.cpp:198] conv1 needs backward computation.
I0428 17:38:40.350756  7412 net.cpp:200] label_data_1_split does not need backward computation.
I0428 17:38:40.350756  7412 net.cpp:200] data does not need backward computation.
I0428 17:38:40.350756  7412 net.cpp:242] This network produces output accuracy
I0428 17:38:40.351757  7412 net.cpp:242] This network produces output loss
I0428 17:38:40.351757  7412 net.cpp:255] Network initialization done.
I0428 17:38:40.351757  7412 solver.cpp:56] Solver scaffolding done.
I0428 17:38:40.355757  7412 caffe.cpp:249] Starting Optimization
I0428 17:38:40.355757  7412 solver.cpp:278] Solving CaffeNet
I0428 17:38:40.355757  7412 solver.cpp:279] Learning Rate Policy: multistep
I0428 17:38:40.358757  7412 solver.cpp:336] Iteration 0, Testing net (#0)
I0428 17:38:47.402160 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:38:47.690176  7412 solver.cpp:403]     Test net output #0: accuracy = 0.0884
I0428 17:38:47.690176  7412 solver.cpp:403]     Test net output #1: loss = 2.3883 (* 1 = 2.3883 loss)
I0428 17:38:47.967192  7412 solver.cpp:224] Iteration 0 (-8.40088e-37 iter/s, 7.61193s/100 iters), loss = 2.36606
I0428 17:38:47.968192  7412 solver.cpp:243]     Train net output #0: loss = 2.36606 (* 1 = 2.36606 loss)
I0428 17:38:47.968192  7412 sgd_solver.cpp:137] Iteration 0, lr = 0.01
I0428 17:38:47.982193  7412 sgd_solver.cpp:169] scale layer:1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 
I0428 17:38:47.984194  7412 sgd_solver.cpp:200] weight diff/data:0.019375 0.025785 0.034169 0.042648 0.583336 0.018994 0.013695 0.015352 0.019094 0.015810 0.305293 0.010095 0.004780 0.007382 0.003625 0.003745 0.002918 0.003627 0.003260 0.002651 0.003311 
I0428 17:39:12.592600  7412 solver.cpp:224] Iteration 100 (4.06087 iter/s, 24.6253s/100 iters), loss = 1.95388
I0428 17:39:12.592600  7412 solver.cpp:243]     Train net output #0: loss = 1.95388 (* 1 = 1.95388 loss)
I0428 17:39:12.593601  7412 sgd_solver.cpp:137] Iteration 100, lr = 0.01
I0428 17:39:12.601601  7412 sgd_solver.cpp:169] scale layer:0.995118 0.995322 0.995301 0.995500 0.995442 0.995445 0.995452 0.995460 0.995455 0.995452 0.995452 0.995457 0.995458 0.995459 0.995448 0.995456 0.995455 0.995438 0.995447 1.006077 
I0428 17:39:12.603601  7412 sgd_solver.cpp:200] weight diff/data:0.006767 0.007885 0.005956 0.010685 0.009913 0.016375 0.011643 0.009855 0.018758 0.010262 0.015260 0.010919 0.007997 0.006401 0.014416 0.004924 0.006977 0.004532 0.004964 0.006858 0.007532 
I0428 17:39:37.246011  7412 solver.cpp:224] Iteration 200 (4.05616 iter/s, 24.6539s/100 iters), loss = 1.78105
I0428 17:39:37.246011  7412 solver.cpp:243]     Train net output #0: loss = 1.78105 (* 1 = 1.78105 loss)
I0428 17:39:37.246011  7412 sgd_solver.cpp:137] Iteration 200, lr = 0.01
I0428 17:39:37.255012  7412 sgd_solver.cpp:169] scale layer:0.989634 0.990369 0.990483 0.990469 0.990336 0.990518 0.990485 0.990506 0.990487 0.990467 0.990482 0.990487 0.990489 0.990473 0.990479 0.990475 0.990480 0.990451 0.990449 1.026922 
I0428 17:39:37.258011  7412 sgd_solver.cpp:200] weight diff/data:0.011376 0.009265 0.014890 0.008530 0.010920 0.007166 0.012183 0.007736 0.009132 0.007644 0.009012 0.008141 0.007642 0.006564 0.013572 0.006181 0.004412 0.004927 0.005404 0.005314 0.004931 
I0428 17:40:01.846418  7412 solver.cpp:224] Iteration 300 (4.06495 iter/s, 24.6006s/100 iters), loss = 1.40479
I0428 17:40:01.846418  7412 solver.cpp:243]     Train net output #0: loss = 1.40479 (* 1 = 1.40479 loss)
I0428 17:40:01.846418  7412 sgd_solver.cpp:137] Iteration 300, lr = 0.01
I0428 17:40:01.855418  7412 sgd_solver.cpp:169] scale layer:0.984362 0.985411 0.985681 0.985664 0.985399 0.985590 0.985515 0.985568 0.985544 0.985477 0.985516 0.985538 0.985531 0.985492 0.985523 0.985510 0.985531 0.985474 0.985453 1.044098 
I0428 17:40:01.857419  7412 sgd_solver.cpp:200] weight diff/data:0.005001 0.054023 0.015379 0.029606 0.009500 0.031680 0.031164 0.014109 0.011056 0.048741 0.016082 0.010446 0.009233 0.010331 0.011706 0.009543 0.010263 0.005607 0.005421 0.007401 0.005605 
I0428 17:40:25.246757 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:40:26.470826  7412 solver.cpp:224] Iteration 400 (4.06077 iter/s, 24.6259s/100 iters), loss = 1.44623
I0428 17:40:26.471827  7412 solver.cpp:243]     Train net output #0: loss = 1.44623 (* 1 = 1.44623 loss)
I0428 17:40:26.471827  7412 sgd_solver.cpp:137] Iteration 400, lr = 0.01
I0428 17:40:26.479827  7412 sgd_solver.cpp:169] scale layer:0.978722 0.980726 0.981003 0.980806 0.980304 0.980679 0.980568 0.980654 0.980628 0.980570 0.980562 0.980610 0.980575 0.980540 0.980594 0.980580 0.980608 0.980534 0.980499 1.053148 
I0428 17:40:26.482827  7412 sgd_solver.cpp:200] weight diff/data:0.006717 0.012717 0.020072 0.018524 0.016580 0.010611 0.012032 0.015128 0.012075 0.009262 0.011551 0.010784 0.011920 0.006878 0.009265 0.011111 0.030871 0.011633 0.011171 0.008185 0.003620 
I0428 17:40:51.064234  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_500.caffemodel
I0428 17:40:51.074234  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_500.solverstate
I0428 17:40:51.077234  7412 solver.cpp:336] Iteration 500, Testing net (#0)
I0428 17:40:59.839735 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:41:00.202755  7412 solver.cpp:403]     Test net output #0: accuracy = 0.4667
I0428 17:41:00.202755  7412 solver.cpp:403]     Test net output #1: loss = 1.44719 (* 1 = 1.44719 loss)
I0428 17:41:00.437769  7412 solver.cpp:224] Iteration 500 (2.94403 iter/s, 33.967s/100 iters), loss = 1.43034
I0428 17:41:00.437769  7412 solver.cpp:243]     Train net output #0: loss = 1.43034 (* 1 = 1.43034 loss)
I0428 17:41:00.437769  7412 sgd_solver.cpp:137] Iteration 500, lr = 0.01
I0428 17:41:00.446769  7412 sgd_solver.cpp:169] scale layer:0.972556 0.975476 0.975978 0.976042 0.975437 0.975730 0.975630 0.975781 0.975738 0.975681 0.975643 0.975691 0.975664 0.975623 0.975680 0.975674 0.975695 0.975620 0.975587 1.060576 
I0428 17:41:00.447769  7412 sgd_solver.cpp:200] weight diff/data:0.006371 0.011394 0.014593 0.224065 0.014983 0.008910 0.012678 0.012137 0.010478 0.033568 0.014937 0.012477 0.009322 0.036488 0.007396 0.008480 0.010059 0.008784 0.012570 0.006062 0.003106 
I0428 17:41:25.039176  7412 solver.cpp:224] Iteration 600 (4.06466 iter/s, 24.6023s/100 iters), loss = 1.44824
I0428 17:41:25.039176  7412 solver.cpp:243]     Train net output #0: loss = 1.44824 (* 1 = 1.44824 loss)
I0428 17:41:25.039176  7412 sgd_solver.cpp:137] Iteration 600, lr = 0.01
I0428 17:41:25.048177  7412 sgd_solver.cpp:169] scale layer:0.965663 0.970363 0.971266 0.971068 0.970396 0.970847 0.970764 0.970905 0.970850 0.970782 0.970760 0.970832 0.970765 0.970741 0.970799 0.970796 0.970854 0.970739 0.970707 1.067461 
I0428 17:41:25.050177  7412 sgd_solver.cpp:200] weight diff/data:0.006743 0.014285 0.014430 0.048501 0.012147 0.011273 0.013242 0.008262 0.012847 0.014326 0.025295 0.014065 0.013121 0.015002 0.009836 0.009930 0.009339 0.006486 0.007017 0.005396 0.005330 
I0428 17:41:49.753590  7412 solver.cpp:224] Iteration 700 (4.04613 iter/s, 24.7149s/100 iters), loss = 1.13262
I0428 17:41:49.753590  7412 solver.cpp:243]     Train net output #0: loss = 1.13262 (* 1 = 1.13262 loss)
I0428 17:41:49.754590  7412 sgd_solver.cpp:137] Iteration 700, lr = 0.01
I0428 17:41:49.763590  7412 sgd_solver.cpp:169] scale layer:0.957465 0.964624 0.966494 0.966129 0.965472 0.965968 0.965937 0.966063 0.965987 0.965900 0.965912 0.966009 0.965866 0.965875 0.965897 0.965922 0.965995 0.965887 0.965853 1.076314 
I0428 17:41:49.765590  7412 sgd_solver.cpp:200] weight diff/data:0.029408 0.022726 0.017561 0.020312 0.010212 0.011820 0.011610 0.008930 0.009135 0.011525 0.358516 0.010488 0.037195 0.010435 0.014968 0.009405 0.012952 0.012052 0.010215 0.006936 0.005041 
I0428 17:42:13.397943 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:42:14.627012  7412 solver.cpp:224] Iteration 800 (4.02035 iter/s, 24.8734s/100 iters), loss = 1.20951
I0428 17:42:14.628012  7412 solver.cpp:243]     Train net output #0: loss = 1.20951 (* 1 = 1.20951 loss)
I0428 17:42:14.628012  7412 sgd_solver.cpp:137] Iteration 800, lr = 0.01
I0428 17:42:14.637013  7412 sgd_solver.cpp:169] scale layer:0.950190 0.959194 0.961463 0.961176 0.960504 0.961113 0.961137 0.961251 0.961141 0.961062 0.961040 0.961221 0.961033 0.961058 0.961050 0.961083 0.961174 0.961095 0.961061 1.079396 
I0428 17:42:14.640013  7412 sgd_solver.cpp:200] weight diff/data:0.007581 0.022033 0.009088 0.014454 0.018239 0.013493 0.022789 0.009082 0.011991 0.011757 0.014675 0.011436 0.010828 0.018463 0.039818 0.007782 0.008375 0.007951 0.007762 0.008466 0.003615 
I0428 17:42:39.362427  7412 solver.cpp:224] Iteration 900 (4.04284 iter/s, 24.7351s/100 iters), loss = 1.11296
I0428 17:42:39.362427  7412 solver.cpp:243]     Train net output #0: loss = 1.11296 (* 1 = 1.11296 loss)
I0428 17:42:39.363427  7412 sgd_solver.cpp:137] Iteration 900, lr = 0.01
I0428 17:42:39.371428  7412 sgd_solver.cpp:169] scale layer:0.942527 0.953376 0.956404 0.956470 0.955326 0.956348 0.956340 0.956494 0.956365 0.956215 0.956283 0.956413 0.956224 0.956268 0.956197 0.956291 0.956342 0.956339 0.956297 1.083237 
I0428 17:42:39.373428  7412 sgd_solver.cpp:200] weight diff/data:0.010833 0.017252 0.011705 0.032659 0.023749 0.007884 0.082466 0.020684 0.010029 0.013589 0.010788 0.013015 0.013674 0.009762 0.008740 0.008034 0.066303 0.011909 0.006957 0.007639 0.006565 
I0428 17:43:03.702819  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_1000.caffemodel
I0428 17:43:03.711820  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_1000.solverstate
I0428 17:43:03.714820  7412 solver.cpp:336] Iteration 1000, Testing net (#0)
I0428 17:43:12.486322 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:43:12.850342  7412 solver.cpp:403]     Test net output #0: accuracy = 0.5943
I0428 17:43:12.850342  7412 solver.cpp:403]     Test net output #1: loss = 1.13748 (* 1 = 1.13748 loss)
I0428 17:43:13.082356  7412 solver.cpp:224] Iteration 1000 (2.96555 iter/s, 33.7205s/100 iters), loss = 1.18289
I0428 17:43:13.082356  7412 solver.cpp:243]     Train net output #0: loss = 1.18289 (* 1 = 1.18289 loss)
I0428 17:43:13.083356  7412 sgd_solver.cpp:137] Iteration 1000, lr = 0.01
I0428 17:43:13.092356  7412 sgd_solver.cpp:169] scale layer:0.934086 0.946540 0.951204 0.951134 0.950032 0.951623 0.951566 0.951730 0.951623 0.951404 0.951431 0.951637 0.951446 0.951489 0.951403 0.951501 0.951595 0.951581 0.951561 1.087166 
I0428 17:43:13.093356  7412 sgd_solver.cpp:200] weight diff/data:0.005373 0.116631 0.007341 0.014156 0.008255 0.009319 0.008907 0.010807 0.007127 0.007894 0.010229 0.012853 0.010952 0.010174 0.009933 0.009454 0.009611 0.011671 0.008212 0.012593 0.004193 
I0428 17:43:37.670763  7412 solver.cpp:224] Iteration 1100 (4.06693 iter/s, 24.5886s/100 iters), loss = 1.01571
I0428 17:43:37.670763  7412 solver.cpp:243]     Train net output #0: loss = 1.01571 (* 1 = 1.01571 loss)
I0428 17:43:37.670763  7412 sgd_solver.cpp:137] Iteration 1100, lr = 0.01
I0428 17:43:37.679764  7412 sgd_solver.cpp:169] scale layer:0.926889 0.940751 0.946018 0.945954 0.945481 0.946955 0.946830 0.946958 0.946834 0.946620 0.946651 0.946934 0.946748 0.946720 0.946655 0.946721 0.946823 0.946836 0.946818 1.095432 
I0428 17:43:37.681763  7412 sgd_solver.cpp:200] weight diff/data:0.010022 0.016086 0.009237 0.013768 0.016555 0.014825 0.013489 0.017630 0.021694 0.012921 0.013770 0.010208 0.009984 0.009167 0.010891 0.010774 0.008936 0.006715 0.006553 0.005470 0.004448 
I0428 17:44:01.047099 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:44:02.271169  7412 solver.cpp:224] Iteration 1200 (4.065 iter/s, 24.6003s/100 iters), loss = 1.17465
I0428 17:44:02.271169  7412 solver.cpp:243]     Train net output #0: loss = 1.17465 (* 1 = 1.17465 loss)
I0428 17:44:02.271169  7412 sgd_solver.cpp:137] Iteration 1200, lr = 0.01
I0428 17:44:02.280170  7412 sgd_solver.cpp:169] scale layer:0.920083 0.935414 0.941513 0.941443 0.940859 0.942190 0.942104 0.942236 0.942101 0.941885 0.941923 0.942257 0.941994 0.942017 0.941922 0.942007 0.942093 0.942118 0.942128 1.095906 
I0428 17:44:02.282171  7412 sgd_solver.cpp:200] weight diff/data:0.007584 0.013673 0.011455 0.023421 0.009595 0.012770 0.012775 0.066465 0.011973 0.015002 0.013845 0.238893 0.014367 0.010506 0.014282 0.022367 0.021486 0.010675 0.007075 0.006063 0.005596 
I0428 17:44:26.892577  7412 solver.cpp:224] Iteration 1300 (4.06143 iter/s, 24.6219s/100 iters), loss = 0.904582
I0428 17:44:26.892577  7412 solver.cpp:243]     Train net output #0: loss = 0.904582 (* 1 = 0.904582 loss)
I0428 17:44:26.892577  7412 sgd_solver.cpp:137] Iteration 1300, lr = 0.01
I0428 17:44:26.902578  7412 sgd_solver.cpp:169] scale layer:0.911852 0.929765 0.936421 0.936140 0.936004 0.937545 0.937500 0.937571 0.937281 0.937154 0.937170 0.937562 0.937286 0.937314 0.937174 0.937278 0.937385 0.937428 0.937456 1.097333 
I0428 17:44:26.904578  7412 sgd_solver.cpp:200] weight diff/data:0.012580 0.008404 0.010318 0.015361 0.026786 0.007326 0.032830 0.015209 0.011740 0.015541 0.037922 0.012130 0.090318 0.034570 0.009607 0.011146 0.016511 0.009076 0.012049 0.009347 0.003996 
I0428 17:44:51.531987  7412 solver.cpp:224] Iteration 1400 (4.05846 iter/s, 24.6399s/100 iters), loss = 0.995148
I0428 17:44:51.531987  7412 solver.cpp:243]     Train net output #0: loss = 0.995148 (* 1 = 0.995148 loss)
I0428 17:44:51.531987  7412 sgd_solver.cpp:137] Iteration 1400, lr = 0.01
I0428 17:44:51.540987  7412 sgd_solver.cpp:169] scale layer:0.903621 0.923746 0.931521 0.931883 0.930983 0.932828 0.932888 0.932894 0.932573 0.932552 0.932586 0.932903 0.932581 0.932670 0.932518 0.932656 0.932712 0.932737 0.932842 1.098548 
I0428 17:44:51.542987  7412 sgd_solver.cpp:200] weight diff/data:0.010660 0.011346 0.010603 0.019035 0.017011 0.010186 0.020885 0.010933 0.010434 0.010963 0.012817 0.014460 0.009802 0.014939 0.009471 0.008661 0.009441 0.014763 0.015319 0.004882 0.003597 
I0428 17:45:15.897380  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_1500.caffemodel
I0428 17:45:15.906381  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_1500.solverstate
I0428 17:45:15.909381  7412 solver.cpp:336] Iteration 1500, Testing net (#0)
I0428 17:45:24.659883 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:45:25.024902  7412 solver.cpp:403]     Test net output #0: accuracy = 0.6491
I0428 17:45:25.024902  7412 solver.cpp:403]     Test net output #1: loss = 0.985407 (* 1 = 0.985407 loss)
I0428 17:45:25.258916  7412 solver.cpp:224] Iteration 1500 (2.96499 iter/s, 33.727s/100 iters), loss = 0.852067
I0428 17:45:25.258916  7412 solver.cpp:243]     Train net output #0: loss = 0.852067 (* 1 = 0.852067 loss)
I0428 17:45:25.258916  7412 sgd_solver.cpp:137] Iteration 1500, lr = 0.01
I0428 17:45:25.266916  7412 sgd_solver.cpp:169] scale layer:0.897489 0.918142 0.926767 0.926989 0.926096 0.928441 0.928222 0.928273 0.927959 0.927866 0.927908 0.928263 0.927927 0.928011 0.927893 0.928016 0.928031 0.928127 0.928212 1.104265 
I0428 17:45:25.268916  7412 sgd_solver.cpp:200] weight diff/data:0.008870 0.011793 0.010916 0.013316 0.011755 0.011935 0.032506 0.008927 0.013889 0.010782 0.014982 0.010914 0.013062 0.020081 0.009549 2.633318 0.011329 0.011632 0.006900 0.010603 0.003691 
I0428 17:45:48.662255 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:45:49.877324  7412 solver.cpp:224] Iteration 1600 (4.06192 iter/s, 24.6189s/100 iters), loss = 1.06259
I0428 17:45:49.877324  7412 solver.cpp:243]     Train net output #0: loss = 1.06259 (* 1 = 1.06259 loss)
I0428 17:45:49.877324  7412 sgd_solver.cpp:137] Iteration 1600, lr = 0.01
I0428 17:45:49.886324  7412 sgd_solver.cpp:169] scale layer:0.891052 0.912986 0.921891 0.921943 0.921124 0.923891 0.923678 0.923611 0.923211 0.923259 0.923299 0.923671 0.923326 0.923441 0.923213 0.923401 0.923383 0.923497 0.923621 1.104009 
I0428 17:45:49.887325  7412 sgd_solver.cpp:200] weight diff/data:0.017923 0.022741 0.020288 0.020056 0.019536 0.011308 0.011887 0.008423 0.012815 0.012480 0.014536 0.017298 0.019656 0.013989 0.012757 0.017745 0.013421 0.009847 0.087144 0.005034 0.012845 
I0428 17:46:14.477731  7412 solver.cpp:224] Iteration 1700 (4.065 iter/s, 24.6003s/100 iters), loss = 0.769954
I0428 17:46:14.477731  7412 solver.cpp:243]     Train net output #0: loss = 0.769954 (* 1 = 0.769954 loss)
I0428 17:46:14.477731  7412 sgd_solver.cpp:137] Iteration 1700, lr = 0.01
I0428 17:46:14.486732  7412 sgd_solver.cpp:169] scale layer:0.884220 0.907496 0.917378 0.917564 0.916553 0.919234 0.919064 0.919025 0.918628 0.918611 0.918752 0.919088 0.918727 0.918778 0.918527 0.918834 0.918805 0.918904 0.919054 1.105058 
I0428 17:46:14.488732  7412 sgd_solver.cpp:200] weight diff/data:0.010967 0.012823 0.013321 0.012981 0.015272 0.011256 0.064838 0.011047 0.060409 0.026564 0.011475 0.024737 0.112151 0.013766 0.018659 0.008372 0.012170 0.007484 0.008240 0.006557 0.002738 
I0428 17:46:39.063138  7412 solver.cpp:224] Iteration 1800 (4.06735 iter/s, 24.586s/100 iters), loss = 0.902657
I0428 17:46:39.063138  7412 solver.cpp:243]     Train net output #0: loss = 0.902657 (* 1 = 0.902657 loss)
I0428 17:46:39.063138  7412 sgd_solver.cpp:137] Iteration 1800, lr = 0.01
I0428 17:46:39.072139  7412 sgd_solver.cpp:169] scale layer:0.876854 0.901880 0.912843 0.913077 0.911249 0.914670 0.914529 0.914511 0.914026 0.914078 0.914194 0.914480 0.914156 0.914239 0.914085 0.914335 0.914258 0.914357 0.914476 1.105491 
I0428 17:46:39.074138  7412 sgd_solver.cpp:200] weight diff/data:0.012277 0.008383 0.017061 0.018280 0.014490 0.010085 0.171877 0.010120 0.012463 0.011892 0.017293 0.014466 0.009274 0.010073 0.010354 0.010498 0.016817 0.011901 0.006593 0.007020 0.002600 
I0428 17:47:03.671545  7412 solver.cpp:224] Iteration 1900 (4.06361 iter/s, 24.6086s/100 iters), loss = 0.84418
I0428 17:47:03.671545  7412 solver.cpp:243]     Train net output #0: loss = 0.84418 (* 1 = 0.84418 loss)
I0428 17:47:03.671545  7412 sgd_solver.cpp:137] Iteration 1900, lr = 0.01
I0428 17:47:03.680546  7412 sgd_solver.cpp:169] scale layer:0.869346 0.895384 0.908003 0.908110 0.906119 0.910232 0.910083 0.909996 0.909528 0.909515 0.909712 0.910015 0.909567 0.909754 0.909465 0.909793 0.909697 0.909802 0.909941 1.109638 
I0428 17:47:03.682545  7412 sgd_solver.cpp:200] weight diff/data:0.007951 0.010200 0.006926 0.014092 0.011473 0.009164 0.009756 0.012888 0.011436 0.012085 0.015291 0.065591 0.012350 0.022866 0.018637 0.013747 0.010558 0.008030 0.012505 0.012320 0.004257 
I0428 17:47:27.042881 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:47:28.021937  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_2000.caffemodel
I0428 17:47:28.030938  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_2000.solverstate
I0428 17:47:28.033938  7412 solver.cpp:336] Iteration 2000, Testing net (#0)
I0428 17:47:36.785439 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:47:37.148459  7412 solver.cpp:403]     Test net output #0: accuracy = 0.6926
I0428 17:47:37.148459  7412 solver.cpp:403]     Test net output #1: loss = 0.874013 (* 1 = 0.874013 loss)
I0428 17:47:37.381474  7412 solver.cpp:224] Iteration 2000 (2.96642 iter/s, 33.7107s/100 iters), loss = 0.972491
I0428 17:47:37.382473  7412 solver.cpp:243]     Train net output #0: loss = 0.972491 (* 1 = 0.972491 loss)
I0428 17:47:37.382473  7412 sgd_solver.cpp:137] Iteration 2000, lr = 0.01
I0428 17:47:37.390473  7412 sgd_solver.cpp:169] scale layer:0.864940 0.889494 0.903209 0.903465 0.900962 0.905567 0.905566 0.905531 0.905086 0.905033 0.905289 0.905561 0.905081 0.905263 0.904957 0.905255 0.905121 0.905292 0.905437 1.108466 
I0428 17:47:37.391474  7412 sgd_solver.cpp:200] weight diff/data:0.006646 0.009940 0.014146 0.012743 0.015547 0.010137 0.018634 0.010132 0.010572 0.011269 0.010718 0.011313 0.022327 0.170049 0.009245 0.009620 0.013648 0.008181 0.006414 0.012345 0.002748 
I0428 17:48:01.986881  7412 solver.cpp:224] Iteration 2100 (4.06414 iter/s, 24.6054s/100 iters), loss = 0.750785
I0428 17:48:01.986881  7412 solver.cpp:243]     Train net output #0: loss = 0.750785 (* 1 = 0.750785 loss)
I0428 17:48:01.987880  7412 sgd_solver.cpp:137] Iteration 2100, lr = 0.01
I0428 17:48:01.996881  7412 sgd_solver.cpp:169] scale layer:0.859096 0.884191 0.898887 0.899304 0.896498 0.901055 0.901053 0.901112 0.900586 0.900442 0.900880 0.901132 0.900605 0.900721 0.900432 0.900730 0.900657 0.900827 0.900968 1.107793 
I0428 17:48:01.998881  7412 sgd_solver.cpp:200] weight diff/data:0.004915 0.008028 0.008521 0.019303 0.009464 0.016753 0.012674 0.009207 0.011599 0.010152 0.015884 0.011223 0.012242 0.010012 0.027242 0.012094 0.012693 0.009862 0.007508 0.009202 0.003506 
I0428 17:48:26.555285  7412 solver.cpp:224] Iteration 2200 (4.07036 iter/s, 24.5679s/100 iters), loss = 0.850472
I0428 17:48:26.555285  7412 solver.cpp:243]     Train net output #0: loss = 0.850472 (* 1 = 0.850472 loss)
I0428 17:48:26.555285  7412 sgd_solver.cpp:137] Iteration 2200, lr = 0.01
I0428 17:48:26.564286  7412 sgd_solver.cpp:169] scale layer:0.852638 0.878649 0.894054 0.894156 0.891276 0.896630 0.896602 0.896647 0.896070 0.895980 0.896542 0.896607 0.896149 0.896225 0.896041 0.896284 0.896113 0.896350 0.896520 1.107941 
I0428 17:48:26.566287  7412 sgd_solver.cpp:200] weight diff/data:0.015945 0.012458 0.018421 0.011814 0.029472 0.011638 0.010637 0.011563 0.016862 0.012546 0.014025 0.013262 0.013379 0.012736 0.015539 0.011655 0.013661 0.121932 0.007445 0.013295 0.002523 
I0428 17:48:51.176694  7412 solver.cpp:224] Iteration 2300 (4.06145 iter/s, 24.6217s/100 iters), loss = 0.700993
I0428 17:48:51.176694  7412 solver.cpp:243]     Train net output #0: loss = 0.700993 (* 1 = 0.700993 loss)
I0428 17:48:51.176694  7412 sgd_solver.cpp:137] Iteration 2300, lr = 0.01
I0428 17:48:51.185694  7412 sgd_solver.cpp:169] scale layer:0.845354 0.872116 0.889557 0.889877 0.886817 0.892172 0.892249 0.892294 0.891664 0.891621 0.892033 0.892171 0.891679 0.891898 0.891577 0.891920 0.891691 0.891871 0.892075 1.110982 
I0428 17:48:51.187695  7412 sgd_solver.cpp:200] weight diff/data:0.006446 0.008040 0.013280 0.028621 0.014370 0.026017 0.012374 0.030706 0.021993 0.009794 0.016929 0.012628 0.015478 0.024023 0.102045 0.026449 0.010802 0.009747 0.008862 0.010937 0.005078 
I0428 17:49:14.578032 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:49:15.816103  7412 solver.cpp:224] Iteration 2400 (4.0584 iter/s, 24.6402s/100 iters), loss = 0.979122
I0428 17:49:15.817103  7412 solver.cpp:243]     Train net output #0: loss = 0.979122 (* 1 = 0.979122 loss)
I0428 17:49:15.817103  7412 sgd_solver.cpp:137] Iteration 2400, lr = 0.01
I0428 17:49:15.825103  7412 sgd_solver.cpp:169] scale layer:0.840926 0.866987 0.884856 0.884920 0.881418 0.887700 0.887853 0.887891 0.887348 0.887285 0.887666 0.887798 0.887282 0.887466 0.887120 0.887493 0.887244 0.887472 0.887648 1.110122 
I0428 17:49:15.827105  7412 sgd_solver.cpp:200] weight diff/data:0.071699 0.010236 0.012488 0.013876 0.011689 0.019752 0.015800 0.012026 0.011563 0.016094 0.017062 0.013670 0.020740 0.016637 0.017735 0.014556 0.013854 0.011241 0.019498 0.019416 0.006239 
I0428 17:49:40.165496  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_2500.caffemodel
I0428 17:49:40.174496  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_2500.solverstate
I0428 17:49:40.177496  7412 solver.cpp:336] Iteration 2500, Testing net (#0)
I0428 17:49:48.950999 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:49:49.312019  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7162
I0428 17:49:49.312019  7412 solver.cpp:403]     Test net output #1: loss = 0.812677 (* 1 = 0.812677 loss)
I0428 17:49:49.545032  7412 solver.cpp:224] Iteration 2500 (2.96478 iter/s, 33.7294s/100 iters), loss = 0.693347
I0428 17:49:49.545032  7412 solver.cpp:243]     Train net output #0: loss = 0.693347 (* 1 = 0.693347 loss)
I0428 17:49:49.545032  7412 sgd_solver.cpp:137] Iteration 2500, lr = 0.01
I0428 17:49:49.554033  7412 sgd_solver.cpp:169] scale layer:0.835063 0.863057 0.880709 0.881299 0.876987 0.883505 0.883439 0.883593 0.882976 0.882929 0.883425 0.883424 0.882923 0.883076 0.882694 0.883114 0.882816 0.883065 0.883276 1.108902 
I0428 17:49:49.556033  7412 sgd_solver.cpp:200] weight diff/data:0.021316 0.009906 0.011494 0.009729 0.009233 0.010992 0.009141 0.018061 0.009535 0.009678 0.011407 0.013901 0.016246 0.012112 0.018603 0.011761 0.019516 0.006991 0.008442 0.009546 0.010146 
I0428 17:50:14.134438  7412 solver.cpp:224] Iteration 2600 (4.06669 iter/s, 24.59s/100 iters), loss = 0.725524
I0428 17:50:14.135439  7412 solver.cpp:243]     Train net output #0: loss = 0.725524 (* 1 = 0.725524 loss)
I0428 17:50:14.135439  7412 sgd_solver.cpp:137] Iteration 2600, lr = 0.01
I0428 17:50:14.143440  7412 sgd_solver.cpp:169] scale layer:0.828631 0.856115 0.876524 0.876876 0.872378 0.879202 0.879196 0.879202 0.878632 0.878465 0.879093 0.879113 0.878534 0.878713 0.878317 0.878770 0.878361 0.878683 0.878958 1.110075 
I0428 17:50:14.145439  7412 sgd_solver.cpp:200] weight diff/data:0.044310 0.007346 0.007858 0.010122 0.011574 0.010875 0.014991 0.023684 0.011191 0.009721 0.016656 0.013238 0.039147 0.013100 0.021053 0.172362 0.009485 0.006993 0.008266 0.007666 0.002970 
I0428 17:50:38.707844  7412 solver.cpp:224] Iteration 2700 (4.06942 iter/s, 24.5736s/100 iters), loss = 0.691779
I0428 17:50:38.707844  7412 solver.cpp:243]     Train net output #0: loss = 0.691779 (* 1 = 0.691779 loss)
I0428 17:50:38.707844  7412 sgd_solver.cpp:137] Iteration 2700, lr = 0.01
I0428 17:50:38.716845  7412 sgd_solver.cpp:169] scale layer:0.823998 0.851735 0.872779 0.872750 0.867441 0.874850 0.874889 0.875005 0.874280 0.874133 0.874848 0.874814 0.874311 0.874517 0.873972 0.874398 0.874064 0.874309 0.874581 1.112758 
I0428 17:50:38.718845  7412 sgd_solver.cpp:200] weight diff/data:0.005487 0.008383 0.009068 0.011333 0.011195 0.016346 0.013347 0.014213 0.008338 0.021402 0.012872 0.009630 0.028153 0.012544 0.015161 0.017050 0.016335 0.014311 0.006514 0.009146 0.005100 
I0428 17:51:02.087182 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:51:03.303251  7412 solver.cpp:224] Iteration 2800 (4.06579 iter/s, 24.5955s/100 iters), loss = 0.938128
I0428 17:51:03.303251  7412 solver.cpp:243]     Train net output #0: loss = 0.938128 (* 1 = 0.938128 loss)
I0428 17:51:03.303251  7412 sgd_solver.cpp:137] Iteration 2800, lr = 0.01
I0428 17:51:03.312252  7412 sgd_solver.cpp:169] scale layer:0.817541 0.846412 0.868667 0.868222 0.862439 0.870628 0.870820 0.870791 0.870087 0.869948 0.870413 0.870535 0.870130 0.870191 0.869535 0.870081 0.869696 0.869982 0.870278 1.108057 
I0428 17:51:03.314251  7412 sgd_solver.cpp:200] weight diff/data:0.010748 0.012508 0.015354 0.026123 0.013515 0.010524 0.013346 0.015663 0.035243 0.011921 0.033473 0.040351 0.012409 0.016957 0.016619 0.019656 0.011524 0.009312 0.007225 0.005821 0.001814 
I0428 17:51:27.875656  7412 solver.cpp:224] Iteration 2900 (4.0695 iter/s, 24.5731s/100 iters), loss = 0.740661
I0428 17:51:27.875656  7412 solver.cpp:243]     Train net output #0: loss = 0.740661 (* 1 = 0.740661 loss)
I0428 17:51:27.875656  7412 sgd_solver.cpp:137] Iteration 2900, lr = 0.01
I0428 17:51:27.883657  7412 sgd_solver.cpp:169] scale layer:0.811369 0.843261 0.864037 0.864737 0.858246 0.866309 0.866424 0.866431 0.865780 0.865775 0.866141 0.866263 0.865936 0.865898 0.865220 0.865723 0.865448 0.865663 0.865994 1.106991 
I0428 17:51:27.885658  7412 sgd_solver.cpp:200] weight diff/data:0.017351 0.009424 0.012583 0.014665 0.025383 0.014861 0.010554 0.024695 0.013677 0.011062 0.031499 0.011888 0.020497 0.014021 0.058451 0.010833 0.007877 0.034035 0.007630 0.009040 0.004766 
I0428 17:51:52.247051  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_3000.caffemodel
I0428 17:51:52.256052  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_3000.solverstate
I0428 17:51:52.258051  7412 solver.cpp:336] Iteration 3000, Testing net (#0)
I0428 17:52:01.010551 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:52:01.374572  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7311
I0428 17:52:01.374572  7412 solver.cpp:403]     Test net output #1: loss = 0.777857 (* 1 = 0.777857 loss)
I0428 17:52:01.607586  7412 solver.cpp:224] Iteration 3000 (2.96446 iter/s, 33.733s/100 iters), loss = 0.664628
I0428 17:52:01.607586  7412 solver.cpp:243]     Train net output #0: loss = 0.664628 (* 1 = 0.664628 loss)
I0428 17:52:01.607586  7412 sgd_solver.cpp:137] Iteration 3000, lr = 0.01
I0428 17:52:01.616586  7412 sgd_solver.cpp:169] scale layer:0.806690 0.836922 0.859450 0.860424 0.853462 0.862183 0.862297 0.862294 0.861511 0.861617 0.861961 0.861931 0.861605 0.861516 0.860871 0.861452 0.861106 0.861397 0.861749 1.108591 
I0428 17:52:01.618587  7412 sgd_solver.cpp:200] weight diff/data:0.017005 0.013574 0.012843 0.010470 0.091000 0.038608 0.011559 0.012786 0.017636 0.013623 0.013330 0.027803 0.009854 0.026075 0.022262 0.011805 0.012346 0.007249 0.009497 0.074940 0.003227 
I0428 17:52:26.197993  7412 solver.cpp:224] Iteration 3100 (4.06647 iter/s, 24.5914s/100 iters), loss = 0.630539
I0428 17:52:26.198992  7412 solver.cpp:243]     Train net output #0: loss = 0.630539 (* 1 = 0.630539 loss)
I0428 17:52:26.198992  7412 sgd_solver.cpp:137] Iteration 3100, lr = 0.01
I0428 17:52:26.206993  7412 sgd_solver.cpp:169] scale layer:0.800927 0.831200 0.855834 0.855524 0.849479 0.857859 0.858091 0.858088 0.857402 0.857439 0.857712 0.857771 0.857474 0.857383 0.856641 0.857176 0.856915 0.857139 0.857517 1.108495 
I0428 17:52:26.208993  7412 sgd_solver.cpp:200] weight diff/data:0.004123 0.008448 0.186385 0.010055 0.009383 0.010161 0.111078 0.012503 0.011784 0.018285 0.248200 0.011831 0.022185 0.013174 0.010988 0.014600 0.013075 0.010468 0.014407 0.015639 0.072217 
I0428 17:52:49.565330 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:52:50.777398  7412 solver.cpp:224] Iteration 3200 (4.06853 iter/s, 24.5789s/100 iters), loss = 0.940723
I0428 17:52:50.777398  7412 solver.cpp:243]     Train net output #0: loss = 0.940723 (* 1 = 0.940723 loss)
I0428 17:52:50.777398  7412 sgd_solver.cpp:137] Iteration 3200, lr = 0.01
I0428 17:52:50.785398  7412 sgd_solver.cpp:169] scale layer:0.795878 0.825892 0.851455 0.852069 0.845177 0.853694 0.853908 0.853785 0.853205 0.853314 0.853521 0.853547 0.853138 0.853076 0.852259 0.852909 0.852623 0.852905 0.853309 1.106459 
I0428 17:52:50.787400  7412 sgd_solver.cpp:200] weight diff/data:0.019870 0.008140 0.011704 0.013533 0.009147 0.011335 0.012958 0.010039 0.014468 0.013497 0.017223 0.020672 0.053061 0.011822 0.011740 0.013377 0.013126 0.008785 0.007445 0.006829 0.001794 
I0428 17:53:15.350803  7412 solver.cpp:224] Iteration 3300 (4.06931 iter/s, 24.5742s/100 iters), loss = 0.587495
I0428 17:53:15.350803  7412 solver.cpp:243]     Train net output #0: loss = 0.587495 (* 1 = 0.587495 loss)
I0428 17:53:15.350803  7412 sgd_solver.cpp:137] Iteration 3300, lr = 0.01
I0428 17:53:15.359804  7412 sgd_solver.cpp:169] scale layer:0.791779 0.821424 0.847797 0.848605 0.839547 0.849699 0.849749 0.849650 0.848987 0.849090 0.849426 0.849416 0.849044 0.848922 0.848079 0.848698 0.848417 0.848712 0.849141 1.103942 
I0428 17:53:15.361804  7412 sgd_solver.cpp:200] weight diff/data:0.005920 0.032726 0.009917 0.057327 0.014575 0.042669 0.055584 0.105932 0.013300 0.013556 0.261960 0.021190 0.014458 0.012086 0.159875 0.014138 0.009678 0.008497 0.007498 0.010015 0.005054 
I0428 17:53:39.910208  7412 solver.cpp:224] Iteration 3400 (4.07175 iter/s, 24.5594s/100 iters), loss = 0.683492
I0428 17:53:39.910208  7412 solver.cpp:243]     Train net output #0: loss = 0.683492 (* 1 = 0.683492 loss)
I0428 17:53:39.910208  7412 sgd_solver.cpp:137] Iteration 3400, lr = 0.01
I0428 17:53:39.918210  7412 sgd_solver.cpp:169] scale layer:0.784985 0.816344 0.843098 0.844510 0.835613 0.845633 0.845729 0.845520 0.844926 0.844850 0.845445 0.845281 0.844914 0.844817 0.843864 0.844474 0.844151 0.844508 0.844952 1.103561 
I0428 17:53:39.920209  7412 sgd_solver.cpp:200] weight diff/data:0.004315 0.011407 0.046953 0.009819 0.013761 0.017360 0.010748 0.020179 0.017973 0.014225 0.013351 0.012187 0.017469 0.011553 0.011392 0.017750 0.009603 0.008539 0.014691 0.006903 0.002495 
I0428 17:54:04.252601  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_3500.caffemodel
I0428 17:54:04.260601  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_3500.solverstate
I0428 17:54:04.263602  7412 solver.cpp:336] Iteration 3500, Testing net (#0)
I0428 17:54:13.009101 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:54:13.372123  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7348
I0428 17:54:13.372123  7412 solver.cpp:403]     Test net output #1: loss = 0.757884 (* 1 = 0.757884 loss)
I0428 17:54:13.609136  7412 solver.cpp:224] Iteration 3500 (2.96739 iter/s, 33.6997s/100 iters), loss = 0.491664
I0428 17:54:13.609136  7412 solver.cpp:243]     Train net output #0: loss = 0.491664 (* 1 = 0.491664 loss)
I0428 17:54:13.609136  7412 sgd_solver.cpp:137] Iteration 3500, lr = 0.01
I0428 17:54:13.617136  7412 sgd_solver.cpp:169] scale layer:0.779592 0.811826 0.838355 0.840545 0.831458 0.841463 0.841534 0.841425 0.840926 0.840689 0.841170 0.841183 0.840723 0.840698 0.839775 0.840367 0.840067 0.840389 0.840772 1.105938 
I0428 17:54:13.619137  7412 sgd_solver.cpp:200] weight diff/data:0.005035 0.011852 0.010582 0.029859 0.013610 0.010619 0.013484 0.039961 0.023119 0.044150 0.015549 0.016733 0.017546 0.034373 0.016168 0.009882 0.016592 0.007733 0.007305 0.005484 0.003044 
I0428 17:54:36.963471 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:54:38.181541  7412 solver.cpp:224] Iteration 3600 (4.06952 iter/s, 24.5729s/100 iters), loss = 0.853406
I0428 17:54:38.181541  7412 solver.cpp:243]     Train net output #0: loss = 0.853406 (* 1 = 0.853406 loss)
I0428 17:54:38.181541  7412 sgd_solver.cpp:137] Iteration 3600, lr = 0.01
I0428 17:54:38.189543  7412 sgd_solver.cpp:169] scale layer:0.775200 0.807249 0.834584 0.836537 0.826331 0.837298 0.837401 0.837389 0.836743 0.836602 0.837179 0.837152 0.836503 0.836644 0.835536 0.836281 0.835940 0.836210 0.836592 1.104186 
I0428 17:54:38.191542  7412 sgd_solver.cpp:200] weight diff/data:0.019382 0.015773 0.023358 0.022645 0.010076 0.013637 0.020915 0.043855 0.015024 0.012381 0.027222 0.016801 0.022243 0.018314 0.009408 0.016552 0.011603 0.011231 0.009147 0.006483 0.002367 
I0428 17:55:02.750946  7412 solver.cpp:224] Iteration 3700 (4.07001 iter/s, 24.5699s/100 iters), loss = 0.561369
I0428 17:55:02.750946  7412 solver.cpp:243]     Train net output #0: loss = 0.561369 (* 1 = 0.561369 loss)
I0428 17:55:02.750946  7412 sgd_solver.cpp:137] Iteration 3700, lr = 0.01
I0428 17:55:02.758947  7412 sgd_solver.cpp:169] scale layer:0.772072 0.803754 0.830689 0.832343 0.822302 0.833404 0.833319 0.833416 0.832736 0.832722 0.833132 0.832955 0.832583 0.832585 0.831448 0.832232 0.831774 0.832058 0.832500 1.102076 
I0428 17:55:02.760947  7412 sgd_solver.cpp:200] weight diff/data:0.005656 0.011874 0.012591 0.011945 0.031685 0.033686 0.016631 0.010932 0.018568 0.014578 0.013304 0.014116 0.011573 0.017094 0.012346 0.019373 0.010242 0.006990 0.015671 0.054631 0.003051 
I0428 17:55:27.352355  7412 solver.cpp:224] Iteration 3800 (4.06462 iter/s, 24.6025s/100 iters), loss = 0.696283
I0428 17:55:27.352355  7412 solver.cpp:243]     Train net output #0: loss = 0.696283 (* 1 = 0.696283 loss)
I0428 17:55:27.353354  7412 sgd_solver.cpp:137] Iteration 3800, lr = 0.01
I0428 17:55:27.362354  7412 sgd_solver.cpp:169] scale layer:0.765823 0.799323 0.826286 0.828119 0.818336 0.829353 0.829136 0.829519 0.828830 0.828762 0.829205 0.829018 0.828463 0.828599 0.827273 0.828152 0.827653 0.827973 0.828437 1.102508 
I0428 17:55:27.364354  7412 sgd_solver.cpp:200] weight diff/data:0.005312 0.008011 0.016199 0.012286 0.012187 0.012889 0.046385 0.011352 0.067813 0.027267 0.046220 0.015776 0.011400 0.014048 0.019821 0.013634 0.023096 0.006878 0.007633 0.008290 0.001395 
I0428 17:55:51.983762  7412 solver.cpp:224] Iteration 3900 (4.05985 iter/s, 24.6315s/100 iters), loss = 0.491115
I0428 17:55:51.983762  7412 solver.cpp:243]     Train net output #0: loss = 0.491115 (* 1 = 0.491115 loss)
I0428 17:55:51.983762  7412 sgd_solver.cpp:137] Iteration 3900, lr = 0.01
I0428 17:55:51.992763  7412 sgd_solver.cpp:169] scale layer:0.763104 0.794000 0.821470 0.824093 0.813517 0.825280 0.825265 0.825477 0.824943 0.824824 0.825140 0.825005 0.824278 0.824555 0.823205 0.824061 0.823560 0.823909 0.824278 1.104786 
I0428 17:55:51.994763  7412 sgd_solver.cpp:200] weight diff/data:0.006589 0.009258 0.010609 0.014131 0.017684 0.013356 0.010817 0.024794 0.012326 0.012454 0.013334 0.048455 0.014367 0.018226 0.013780 0.013854 0.010019 0.014017 0.011385 0.008370 0.002676 
I0428 17:56:15.345099 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:56:16.323155  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_4000.caffemodel
I0428 17:56:16.332155  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_4000.solverstate
I0428 17:56:16.335155  7412 solver.cpp:336] Iteration 4000, Testing net (#0)
I0428 17:56:25.080655 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:56:25.447676  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7381
I0428 17:56:25.447676  7412 solver.cpp:403]     Test net output #1: loss = 0.749198 (* 1 = 0.749198 loss)
I0428 17:56:25.680691  7412 solver.cpp:224] Iteration 4000 (2.96762 iter/s, 33.697s/100 iters), loss = 0.815109
I0428 17:56:25.680691  7412 solver.cpp:243]     Train net output #0: loss = 0.815109 (* 1 = 0.815109 loss)
I0428 17:56:25.680691  7412 sgd_solver.cpp:137] Iteration 4000, lr = 0.01
I0428 17:56:25.689690  7412 sgd_solver.cpp:169] scale layer:0.757055 0.791688 0.818274 0.820710 0.809132 0.821204 0.821404 0.821601 0.820841 0.820736 0.821134 0.821029 0.820265 0.820521 0.819139 0.820028 0.819405 0.819829 0.820245 1.101898 
I0428 17:56:25.690690  7412 sgd_solver.cpp:200] weight diff/data:0.008351 0.010828 0.013710 0.011605 0.016659 0.013619 0.030008 0.017554 0.016565 0.009134 0.020707 0.022785 0.015779 0.011510 0.015955 0.019566 0.012008 0.008736 0.010641 0.091014 0.001843 
I0428 17:56:50.240094  7412 solver.cpp:224] Iteration 4100 (4.07157 iter/s, 24.5606s/100 iters), loss = 0.485241
I0428 17:56:50.241096  7412 solver.cpp:243]     Train net output #0: loss = 0.485241 (* 1 = 0.485241 loss)
I0428 17:56:50.241096  7412 sgd_solver.cpp:137] Iteration 4100, lr = 0.01
I0428 17:56:50.250095  7412 sgd_solver.cpp:169] scale layer:0.752737 0.787011 0.815421 0.818015 0.803333 0.817249 0.817543 0.817678 0.817192 0.816957 0.817112 0.817027 0.816449 0.816624 0.815087 0.816029 0.815349 0.815777 0.816215 1.099405 
I0428 17:56:50.252095  7412 sgd_solver.cpp:200] weight diff/data:0.009074 0.006579 0.008284 0.020854 0.015890 0.014595 0.098300 0.010511 0.009848 0.018472 0.015006 0.028678 0.016675 0.011422 0.023515 0.011305 0.008242 0.008845 0.016323 0.012166 0.005755 
I0428 17:57:14.803499  7412 solver.cpp:224] Iteration 4200 (4.07119 iter/s, 24.5629s/100 iters), loss = 0.711426
I0428 17:57:14.803499  7412 solver.cpp:243]     Train net output #0: loss = 0.711426 (* 1 = 0.711426 loss)
I0428 17:57:14.803499  7412 sgd_solver.cpp:137] Iteration 4200, lr = 0.01
I0428 17:57:14.811501  7412 sgd_solver.cpp:169] scale layer:0.746395 0.781345 0.811545 0.814630 0.800216 0.813296 0.813508 0.813692 0.813257 0.813104 0.813294 0.812923 0.812419 0.812675 0.811139 0.812073 0.811394 0.811789 0.812278 1.099994 
I0428 17:57:14.813500  7412 sgd_solver.cpp:200] weight diff/data:0.009832 0.010066 0.015822 0.024428 0.019581 0.011743 0.011869 0.017872 0.020771 0.010632 0.028252 0.017066 0.013055 0.083569 0.010690 0.011049 0.015069 0.007395 0.008157 0.006836 0.002104 
I0428 17:57:39.397907  7412 solver.cpp:224] Iteration 4300 (4.06589 iter/s, 24.5948s/100 iters), loss = 0.497411
I0428 17:57:39.397907  7412 solver.cpp:243]     Train net output #0: loss = 0.497411 (* 1 = 0.497411 loss)
I0428 17:57:39.397907  7412 sgd_solver.cpp:137] Iteration 4300, lr = 0.01
I0428 17:57:39.406908  7412 sgd_solver.cpp:169] scale layer:0.744620 0.778630 0.808131 0.810425 0.796520 0.809332 0.809504 0.809876 0.809258 0.809150 0.809465 0.809125 0.808400 0.808627 0.807161 0.807957 0.807412 0.807774 0.808247 1.099557 
I0428 17:57:39.408907  7412 sgd_solver.cpp:200] weight diff/data:0.026508 0.009597 0.025658 0.013760 0.016398 0.013285 0.012902 0.015023 0.016367 0.010633 0.015876 0.012986 0.018492 0.014955 0.018357 0.024677 0.011048 0.009064 0.015772 0.015375 0.002124 
I0428 17:58:02.753242 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:58:03.972312  7412 solver.cpp:224] Iteration 4400 (4.06918 iter/s, 24.575s/100 iters), loss = 0.739128
I0428 17:58:03.972312  7412 solver.cpp:243]     Train net output #0: loss = 0.739128 (* 1 = 0.739128 loss)
I0428 17:58:03.972312  7412 sgd_solver.cpp:137] Iteration 4400, lr = 0.01
I0428 17:58:03.981312  7412 sgd_solver.cpp:169] scale layer:0.740027 0.773912 0.803398 0.807565 0.792559 0.805211 0.805506 0.805885 0.805298 0.805332 0.805577 0.805148 0.804521 0.804710 0.803119 0.804025 0.803452 0.803803 0.804296 1.098117 
I0428 17:58:03.983312  7412 sgd_solver.cpp:200] weight diff/data:0.010965 0.008828 0.009327 0.012597 0.027728 0.013001 0.018956 0.012234 0.026497 0.010868 0.014790 0.020994 0.019344 0.015819 0.017655 0.027660 0.014811 0.021239 0.013932 0.014647 0.001740 
I0428 17:58:28.354707  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_4500.caffemodel
I0428 17:58:28.363708  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_4500.solverstate
I0428 17:58:28.366708  7412 solver.cpp:336] Iteration 4500, Testing net (#0)
I0428 17:58:37.104207 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:58:37.470228  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7522
I0428 17:58:37.470228  7412 solver.cpp:403]     Test net output #1: loss = 0.732972 (* 1 = 0.732972 loss)
I0428 17:58:37.702241  7412 solver.cpp:224] Iteration 4500 (2.96465 iter/s, 33.7309s/100 iters), loss = 0.53427
I0428 17:58:37.702241  7412 solver.cpp:243]     Train net output #0: loss = 0.53427 (* 1 = 0.53427 loss)
I0428 17:58:37.702241  7412 sgd_solver.cpp:137] Iteration 4500, lr = 0.01
I0428 17:58:37.711241  7412 sgd_solver.cpp:169] scale layer:0.734322 0.768970 0.800129 0.803925 0.787495 0.801232 0.801798 0.802110 0.801486 0.801443 0.801649 0.801309 0.800795 0.800763 0.799139 0.800128 0.799529 0.799846 0.800386 1.096376 
I0428 17:58:37.712242  7412 sgd_solver.cpp:200] weight diff/data:0.007906 0.014518 0.016997 0.011593 0.013388 0.035335 0.014355 0.014779 0.017882 0.010800 0.017099 0.012190 0.018259 0.017660 0.012368 0.018827 0.011435 0.013199 0.011925 0.006452 0.002852 
I0428 17:59:02.304648  7412 solver.cpp:224] Iteration 4600 (4.06463 iter/s, 24.6025s/100 iters), loss = 0.65457
I0428 17:59:02.304648  7412 solver.cpp:243]     Train net output #0: loss = 0.65457 (* 1 = 0.65457 loss)
I0428 17:59:02.304648  7412 sgd_solver.cpp:137] Iteration 4600, lr = 0.01
I0428 17:59:02.313649  7412 sgd_solver.cpp:169] scale layer:0.729672 0.766305 0.795196 0.799985 0.783472 0.797639 0.797890 0.798360 0.797733 0.797650 0.797585 0.797550 0.796891 0.796787 0.795190 0.796182 0.795595 0.795892 0.796477 1.095495 
I0428 17:59:02.315649  7412 sgd_solver.cpp:200] weight diff/data:0.127956 0.011197 0.015780 0.014076 0.018109 0.015156 0.014667 0.013977 0.014099 0.039890 0.014570 0.014097 0.012052 0.010651 0.014028 0.012476 0.010834 0.009234 0.007763 0.019620 0.002439 
I0428 17:59:26.865053  7412 solver.cpp:224] Iteration 4700 (4.07145 iter/s, 24.5613s/100 iters), loss = 0.498828
I0428 17:59:26.865053  7412 solver.cpp:243]     Train net output #0: loss = 0.498828 (* 1 = 0.498828 loss)
I0428 17:59:26.865053  7412 sgd_solver.cpp:137] Iteration 4700, lr = 0.01
I0428 17:59:26.875053  7412 sgd_solver.cpp:169] scale layer:0.724993 0.761800 0.793034 0.795917 0.778788 0.794064 0.793979 0.794648 0.793792 0.793602 0.794030 0.793622 0.792779 0.792971 0.791345 0.792284 0.791723 0.791973 0.792513 1.097700 
I0428 17:59:26.877054  7412 sgd_solver.cpp:200] weight diff/data:0.026217 0.010199 0.044730 0.010379 0.013923 0.013904 0.012735 0.011052 0.011366 0.015708 0.022694 0.026418 0.012563 0.018315 0.022655 0.013959 0.013044 0.007715 0.006772 0.010341 0.002266 
I0428 17:59:50.236390 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 17:59:51.459460  7412 solver.cpp:224] Iteration 4800 (4.06597 iter/s, 24.5944s/100 iters), loss = 0.623072
I0428 17:59:51.459460  7412 solver.cpp:243]     Train net output #0: loss = 0.623072 (* 1 = 0.623072 loss)
I0428 17:59:51.459460  7412 sgd_solver.cpp:137] Iteration 4800, lr = 0.01
I0428 17:59:51.468461  7412 sgd_solver.cpp:169] scale layer:0.720003 0.758087 0.788845 0.791623 0.774367 0.790056 0.790277 0.790694 0.790134 0.790334 0.790220 0.789807 0.788973 0.789270 0.787338 0.788349 0.787826 0.788046 0.788688 1.096048 
I0428 17:59:51.470460  7412 sgd_solver.cpp:200] weight diff/data:0.016037 0.016065 0.009199 0.012777 0.017649 0.035897 0.013532 0.014308 0.013824 0.014791 0.012498 0.035515 0.012687 0.011458 0.022909 0.018442 0.010765 0.010670 0.021147 0.007888 0.005025 
I0428 18:00:16.070868  7412 solver.cpp:224] Iteration 4900 (4.06304 iter/s, 24.6121s/100 iters), loss = 0.500415
I0428 18:00:16.070868  7412 solver.cpp:243]     Train net output #0: loss = 0.500415 (* 1 = 0.500415 loss)
I0428 18:00:16.070868  7412 sgd_solver.cpp:137] Iteration 4900, lr = 0.01
I0428 18:00:16.079869  7412 sgd_solver.cpp:169] scale layer:0.718151 0.752588 0.785597 0.788622 0.768730 0.786120 0.786287 0.786924 0.786385 0.786743 0.786783 0.785971 0.785426 0.785351 0.783486 0.784522 0.783916 0.784178 0.784884 1.095556 
I0428 18:00:16.081868  7412 sgd_solver.cpp:200] weight diff/data:0.010283 0.025911 0.024894 0.021579 0.014093 0.009627 0.032360 0.014908 0.009705 0.013261 0.013704 0.014084 0.016721 0.018098 0.013232 0.011607 0.013766 0.012892 0.009799 0.007413 0.002466 
I0428 18:00:40.404259  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_5000.caffemodel
I0428 18:00:40.413260  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_5000.solverstate
I0428 18:00:40.416260  7412 solver.cpp:336] Iteration 5000, Testing net (#0)
I0428 18:00:49.172761 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:00:49.535781  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7647
I0428 18:00:49.535781  7412 solver.cpp:403]     Test net output #1: loss = 0.694303 (* 1 = 0.694303 loss)
I0428 18:00:49.769795  7412 solver.cpp:224] Iteration 5000 (2.96741 iter/s, 33.6994s/100 iters), loss = 0.70263
I0428 18:00:49.769795  7412 solver.cpp:243]     Train net output #0: loss = 0.70263 (* 1 = 0.70263 loss)
I0428 18:00:49.769795  7412 sgd_solver.cpp:137] Iteration 5000, lr = 0.01
I0428 18:00:49.777796  7412 sgd_solver.cpp:169] scale layer:0.712109 0.748237 0.780548 0.784787 0.765886 0.782359 0.782419 0.783185 0.782709 0.782929 0.783107 0.782325 0.781767 0.781370 0.779699 0.780723 0.780116 0.780317 0.781086 1.095199 
I0428 18:00:49.779795  7412 sgd_solver.cpp:200] weight diff/data:0.006560 0.009442 0.007161 0.012720 0.136500 0.013173 0.018178 0.066027 0.030560 0.013360 0.012232 0.022641 0.067889 0.014052 0.012709 0.062986 0.063603 0.009248 0.008983 0.011045 0.001306 
I0428 18:01:14.358201  7412 solver.cpp:224] Iteration 5100 (4.06688 iter/s, 24.5888s/100 iters), loss = 0.513439
I0428 18:01:14.358201  7412 solver.cpp:243]     Train net output #0: loss = 0.513439 (* 1 = 0.513439 loss)
I0428 18:01:14.358201  7412 sgd_solver.cpp:137] Iteration 5100, lr = 0.01
I0428 18:01:14.367202  7412 sgd_solver.cpp:169] scale layer:0.708241 0.745610 0.778217 0.781926 0.763432 0.778869 0.778734 0.779407 0.778995 0.779371 0.779292 0.778578 0.777967 0.777782 0.775871 0.776959 0.776383 0.776518 0.777259 1.096254 
I0428 18:01:14.369202  7412 sgd_solver.cpp:200] weight diff/data:0.013090 0.025048 0.008786 0.022791 0.055109 0.013457 0.015953 0.056415 0.012437 0.013344 0.013843 0.021572 0.024202 0.013916 0.012907 0.019589 0.066524 0.018369 0.011083 0.008842 0.003137 
I0428 18:01:37.744539 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:01:38.963608  7412 solver.cpp:224] Iteration 5200 (4.06402 iter/s, 24.6062s/100 iters), loss = 0.809662
I0428 18:01:38.963608  7412 solver.cpp:243]     Train net output #0: loss = 0.809662 (* 1 = 0.809662 loss)
I0428 18:01:38.963608  7412 sgd_solver.cpp:137] Iteration 5200, lr = 0.01
I0428 18:01:38.973609  7412 sgd_solver.cpp:169] scale layer:0.702967 0.740575 0.774789 0.778687 0.760864 0.775030 0.774879 0.775766 0.775302 0.775675 0.775494 0.774935 0.774195 0.774063 0.771976 0.773100 0.772632 0.772754 0.773485 1.094422 
I0428 18:01:38.975610  7412 sgd_solver.cpp:200] weight diff/data:0.013019 0.010225 0.015120 0.061715 0.022852 0.012809 0.018516 0.014107 0.029872 0.079376 0.205840 0.193279 0.017466 0.018144 0.013002 0.012881 0.010191 0.008522 0.009004 0.011001 0.001723 
I0428 18:02:03.574017  7412 solver.cpp:224] Iteration 5300 (4.06319 iter/s, 24.6112s/100 iters), loss = 0.511025
I0428 18:02:03.574017  7412 solver.cpp:243]     Train net output #0: loss = 0.511025 (* 1 = 0.511025 loss)
I0428 18:02:03.575016  7412 sgd_solver.cpp:137] Iteration 5300, lr = 0.01
I0428 18:02:03.583017  7412 sgd_solver.cpp:169] scale layer:0.700677 0.736056 0.770917 0.774687 0.755886 0.771259 0.771298 0.772132 0.771640 0.772016 0.772045 0.771284 0.770817 0.770387 0.768430 0.769447 0.768866 0.768999 0.769695 1.091430 
I0428 18:02:03.585017  7412 sgd_solver.cpp:200] weight diff/data:0.012697 0.007101 0.010893 0.029236 0.012122 0.025194 0.020130 0.017257 0.046821 0.020836 0.015492 0.021585 0.031340 0.016019 0.014431 0.017662 0.013705 0.008190 0.010239 0.009424 0.003404 
I0428 18:02:28.200425  7412 solver.cpp:224] Iteration 5400 (4.06062 iter/s, 24.6268s/100 iters), loss = 0.603807
I0428 18:02:28.200425  7412 solver.cpp:243]     Train net output #0: loss = 0.603807 (* 1 = 0.603807 loss)
I0428 18:02:28.200425  7412 sgd_solver.cpp:137] Iteration 5400, lr = 0.01
I0428 18:02:28.209425  7412 sgd_solver.cpp:169] scale layer:0.693883 0.732561 0.766555 0.771181 0.751626 0.767491 0.767637 0.768359 0.767930 0.768651 0.768316 0.767739 0.767173 0.766672 0.764595 0.765649 0.765044 0.765210 0.765888 1.091153 
I0428 18:02:28.211426  7412 sgd_solver.cpp:200] weight diff/data:0.017965 0.008372 0.009929 0.017314 0.010353 0.017738 0.015240 0.016035 0.010816 0.015603 0.037794 0.016274 0.014411 0.018238 0.016045 0.011266 0.009338 0.012295 0.007045 0.010179 0.001934 
I0428 18:02:52.540817  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_5500.caffemodel
I0428 18:02:52.548817  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_5500.solverstate
I0428 18:02:52.551817  7412 solver.cpp:336] Iteration 5500, Testing net (#0)
I0428 18:03:01.305318 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:03:01.671339  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7597
I0428 18:03:01.672339  7412 solver.cpp:403]     Test net output #1: loss = 0.69641 (* 1 = 0.69641 loss)
I0428 18:03:01.906353  7412 solver.cpp:224] Iteration 5500 (2.96682 iter/s, 33.7061s/100 iters), loss = 0.4833
I0428 18:03:01.906353  7412 solver.cpp:243]     Train net output #0: loss = 0.4833 (* 1 = 0.4833 loss)
I0428 18:03:01.906353  7412 sgd_solver.cpp:137] Iteration 5500, lr = 0.01
I0428 18:03:01.914353  7412 sgd_solver.cpp:169] scale layer:0.690443 0.730388 0.763878 0.768025 0.748398 0.764129 0.764063 0.765046 0.764273 0.765202 0.764720 0.763990 0.763530 0.762954 0.761204 0.761873 0.761340 0.761445 0.762190 1.091058 
I0428 18:03:01.916353  7412 sgd_solver.cpp:200] weight diff/data:0.009870 0.011697 0.010434 0.021752 0.102540 0.010948 0.015760 0.012704 0.026118 0.013313 0.064475 0.013788 0.020072 0.027199 0.011914 0.013544 0.012056 0.020611 0.009361 0.013145 0.002338 
I0428 18:03:25.269690 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:03:26.487759  7412 solver.cpp:224] Iteration 5600 (4.06803 iter/s, 24.5819s/100 iters), loss = 0.66706
I0428 18:03:26.487759  7412 solver.cpp:243]     Train net output #0: loss = 0.66706 (* 1 = 0.66706 loss)
I0428 18:03:26.487759  7412 sgd_solver.cpp:137] Iteration 5600, lr = 0.01
I0428 18:03:26.495759  7412 sgd_solver.cpp:169] scale layer:0.683852 0.725415 0.760067 0.764921 0.744261 0.760126 0.760524 0.761241 0.760548 0.761895 0.760925 0.760387 0.759927 0.759392 0.757658 0.758208 0.757602 0.757709 0.758456 1.087879 
I0428 18:03:26.497759  7412 sgd_solver.cpp:200] weight diff/data:0.016513 0.012850 0.009696 0.009629 0.021577 0.012556 0.039438 0.015690 0.017180 0.011656 0.034015 0.047833 0.023560 0.016349 0.013485 0.018392 0.012274 0.008079 0.015431 0.012340 0.001855 
I0428 18:03:51.062165  7412 solver.cpp:224] Iteration 5700 (4.06909 iter/s, 24.5755s/100 iters), loss = 0.558596
I0428 18:03:51.062165  7412 solver.cpp:243]     Train net output #0: loss = 0.558596 (* 1 = 0.558596 loss)
I0428 18:03:51.062165  7412 sgd_solver.cpp:137] Iteration 5700, lr = 0.01
I0428 18:03:51.071166  7412 sgd_solver.cpp:169] scale layer:0.681486 0.722092 0.756921 0.761113 0.739593 0.756522 0.756867 0.757692 0.757021 0.758238 0.757238 0.756806 0.756385 0.755910 0.753957 0.754594 0.753878 0.753992 0.754757 1.087374 
I0428 18:03:51.073165  7412 sgd_solver.cpp:200] weight diff/data:0.006821 0.009361 0.014429 0.019079 0.015721 0.013992 0.019025 0.011974 0.064053 0.012748 0.015350 0.014559 0.013640 0.013646 0.014020 0.010252 0.012802 0.012947 0.007899 0.006386 0.002568 
I0428 18:04:15.661571  7412 solver.cpp:224] Iteration 5800 (4.06504 iter/s, 24.6s/100 iters), loss = 0.514187
I0428 18:04:15.661571  7412 solver.cpp:243]     Train net output #0: loss = 0.514187 (* 1 = 0.514187 loss)
I0428 18:04:15.661571  7412 sgd_solver.cpp:137] Iteration 5800, lr = 0.01
I0428 18:04:15.670572  7412 sgd_solver.cpp:169] scale layer:0.676580 0.716080 0.753700 0.757498 0.735958 0.752859 0.753310 0.753975 0.753599 0.754861 0.753681 0.753349 0.752719 0.752135 0.750271 0.750837 0.750130 0.750297 0.751126 1.087117 
I0428 18:04:15.672572  7412 sgd_solver.cpp:200] weight diff/data:0.005934 0.007676 0.014150 0.011124 0.010094 0.088217 0.010637 0.014256 0.019905 0.011873 0.013551 0.012219 0.019956 0.017364 0.008976 0.035841 0.010686 0.008909 0.009556 0.016082 0.001530 
I0428 18:04:40.255978  7412 solver.cpp:224] Iteration 5900 (4.0659 iter/s, 24.5948s/100 iters), loss = 0.528375
I0428 18:04:40.255978  7412 solver.cpp:243]     Train net output #0: loss = 0.528375 (* 1 = 0.528375 loss)
I0428 18:04:40.255978  7412 sgd_solver.cpp:137] Iteration 5900, lr = 0.01
I0428 18:04:40.264978  7412 sgd_solver.cpp:169] scale layer:0.674331 0.713473 0.750194 0.753567 0.733834 0.749432 0.749587 0.750682 0.750057 0.751484 0.750437 0.749933 0.749243 0.748495 0.746686 0.747171 0.746543 0.746647 0.747454 1.088296 
I0428 18:04:40.267979  7412 sgd_solver.cpp:200] weight diff/data:0.007666 0.024882 0.013790 0.010064 0.015648 0.013061 0.021995 0.012445 0.080270 0.068072 0.031757 0.096470 0.014119 0.017845 0.028869 0.019385 0.010386 0.012068 0.008057 0.009224 0.003898 
I0428 18:05:03.630316 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:05:04.612371  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_6000.caffemodel
I0428 18:05:04.620371  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_6000.solverstate
I0428 18:05:04.623373  7412 solver.cpp:336] Iteration 6000, Testing net (#0)
I0428 18:05:13.383873 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:05:13.745893  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7751
I0428 18:05:13.745893  7412 solver.cpp:403]     Test net output #1: loss = 0.664105 (* 1 = 0.664105 loss)
I0428 18:05:13.978907  7412 solver.cpp:224] Iteration 6000 (2.96533 iter/s, 33.723s/100 iters), loss = 0.698198
I0428 18:05:13.978907  7412 solver.cpp:243]     Train net output #0: loss = 0.698198 (* 1 = 0.698198 loss)
I0428 18:05:13.978907  7412 sgd_solver.cpp:137] Iteration 6000, lr = 0.01
I0428 18:05:13.987907  7412 sgd_solver.cpp:169] scale layer:0.670800 0.712595 0.748347 0.750764 0.729953 0.745499 0.746161 0.747395 0.746642 0.747977 0.746966 0.746195 0.745306 0.744796 0.743186 0.743618 0.742798 0.742983 0.743883 1.085307 
I0428 18:05:13.989907  7412 sgd_solver.cpp:200] weight diff/data:0.007350 0.024151 1.319893 0.011818 0.066196 0.012091 0.019212 0.012490 0.018402 0.039021 0.017422 0.046458 0.027793 0.040968 0.016549 0.024162 0.025025 0.177261 0.009605 0.009131 0.002113 
I0428 18:05:38.564313  7412 solver.cpp:224] Iteration 6100 (4.06727 iter/s, 24.5865s/100 iters), loss = 0.456793
I0428 18:05:38.565313  7412 solver.cpp:243]     Train net output #0: loss = 0.456793 (* 1 = 0.456793 loss)
I0428 18:05:38.565313  7412 sgd_solver.cpp:137] Iteration 6100, lr = 0.01
I0428 18:05:38.574313  7412 sgd_solver.cpp:169] scale layer:0.666603 0.709583 0.745316 0.748348 0.726819 0.742611 0.742559 0.743818 0.743158 0.744608 0.743662 0.742760 0.741798 0.741359 0.739661 0.740186 0.739307 0.739412 0.740370 1.083941 
I0428 18:05:38.576314  7412 sgd_solver.cpp:200] weight diff/data:0.006093 0.008332 0.065654 0.013062 0.013493 0.010164 0.013547 0.011799 0.014236 0.224240 0.019527 0.054774 0.013227 0.016947 0.011256 0.023714 0.010097 0.014101 0.008362 0.008403 0.002050 
I0428 18:06:03.112717  7412 solver.cpp:224] Iteration 6200 (4.07362 iter/s, 24.5482s/100 iters), loss = 0.473836
I0428 18:06:03.112717  7412 solver.cpp:243]     Train net output #0: loss = 0.473836 (* 1 = 0.473836 loss)
I0428 18:06:03.112717  7412 sgd_solver.cpp:137] Iteration 6200, lr = 0.01
I0428 18:06:03.120718  7412 sgd_solver.cpp:169] scale layer:0.661122 0.706342 0.741514 0.744539 0.721473 0.738758 0.738944 0.740207 0.739552 0.741223 0.740382 0.739327 0.738135 0.737471 0.736019 0.736584 0.735659 0.735840 0.736756 1.083471 
I0428 18:06:03.122719  7412 sgd_solver.cpp:200] weight diff/data:0.011090 0.008430 0.015476 0.011063 0.015850 0.009851 0.024783 0.018375 0.011117 0.012061 0.012288 0.038266 0.011094 0.016361 0.011924 0.015187 0.010441 0.009239 0.009099 0.006236 0.001485 
I0428 18:06:27.683122  7412 solver.cpp:224] Iteration 6300 (4.06985 iter/s, 24.5709s/100 iters), loss = 0.353863
I0428 18:06:27.683122  7412 solver.cpp:243]     Train net output #0: loss = 0.353863 (* 1 = 0.353863 loss)
I0428 18:06:27.683122  7412 sgd_solver.cpp:137] Iteration 6300, lr = 0.01
I0428 18:06:27.691123  7412 sgd_solver.cpp:169] scale layer:0.658915 0.703757 0.738841 0.741726 0.716619 0.735563 0.735489 0.737150 0.736019 0.737695 0.737171 0.735936 0.734599 0.734107 0.732669 0.732851 0.732045 0.732176 0.733150 1.086570 
I0428 18:06:27.693123  7412 sgd_solver.cpp:200] weight diff/data:0.006763 0.019262 0.011976 0.012129 0.012327 0.044513 0.013296 0.025818 0.012168 0.013505 0.016851 0.030034 0.015214 0.016062 0.020102 0.025223 0.060139 0.057240 0.011678 0.005949 0.003365 
I0428 18:06:51.040458 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:06:52.256528  7412 solver.cpp:224] Iteration 6400 (4.06926 iter/s, 24.5745s/100 iters), loss = 0.667747
I0428 18:06:52.257529  7412 solver.cpp:243]     Train net output #0: loss = 0.667747 (* 1 = 0.667747 loss)
I0428 18:06:52.257529  7412 sgd_solver.cpp:137] Iteration 6400, lr = 0.01
I0428 18:06:52.265528  7412 sgd_solver.cpp:169] scale layer:0.654884 0.698940 0.734641 0.738224 0.714684 0.732171 0.731906 0.733808 0.732756 0.734426 0.733826 0.732516 0.731186 0.730834 0.729113 0.729334 0.728457 0.728655 0.729645 1.084522 
I0428 18:06:52.267529  7412 sgd_solver.cpp:200] weight diff/data:0.010231 0.012129 0.013396 0.010990 0.020190 0.014170 0.014597 0.015413 0.010840 0.026812 0.031944 0.023689 0.017846 0.019700 0.012679 0.019092 0.019191 0.016900 0.008686 0.012359 0.001954 
I0428 18:07:16.587919  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_6500.caffemodel
I0428 18:07:16.596920  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_6500.solverstate
I0428 18:07:16.599920  7412 solver.cpp:336] Iteration 6500, Testing net (#0)
I0428 18:07:25.341420 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:07:25.703441  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7737
I0428 18:07:25.703441  7412 solver.cpp:403]     Test net output #1: loss = 0.673666 (* 1 = 0.673666 loss)
I0428 18:07:25.936455  7412 solver.cpp:224] Iteration 6500 (2.96912 iter/s, 33.6801s/100 iters), loss = 0.493206
I0428 18:07:25.936455  7412 solver.cpp:243]     Train net output #0: loss = 0.493206 (* 1 = 0.493206 loss)
I0428 18:07:25.936455  7412 sgd_solver.cpp:137] Iteration 6500, lr = 0.01
I0428 18:07:25.944455  7412 sgd_solver.cpp:169] scale layer:0.653225 0.696344 0.731951 0.735777 0.709165 0.729151 0.728396 0.730434 0.729227 0.731215 0.730759 0.729292 0.728178 0.727499 0.725918 0.725849 0.724951 0.725154 0.726112 1.081740 
I0428 18:07:25.945456  7412 sgd_solver.cpp:200] weight diff/data:0.009178 0.013623 0.013071 0.013976 0.013310 0.020162 0.010341 0.016144 0.011748 0.026174 0.020370 0.018256 0.022658 0.017589 0.015248 0.110468 0.011334 0.008322 0.008657 0.007020 0.001666 
I0428 18:07:50.553863  7412 solver.cpp:224] Iteration 6600 (4.06205 iter/s, 24.6181s/100 iters), loss = 0.617724
I0428 18:07:50.553863  7412 solver.cpp:243]     Train net output #0: loss = 0.617724 (* 1 = 0.617724 loss)
I0428 18:07:50.553863  7412 sgd_solver.cpp:137] Iteration 6600, lr = 0.01
I0428 18:07:50.562863  7412 sgd_solver.cpp:169] scale layer:0.649410 0.691075 0.729182 0.733245 0.706568 0.725444 0.725007 0.727027 0.725878 0.727965 0.727395 0.726155 0.724741 0.723852 0.722292 0.722211 0.721410 0.721655 0.722649 1.082803 
I0428 18:07:50.564863  7412 sgd_solver.cpp:200] weight diff/data:0.006982 0.010094 0.016549 0.010437 0.013916 0.019389 0.440355 0.025499 0.020397 0.023545 0.014468 0.012678 0.020583 0.016255 0.041818 0.015425 0.011379 0.013725 0.009696 0.009343 0.001688 
I0428 18:08:15.147269  7412 solver.cpp:224] Iteration 6700 (4.06604 iter/s, 24.594s/100 iters), loss = 0.390702
I0428 18:08:15.147269  7412 solver.cpp:243]     Train net output #0: loss = 0.390702 (* 1 = 0.390702 loss)
I0428 18:08:15.147269  7412 sgd_solver.cpp:137] Iteration 6700, lr = 0.01
I0428 18:08:15.156270  7412 sgd_solver.cpp:169] scale layer:0.647807 0.688682 0.725865 0.731245 0.704017 0.722437 0.721709 0.723869 0.722307 0.724591 0.723955 0.722624 0.721254 0.720524 0.718953 0.718615 0.717773 0.718048 0.719180 1.082694 
I0428 18:08:15.159271  7412 sgd_solver.cpp:200] weight diff/data:0.006729 0.012723 0.016425 0.009432 0.012136 0.011657 0.013628 0.010792 0.012305 0.026511 0.013004 0.016273 0.017384 0.016431 0.013042 0.019925 0.018080 0.010117 0.006716 0.011778 0.001655 
I0428 18:08:38.561609 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:08:39.784678  7412 solver.cpp:224] Iteration 6800 (4.05871 iter/s, 24.6384s/100 iters), loss = 0.710424
I0428 18:08:39.785678  7412 solver.cpp:243]     Train net output #0: loss = 0.710424 (* 1 = 0.710424 loss)
I0428 18:08:39.785678  7412 sgd_solver.cpp:137] Iteration 6800, lr = 0.01
I0428 18:08:39.793679  7412 sgd_solver.cpp:169] scale layer:0.643697 0.684709 0.722493 0.727082 0.703079 0.718997 0.718423 0.720341 0.719298 0.721758 0.720765 0.718974 0.718062 0.717121 0.715535 0.715042 0.714285 0.714574 0.715720 1.080377 
I0428 18:08:39.795680  7412 sgd_solver.cpp:200] weight diff/data:0.006915 0.012075 0.015876 0.014289 0.016307 0.043362 0.015239 0.021914 0.019008 0.014207 0.024501 0.016472 0.020432 0.013780 0.092932 0.015449 0.010255 0.009121 0.010906 0.016662 0.001475 
I0428 18:09:04.393086  7412 solver.cpp:224] Iteration 6900 (4.0637 iter/s, 24.6081s/100 iters), loss = 0.505287
I0428 18:09:04.393086  7412 solver.cpp:243]     Train net output #0: loss = 0.505287 (* 1 = 0.505287 loss)
I0428 18:09:04.393086  7412 sgd_solver.cpp:137] Iteration 6900, lr = 0.01
I0428 18:09:04.401087  7412 sgd_solver.cpp:169] scale layer:0.639606 0.683421 0.720886 0.725404 0.698641 0.715452 0.715092 0.717199 0.715695 0.718017 0.717677 0.715749 0.714814 0.714014 0.712520 0.711628 0.710841 0.711084 0.712205 1.079813 
I0428 18:09:04.403086  7412 sgd_solver.cpp:200] weight diff/data:0.009572 0.011614 0.012173 0.019810 0.013557 0.021782 0.014010 0.023405 0.015593 0.019758 0.017158 0.013529 0.029760 0.015183 0.012638 0.017497 0.012189 0.011396 0.007871 0.011702 0.001973 
I0428 18:09:28.793481  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_7000.caffemodel
I0428 18:09:28.801482  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_7000.solverstate
I0428 18:09:28.804482  7412 solver.cpp:336] Iteration 7000, Testing net (#0)
I0428 18:09:37.548982 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:09:37.914003  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7799
I0428 18:09:37.914003  7412 solver.cpp:403]     Test net output #1: loss = 0.659511 (* 1 = 0.659511 loss)
I0428 18:09:38.148016  7412 solver.cpp:224] Iteration 7000 (2.96245 iter/s, 33.7558s/100 iters), loss = 0.497368
I0428 18:09:38.148016  7412 solver.cpp:243]     Train net output #0: loss = 0.497368 (* 1 = 0.497368 loss)
I0428 18:09:38.148016  7412 sgd_solver.cpp:137] Iteration 7000, lr = 0.01
I0428 18:09:38.156018  7412 sgd_solver.cpp:169] scale layer:0.635380 0.679695 0.717300 0.722453 0.697434 0.711923 0.711698 0.713933 0.712713 0.715006 0.714276 0.712752 0.711340 0.710653 0.709053 0.708325 0.707312 0.707665 0.708709 1.076358 
I0428 18:09:38.158017  7412 sgd_solver.cpp:200] weight diff/data:0.008169 0.008676 0.028756 0.026613 0.010463 0.013747 0.015349 0.015383 0.014480 0.013086 0.015849 0.056003 0.017912 0.015954 0.020380 0.015210 0.023398 0.008943 0.006937 0.006628 0.001482 
I0428 18:10:02.752424  7412 solver.cpp:224] Iteration 7100 (4.06423 iter/s, 24.6049s/100 iters), loss = 0.371245
I0428 18:10:02.752424  7412 solver.cpp:243]     Train net output #0: loss = 0.371245 (* 1 = 0.371245 loss)
I0428 18:10:02.752424  7412 sgd_solver.cpp:137] Iteration 7100, lr = 0.01
I0428 18:10:02.760424  7412 sgd_solver.cpp:169] scale layer:0.632717 0.677366 0.713127 0.719784 0.693298 0.709399 0.708720 0.710776 0.709440 0.711599 0.710989 0.709737 0.707871 0.707409 0.705638 0.704629 0.703865 0.704197 0.705327 1.079146 
I0428 18:10:02.762424  7412 sgd_solver.cpp:200] weight diff/data:0.017074 0.012966 0.009924 0.013406 0.037535 0.017358 0.015664 0.028016 0.012684 0.016130 0.012297 0.012338 0.030491 0.025122 0.016358 0.018258 0.022326 0.011994 0.008791 0.016489 0.001906 
I0428 18:10:26.121760 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:10:27.343830  7412 solver.cpp:224] Iteration 7200 (4.06638 iter/s, 24.5919s/100 iters), loss = 0.601058
I0428 18:10:27.343830  7412 solver.cpp:243]     Train net output #0: loss = 0.601058 (* 1 = 0.601058 loss)
I0428 18:10:27.343830  7412 sgd_solver.cpp:137] Iteration 7200, lr = 0.01
I0428 18:10:27.352831  7412 sgd_solver.cpp:169] scale layer:0.629587 0.673803 0.711173 0.716754 0.690872 0.705780 0.705325 0.707718 0.706266 0.708601 0.707975 0.706604 0.704682 0.704181 0.702173 0.701441 0.700420 0.700701 0.702006 1.079752 
I0428 18:10:27.354831  7412 sgd_solver.cpp:200] weight diff/data:0.006366 0.011580 0.016081 0.019101 0.019510 0.017736 0.014589 0.097298 0.019877 0.032425 0.016038 0.014286 0.025287 0.020060 0.015515 0.023999 0.038501 0.013341 0.009202 0.008808 0.003108 
I0428 18:10:51.949237  7412 solver.cpp:224] Iteration 7300 (4.06398 iter/s, 24.6064s/100 iters), loss = 0.406782
I0428 18:10:51.949237  7412 solver.cpp:243]     Train net output #0: loss = 0.406782 (* 1 = 0.406782 loss)
I0428 18:10:51.950238  7412 sgd_solver.cpp:137] Iteration 7300, lr = 0.01
I0428 18:10:51.958238  7412 sgd_solver.cpp:169] scale layer:0.626314 0.670461 0.706969 0.713487 0.686690 0.702492 0.702175 0.704147 0.702819 0.705530 0.704780 0.703244 0.701474 0.700943 0.699302 0.698180 0.697003 0.697339 0.698672 1.077283 
I0428 18:10:51.960238  7412 sgd_solver.cpp:200] weight diff/data:0.029484 0.016157 0.014727 0.014487 0.013727 0.022529 0.138387 0.022026 0.027932 0.030168 0.017844 0.024697 0.014847 0.021227 0.012753 0.016866 0.011369 0.009371 0.011293 0.014154 0.002276 
I0428 18:11:16.546644  7412 solver.cpp:224] Iteration 7400 (4.06539 iter/s, 24.5979s/100 iters), loss = 0.539261
I0428 18:11:16.546644  7412 solver.cpp:243]     Train net output #0: loss = 0.539261 (* 1 = 0.539261 loss)
I0428 18:11:16.547644  7412 sgd_solver.cpp:137] Iteration 7400, lr = 0.01
I0428 18:11:16.556645  7412 sgd_solver.cpp:169] scale layer:0.621278 0.666464 0.705202 0.710627 0.685279 0.699540 0.698668 0.701141 0.699771 0.702320 0.702227 0.700066 0.698135 0.697859 0.695681 0.694889 0.693483 0.694021 0.695494 1.078351 
I0428 18:11:16.558645  7412 sgd_solver.cpp:200] weight diff/data:0.013200 0.007874 0.013378 0.011390 0.014556 0.075774 0.012926 0.016236 0.016889 0.024536 0.016852 0.024982 0.017136 0.029461 0.012985 0.013480 0.014508 0.015248 0.008783 0.009730 0.001463 
I0428 18:11:40.879036  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_7500.caffemodel
I0428 18:11:40.888037  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_7500.solverstate
I0428 18:11:40.891037  7412 solver.cpp:336] Iteration 7500, Testing net (#0)
I0428 18:11:49.639538 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:11:50.000558  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7744
I0428 18:11:50.000558  7412 solver.cpp:403]     Test net output #1: loss = 0.676025 (* 1 = 0.676025 loss)
I0428 18:11:50.232571  7412 solver.cpp:224] Iteration 7500 (2.96856 iter/s, 33.6863s/100 iters), loss = 0.371828
I0428 18:11:50.232571  7412 solver.cpp:243]     Train net output #0: loss = 0.371828 (* 1 = 0.371828 loss)
I0428 18:11:50.232571  7412 sgd_solver.cpp:137] Iteration 7500, lr = 0.01
I0428 18:11:50.240572  7412 sgd_solver.cpp:169] scale layer:0.621310 0.661974 0.701712 0.707726 0.681678 0.696776 0.695731 0.698294 0.696666 0.699387 0.699279 0.696968 0.695024 0.694450 0.692538 0.691582 0.690221 0.690665 0.692137 1.078351 
I0428 18:11:50.242573  7412 sgd_solver.cpp:200] weight diff/data:0.018765 0.020025 0.013715 0.008939 0.011397 0.017096 0.031070 0.013841 0.030198 0.013313 0.015561 0.017782 0.017462 0.013968 0.017873 0.014784 0.013419 0.011348 0.007038 0.010510 0.002160 
I0428 18:12:13.596907 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:12:14.812978  7412 solver.cpp:224] Iteration 7600 (4.06816 iter/s, 24.5811s/100 iters), loss = 0.492869
I0428 18:12:14.812978  7412 solver.cpp:243]     Train net output #0: loss = 0.492869 (* 1 = 0.492869 loss)
I0428 18:12:14.812978  7412 sgd_solver.cpp:137] Iteration 7600, lr = 0.01
I0428 18:12:14.821979  7412 sgd_solver.cpp:169] scale layer:0.617504 0.661404 0.699641 0.704824 0.676700 0.693626 0.693197 0.694845 0.693840 0.696470 0.696191 0.693722 0.692043 0.691333 0.689250 0.688359 0.686934 0.687282 0.688880 1.077852 
I0428 18:12:14.823978  7412 sgd_solver.cpp:200] weight diff/data:0.006114 0.008204 0.012611 0.013204 0.011776 0.015049 0.013579 0.042579 0.038300 0.044155 0.079126 0.034325 0.012194 0.016167 0.045215 0.010978 0.018838 0.010280 0.008151 0.009754 0.002913 
I0428 18:12:39.399384  7412 solver.cpp:224] Iteration 7700 (4.06725 iter/s, 24.5867s/100 iters), loss = 0.479249
I0428 18:12:39.399384  7412 solver.cpp:243]     Train net output #0: loss = 0.479249 (* 1 = 0.479249 loss)
I0428 18:12:39.399384  7412 sgd_solver.cpp:137] Iteration 7700, lr = 0.01
I0428 18:12:39.409384  7412 sgd_solver.cpp:169] scale layer:0.614743 0.660067 0.695916 0.702994 0.674094 0.690903 0.689792 0.691833 0.690564 0.693534 0.693022 0.690503 0.688625 0.688142 0.686055 0.685114 0.683489 0.684011 0.685595 1.075210 
I0428 18:12:39.411384  7412 sgd_solver.cpp:200] weight diff/data:0.010698 0.024717 0.021251 0.194964 0.012963 0.028945 0.015636 0.030808 0.015698 0.029324 0.017785 0.019838 0.013568 0.013378 0.017273 0.026857 0.041174 0.008507 0.012002 0.006625 0.002251 
I0428 18:13:03.990790  7412 solver.cpp:224] Iteration 7800 (4.06642 iter/s, 24.5916s/100 iters), loss = 0.378555
I0428 18:13:03.990790  7412 solver.cpp:243]     Train net output #0: loss = 0.378555 (* 1 = 0.378555 loss)
I0428 18:13:03.990790  7412 sgd_solver.cpp:137] Iteration 7800, lr = 0.01
I0428 18:13:03.999790  7412 sgd_solver.cpp:169] scale layer:0.607117 0.656735 0.694305 0.700232 0.674060 0.687767 0.686549 0.688696 0.687652 0.690358 0.690211 0.687689 0.685581 0.684868 0.682888 0.681919 0.680265 0.680690 0.682370 1.076806 
I0428 18:13:04.001791  7412 sgd_solver.cpp:200] weight diff/data:0.005841 0.009666 0.024773 0.011222 0.015669 0.015098 0.018674 0.013187 0.026250 0.014122 0.013028 0.014223 0.012680 0.013248 0.011320 0.010522 0.010347 0.012988 0.036584 0.008543 0.001888 
I0428 18:13:28.556195  7412 solver.cpp:224] Iteration 7900 (4.07077 iter/s, 24.5654s/100 iters), loss = 0.349956
I0428 18:13:28.556195  7412 solver.cpp:243]     Train net output #0: loss = 0.349956 (* 1 = 0.349956 loss)
I0428 18:13:28.556195  7412 sgd_solver.cpp:137] Iteration 7900, lr = 0.01
I0428 18:13:28.565196  7412 sgd_solver.cpp:169] scale layer:0.606910 0.654559 0.691394 0.697952 0.670591 0.684236 0.684123 0.685779 0.684814 0.687402 0.687134 0.684764 0.682353 0.681449 0.679695 0.678481 0.677041 0.677395 0.679054 1.077081 
I0428 18:13:28.567195  7412 sgd_solver.cpp:200] weight diff/data:0.007004 4.041327 0.013385 0.012758 0.110014 0.025298 0.090868 0.018439 0.013005 0.016646 0.018801 0.026567 0.020096 0.018179 0.014700 0.018633 0.022011 0.023191 0.011737 0.008895 0.003480 
I0428 18:13:51.989536 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:13:52.967592  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_8000.caffemodel
I0428 18:13:52.975592  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_8000.solverstate
I0428 18:13:52.978592  7412 solver.cpp:336] Iteration 8000, Testing net (#0)
I0428 18:14:01.721092 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:14:02.087113  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7853
I0428 18:14:02.087113  7412 solver.cpp:403]     Test net output #1: loss = 0.646367 (* 1 = 0.646367 loss)
I0428 18:14:02.322126  7412 solver.cpp:224] Iteration 8000 (2.96152 iter/s, 33.7665s/100 iters), loss = 0.591824
I0428 18:14:02.322126  7412 solver.cpp:243]     Train net output #0: loss = 0.591824 (* 1 = 0.591824 loss)
I0428 18:14:02.322126  7412 sgd_solver.cpp:137] Iteration 8000, lr = 0.01
I0428 18:14:02.331127  7412 sgd_solver.cpp:169] scale layer:0.604182 0.651396 0.688656 0.695183 0.668454 0.681634 0.681306 0.682656 0.681849 0.684668 0.683966 0.681868 0.679521 0.678491 0.676419 0.675300 0.673712 0.674152 0.675919 1.075348 
I0428 18:14:02.332128  7412 sgd_solver.cpp:200] weight diff/data:0.006455 0.008405 0.024154 0.014170 0.022401 0.033075 0.056157 0.010971 0.016646 0.016747 0.018644 0.012642 0.021237 0.015135 0.010992 0.046318 0.015526 0.009228 0.010524 0.013198 0.002850 
I0428 18:14:26.926533  7412 solver.cpp:224] Iteration 8100 (4.06412 iter/s, 24.6056s/100 iters), loss = 0.437174
I0428 18:14:26.926533  7412 solver.cpp:243]     Train net output #0: loss = 0.437174 (* 1 = 0.437174 loss)
I0428 18:14:26.926533  7412 sgd_solver.cpp:137] Iteration 8100, lr = 0.01
I0428 18:14:26.935534  7412 sgd_solver.cpp:169] scale layer:0.599831 0.648858 0.686590 0.692275 0.665139 0.678412 0.677981 0.679724 0.678832 0.681659 0.680519 0.678557 0.676137 0.675678 0.673517 0.672077 0.670511 0.670946 0.672626 1.073969 
I0428 18:14:26.937535  7412 sgd_solver.cpp:200] weight diff/data:0.004656 0.010033 0.009976 0.017052 0.020559 0.012742 0.260528 0.020468 0.014492 0.023288 0.013575 0.065230 0.014035 0.012119 0.012208 0.016269 0.018020 0.009470 0.006623 0.007938 0.001841 
I0428 18:14:51.554942  7412 solver.cpp:224] Iteration 8200 (4.06035 iter/s, 24.6284s/100 iters), loss = 0.420584
I0428 18:14:51.554942  7412 solver.cpp:243]     Train net output #0: loss = 0.420584 (* 1 = 0.420584 loss)
I0428 18:14:51.554942  7412 sgd_solver.cpp:137] Iteration 8200, lr = 0.01
I0428 18:14:51.562943  7412 sgd_solver.cpp:169] scale layer:0.599090 0.646919 0.683107 0.690953 0.662264 0.676072 0.675154 0.676377 0.675937 0.678927 0.677703 0.675867 0.673483 0.672427 0.670167 0.669025 0.667129 0.667712 0.669621 1.071516 
I0428 18:14:51.564944  7412 sgd_solver.cpp:200] weight diff/data:0.003671 0.016412 0.018398 0.014606 0.023337 0.011381 0.019084 0.012007 0.010903 0.016804 0.013060 0.012662 0.014740 0.021820 0.016522 0.015567 0.017903 0.015118 0.007261 0.009154 0.001765 
I0428 18:15:16.104346  7412 solver.cpp:224] Iteration 8300 (4.0733 iter/s, 24.5501s/100 iters), loss = 0.332846
I0428 18:15:16.104346  7412 solver.cpp:243]     Train net output #0: loss = 0.332846 (* 1 = 0.332846 loss)
I0428 18:15:16.104346  7412 sgd_solver.cpp:137] Iteration 8300, lr = 0.01
I0428 18:15:16.113348  7412 sgd_solver.cpp:169] scale layer:0.595714 0.642568 0.682305 0.687599 0.658563 0.673434 0.672439 0.673703 0.672744 0.676004 0.675002 0.672944 0.670546 0.669304 0.667009 0.665910 0.664013 0.664460 0.666308 1.071425 
I0428 18:15:16.115347  7412 sgd_solver.cpp:200] weight diff/data:0.055665 0.013810 0.017143 0.017053 0.015933 0.017033 0.014275 0.023420 0.009556 0.021969 0.018094 0.017224 0.011608 0.018296 0.015822 0.036138 0.009114 0.009069 0.015565 0.007657 0.006521 
I0428 18:15:39.551687 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:15:40.771757  7412 solver.cpp:224] Iteration 8400 (4.0538 iter/s, 24.6682s/100 iters), loss = 0.617711
I0428 18:15:40.771757  7412 solver.cpp:243]     Train net output #0: loss = 0.617711 (* 1 = 0.617711 loss)
I0428 18:15:40.771757  7412 sgd_solver.cpp:137] Iteration 8400, lr = 0.01
I0428 18:15:40.780758  7412 sgd_solver.cpp:169] scale layer:0.593087 0.639051 0.679487 0.684636 0.657632 0.670317 0.669839 0.670385 0.670538 0.673358 0.672319 0.670267 0.667309 0.666151 0.663677 0.662734 0.660867 0.661309 0.663225 1.071729 
I0428 18:15:40.782758  7412 sgd_solver.cpp:200] weight diff/data:0.011407 0.008622 0.020649 0.014340 0.010534 0.013337 0.021280 0.013613 0.047447 0.024565 0.014867 0.025241 0.021776 0.012362 0.017831 0.020488 0.011795 0.015114 0.010458 0.007786 0.003562 
I0428 18:16:05.122150  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_8500.caffemodel
I0428 18:16:05.130151  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_8500.solverstate
I0428 18:16:05.134151  7412 solver.cpp:336] Iteration 8500, Testing net (#0)
I0428 18:16:13.880651 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:16:14.242672  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7844
I0428 18:16:14.243672  7412 solver.cpp:403]     Test net output #1: loss = 0.641876 (* 1 = 0.641876 loss)
I0428 18:16:14.472685  7412 solver.cpp:224] Iteration 8500 (2.96721 iter/s, 33.7017s/100 iters), loss = 0.368584
I0428 18:16:14.472685  7412 solver.cpp:243]     Train net output #0: loss = 0.368584 (* 1 = 0.368584 loss)
I0428 18:16:14.472685  7412 sgd_solver.cpp:137] Iteration 8500, lr = 0.01
I0428 18:16:14.480685  7412 sgd_solver.cpp:169] scale layer:0.589824 0.637409 0.678499 0.681093 0.657310 0.667387 0.667071 0.667744 0.667397 0.670745 0.669105 0.667729 0.664482 0.663119 0.661124 0.659754 0.657782 0.658126 0.660053 1.069319 
I0428 18:16:14.482686  7412 sgd_solver.cpp:200] weight diff/data:0.007346 0.026645 0.017123 0.011777 0.021232 0.028503 0.112309 0.075872 0.014485 0.013454 0.016665 0.020121 0.037588 0.013442 0.012385 0.011024 0.011783 0.014297 0.008633 0.006784 0.003069 
I0428 18:16:39.066092  7412 solver.cpp:224] Iteration 8600 (4.06608 iter/s, 24.5937s/100 iters), loss = 0.410418
I0428 18:16:39.066092  7412 solver.cpp:243]     Train net output #0: loss = 0.410418 (* 1 = 0.410418 loss)
I0428 18:16:39.066092  7412 sgd_solver.cpp:137] Iteration 8600, lr = 0.01
I0428 18:16:39.075093  7412 sgd_solver.cpp:169] scale layer:0.589746 0.633606 0.674618 0.679817 0.653527 0.664946 0.664025 0.665267 0.664654 0.668098 0.666960 0.665042 0.661886 0.660148 0.658483 0.656703 0.654530 0.654940 0.657043 1.068057 
I0428 18:16:39.077092  7412 sgd_solver.cpp:200] weight diff/data:0.007947 0.010939 0.009986 0.111168 0.012592 0.012949 0.015594 0.021183 0.016013 0.019031 0.017245 0.026470 0.016335 0.050565 0.015612 0.016870 0.038034 0.011210 0.011541 0.008032 0.002618 
I0428 18:17:03.658499  7412 solver.cpp:224] Iteration 8700 (4.06614 iter/s, 24.5934s/100 iters), loss = 0.273406
I0428 18:17:03.659498  7412 solver.cpp:243]     Train net output #0: loss = 0.273406 (* 1 = 0.273406 loss)
I0428 18:17:03.659498  7412 sgd_solver.cpp:137] Iteration 8700, lr = 0.01
I0428 18:17:03.668499  7412 sgd_solver.cpp:169] scale layer:0.588079 0.632075 0.671861 0.677264 0.651517 0.662326 0.661549 0.662503 0.661672 0.665576 0.663553 0.662278 0.658574 0.657184 0.655456 0.653345 0.651334 0.651721 0.653838 1.067983 
I0428 18:17:03.670500  7412 sgd_solver.cpp:200] weight diff/data:0.016814 0.008807 0.028884 0.013747 0.022998 0.014947 0.033479 0.018220 0.014840 0.035907 0.019141 0.026524 0.033333 0.014236 0.010271 0.043538 0.016206 0.019050 0.015175 0.008636 0.004157 
I0428 18:17:26.997833 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:17:28.211902  7412 solver.cpp:224] Iteration 8800 (4.07279 iter/s, 24.5532s/100 iters), loss = 0.604152
I0428 18:17:28.211902  7412 solver.cpp:243]     Train net output #0: loss = 0.604152 (* 1 = 0.604152 loss)
I0428 18:17:28.211902  7412 sgd_solver.cpp:137] Iteration 8800, lr = 0.01
I0428 18:17:28.220903  7412 sgd_solver.cpp:169] scale layer:0.584963 0.628554 0.670186 0.673843 0.650291 0.659210 0.658642 0.659508 0.659347 0.663047 0.660873 0.659077 0.655732 0.654227 0.652458 0.650276 0.648331 0.648695 0.650830 1.068927 
I0428 18:17:28.222903  7412 sgd_solver.cpp:200] weight diff/data:0.006378 0.009754 0.008897 0.010801 0.017907 0.009720 0.015948 0.021352 0.013793 0.011947 0.014807 0.020805 0.016537 0.011640 0.012344 0.055176 0.027815 0.012282 0.016547 0.037885 0.001382 
I0428 18:17:52.800309  7412 solver.cpp:224] Iteration 8900 (4.06689 iter/s, 24.5888s/100 iters), loss = 0.431867
I0428 18:17:52.800309  7412 solver.cpp:243]     Train net output #0: loss = 0.431867 (* 1 = 0.431867 loss)
I0428 18:17:52.800309  7412 sgd_solver.cpp:137] Iteration 8900, lr = 0.01
I0428 18:17:52.809309  7412 sgd_solver.cpp:169] scale layer:0.581767 0.627404 0.667420 0.671840 0.645941 0.655998 0.655887 0.656251 0.655840 0.660115 0.658375 0.656088 0.653280 0.651136 0.649267 0.647323 0.645347 0.645632 0.647788 1.068448 
I0428 18:17:52.812309  7412 sgd_solver.cpp:200] weight diff/data:0.007921 0.012889 0.013161 0.009753 0.010431 0.016447 0.015155 0.022078 0.025066 0.019631 0.016292 0.016651 0.016280 0.058498 0.015700 0.019334 0.010407 0.014690 0.011946 0.010978 0.002000 
I0428 18:18:17.175704  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_9000.caffemodel
I0428 18:18:17.183703  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_9000.solverstate
I0428 18:18:17.186704  7412 solver.cpp:336] Iteration 9000, Testing net (#0)
I0428 18:18:25.962206 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:18:26.326226  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7901
I0428 18:18:26.326226  7412 solver.cpp:403]     Test net output #1: loss = 0.627568 (* 1 = 0.627568 loss)
I0428 18:18:26.556239  7412 solver.cpp:224] Iteration 9000 (2.96239 iter/s, 33.7565s/100 iters), loss = 0.478016
I0428 18:18:26.556239  7412 solver.cpp:243]     Train net output #0: loss = 0.478016 (* 1 = 0.478016 loss)
I0428 18:18:26.556239  7412 sgd_solver.cpp:137] Iteration 9000, lr = 0.01
I0428 18:18:26.565240  7412 sgd_solver.cpp:169] scale layer:0.580105 0.625595 0.663931 0.670341 0.643151 0.653340 0.652644 0.653612 0.653339 0.657102 0.656019 0.653315 0.650540 0.648353 0.646676 0.644209 0.642306 0.642537 0.644853 1.069538 
I0428 18:18:26.567240  7412 sgd_solver.cpp:200] weight diff/data:0.021131 0.010910 0.013592 0.013413 0.040323 0.173485 0.015258 0.014341 0.012015 0.018738 0.016113 0.050572 0.015778 0.022662 0.115108 0.011983 0.044610 0.009971 0.008150 0.009279 0.002199 
I0428 18:18:51.190649  7412 solver.cpp:224] Iteration 9100 (4.0593 iter/s, 24.6348s/100 iters), loss = 0.346476
I0428 18:18:51.190649  7412 solver.cpp:243]     Train net output #0: loss = 0.346476 (* 1 = 0.346476 loss)
I0428 18:18:51.190649  7412 sgd_solver.cpp:137] Iteration 9100, lr = 0.01
I0428 18:18:51.198649  7412 sgd_solver.cpp:169] scale layer:0.576149 0.623386 0.660933 0.668313 0.642139 0.650989 0.650437 0.651083 0.650606 0.654576 0.653066 0.650532 0.647474 0.645311 0.643824 0.641097 0.639228 0.639349 0.641911 1.069965 
I0428 18:18:51.201649  7412 sgd_solver.cpp:200] weight diff/data:0.012351 0.015331 0.012961 0.021689 0.019380 0.013477 0.015006 0.019601 0.024181 0.022752 0.014727 0.024707 0.013668 0.023067 0.011346 0.012373 0.013745 0.008920 0.011668 0.009968 0.003229 
I0428 18:19:14.600987 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:19:15.817057  7412 solver.cpp:224] Iteration 9200 (4.06051 iter/s, 24.6275s/100 iters), loss = 0.557364
I0428 18:19:15.818058  7412 solver.cpp:243]     Train net output #0: loss = 0.557364 (* 1 = 0.557364 loss)
I0428 18:19:15.818058  7412 sgd_solver.cpp:137] Iteration 9200, lr = 0.01
I0428 18:19:15.827059  7412 sgd_solver.cpp:169] scale layer:0.575347 0.621562 0.659513 0.666564 0.639885 0.647910 0.647629 0.648288 0.648209 0.652216 0.651154 0.648185 0.644927 0.642515 0.640654 0.638353 0.636198 0.636271 0.638880 1.067956 
I0428 18:19:15.829058  7412 sgd_solver.cpp:200] weight diff/data:0.008470 0.008055 0.016982 0.021746 0.010876 0.014749 0.017104 0.013335 0.022893 0.014927 0.014173 0.014579 0.017044 0.013997 0.021596 0.030574 0.060296 0.010958 0.011128 0.009214 0.001434 
I0428 18:19:40.525470  7412 solver.cpp:224] Iteration 9300 (4.04713 iter/s, 24.7089s/100 iters), loss = 0.334233
I0428 18:19:40.526470  7412 solver.cpp:243]     Train net output #0: loss = 0.334233 (* 1 = 0.334233 loss)
I0428 18:19:40.526470  7412 sgd_solver.cpp:137] Iteration 9300, lr = 0.01
I0428 18:19:40.534471  7412 sgd_solver.cpp:169] scale layer:0.577173 0.617244 0.658159 0.663472 0.636200 0.645648 0.645550 0.645590 0.645300 0.648808 0.648632 0.645164 0.642717 0.639385 0.638471 0.635480 0.633162 0.633227 0.635794 1.067287 
I0428 18:19:40.536471  7412 sgd_solver.cpp:200] weight diff/data:0.021636 0.015819 0.015730 0.014754 0.011854 0.012183 0.020467 0.014864 0.014185 0.019153 0.014751 0.087945 0.012910 0.012788 0.011744 0.013882 0.011552 0.009676 0.009320 0.006738 0.002037 
I0428 18:20:05.171880  7412 solver.cpp:224] Iteration 9400 (4.05732 iter/s, 24.6468s/100 iters), loss = 0.446967
I0428 18:20:05.172880  7412 solver.cpp:243]     Train net output #0: loss = 0.446967 (* 1 = 0.446967 loss)
I0428 18:20:05.172880  7412 sgd_solver.cpp:137] Iteration 9400, lr = 0.01
I0428 18:20:05.180881  7412 sgd_solver.cpp:169] scale layer:0.573178 0.616544 0.657189 0.663616 0.632632 0.642756 0.642519 0.643295 0.642824 0.646240 0.645721 0.642386 0.639515 0.636734 0.635575 0.632586 0.630169 0.630240 0.632935 1.064980 
I0428 18:20:05.182881  7412 sgd_solver.cpp:200] weight diff/data:0.010300 0.017198 0.011864 0.012553 0.013428 0.012602 0.019851 0.013658 0.044162 0.018147 0.019807 0.016202 0.013298 0.014210 0.019399 0.012336 0.016891 0.014535 0.006205 0.007531 0.003076 
I0428 18:20:29.570276  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_9500.caffemodel
I0428 18:20:29.578276  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_9500.solverstate
I0428 18:20:29.582276  7412 solver.cpp:336] Iteration 9500, Testing net (#0)
I0428 18:20:38.359778 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:20:38.722800  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7877
I0428 18:20:38.722800  7412 solver.cpp:403]     Test net output #1: loss = 0.63541 (* 1 = 0.63541 loss)
I0428 18:20:38.957813  7412 solver.cpp:224] Iteration 9500 (2.95978 iter/s, 33.7863s/100 iters), loss = 0.247371
I0428 18:20:38.957813  7412 solver.cpp:243]     Train net output #0: loss = 0.247371 (* 1 = 0.247371 loss)
I0428 18:20:38.957813  7412 sgd_solver.cpp:137] Iteration 9500, lr = 0.01
I0428 18:20:38.965813  7412 sgd_solver.cpp:169] scale layer:0.566891 0.613283 0.654758 0.659313 0.629747 0.640685 0.639882 0.641081 0.640362 0.643700 0.643129 0.640051 0.636451 0.633873 0.632356 0.629247 0.627334 0.627198 0.629886 1.066695 
I0428 18:20:38.967813  7412 sgd_solver.cpp:200] weight diff/data:0.014720 0.014969 0.017352 0.012541 0.012708 0.010404 0.012698 0.014200 0.104014 0.010786 0.014126 0.012338 0.015366 0.023363 0.010674 0.019181 0.023925 0.013193 0.012516 0.009172 0.004378 
I0428 18:21:02.379153 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:21:03.601222  7412 solver.cpp:224] Iteration 9600 (4.05778 iter/s, 24.644s/100 iters), loss = 0.554636
I0428 18:21:03.601222  7412 solver.cpp:243]     Train net output #0: loss = 0.554636 (* 1 = 0.554636 loss)
I0428 18:21:03.601222  7412 sgd_solver.cpp:137] Iteration 9600, lr = 0.01
I0428 18:21:03.610222  7412 sgd_solver.cpp:169] scale layer:0.564346 0.613065 0.654191 0.657056 0.628673 0.637751 0.637183 0.638260 0.637905 0.641440 0.641081 0.637097 0.634176 0.631632 0.629612 0.626474 0.624391 0.624149 0.626974 1.064788 
I0428 18:21:03.612223  7412 sgd_solver.cpp:200] weight diff/data:0.005379 0.006635 0.014270 0.015559 0.016223 0.010790 0.019206 0.018141 0.015787 0.014138 0.013662 0.014798 0.012637 0.020854 0.011801 0.041226 0.029679 0.012277 0.010084 0.015594 0.002000 
I0428 18:21:28.216630  7412 solver.cpp:224] Iteration 9700 (4.06243 iter/s, 24.6158s/100 iters), loss = 0.353053
I0428 18:21:28.216630  7412 solver.cpp:243]     Train net output #0: loss = 0.353053 (* 1 = 0.353053 loss)
I0428 18:21:28.216630  7412 sgd_solver.cpp:137] Iteration 9700, lr = 0.01
I0428 18:21:28.225630  7412 sgd_solver.cpp:169] scale layer:0.558932 0.610827 0.650073 0.654070 0.628231 0.634985 0.634961 0.635692 0.634920 0.639083 0.638280 0.634953 0.631227 0.628784 0.626898 0.623827 0.621403 0.621249 0.624142 1.062835 
I0428 18:21:28.227630  7412 sgd_solver.cpp:200] weight diff/data:0.007112 0.010646 0.016253 0.035409 0.013931 0.024185 0.020194 0.020074 0.020480 0.017436 0.016797 0.020787 0.020163 0.014229 0.014728 0.024813 0.010994 0.010042 0.043729 0.007655 0.001207 
I0428 18:21:52.856040  7412 solver.cpp:224] Iteration 9800 (4.05857 iter/s, 24.6392s/100 iters), loss = 0.491583
I0428 18:21:52.856040  7412 solver.cpp:243]     Train net output #0: loss = 0.491583 (* 1 = 0.491583 loss)
I0428 18:21:52.856040  7412 sgd_solver.cpp:137] Iteration 9800, lr = 0.01
I0428 18:21:52.864040  7412 sgd_solver.cpp:169] scale layer:0.555092 0.604424 0.648330 0.653036 0.626889 0.632601 0.632379 0.633310 0.632556 0.636594 0.636579 0.632574 0.628341 0.625760 0.624595 0.621190 0.618361 0.618246 0.621205 1.062790 
I0428 18:21:52.866040  7412 sgd_solver.cpp:200] weight diff/data:0.007342 0.049487 0.011619 0.011445 0.015033 0.014910 0.023399 0.014436 0.016604 0.026504 0.013085 0.040867 0.015893 0.017368 0.021275 0.017154 0.016059 0.012438 0.012266 0.006693 0.002546 
I0428 18:22:17.473448  7412 solver.cpp:224] Iteration 9900 (4.06195 iter/s, 24.6187s/100 iters), loss = 0.300376
I0428 18:22:17.473448  7412 solver.cpp:243]     Train net output #0: loss = 0.300376 (* 1 = 0.300376 loss)
I0428 18:22:17.474447  7412 sgd_solver.cpp:137] Iteration 9900, lr = 0.01
I0428 18:22:17.482448  7412 sgd_solver.cpp:169] scale layer:0.551768 0.603692 0.647610 0.652047 0.625002 0.630301 0.630176 0.631107 0.629296 0.634381 0.634129 0.629941 0.625493 0.622945 0.621691 0.618305 0.615277 0.615315 0.618302 1.063497 
I0428 18:22:17.484448  7412 sgd_solver.cpp:200] weight diff/data:0.009516 0.028102 0.017383 0.015774 0.008479 0.051454 0.041995 0.013444 0.035727 0.010729 0.027405 0.019884 0.010739 0.018646 0.023990 0.010635 0.013678 0.008136 0.009720 0.012647 0.003374 
I0428 18:22:40.874786 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:22:41.855842  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_10000.caffemodel
I0428 18:22:41.864842  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_10000.solverstate
I0428 18:22:41.867842  7412 solver.cpp:336] Iteration 10000, Testing net (#0)
I0428 18:22:50.694347 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:22:51.060369  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7859
I0428 18:22:51.060369  7412 solver.cpp:403]     Test net output #1: loss = 0.64507 (* 1 = 0.64507 loss)
I0428 18:22:51.291381  7412 solver.cpp:224] Iteration 10000 (2.95695 iter/s, 33.8186s/100 iters), loss = 0.498306
I0428 18:22:51.291381  7412 solver.cpp:243]     Train net output #0: loss = 0.498306 (* 1 = 0.498306 loss)
I0428 18:22:51.291381  7412 sgd_solver.cpp:137] Iteration 10000, lr = 0.01
I0428 18:22:51.300382  7412 sgd_solver.cpp:169] scale layer:0.551138 0.601406 0.645654 0.649157 0.623838 0.627749 0.627504 0.628420 0.626818 0.631854 0.631407 0.627262 0.623070 0.620527 0.618538 0.615559 0.612298 0.612381 0.615518 1.062539 
I0428 18:22:51.301383  7412 sgd_solver.cpp:200] weight diff/data:0.012645 0.010788 0.008943 0.014355 0.011542 0.019435 0.017299 0.019189 0.025655 0.016677 0.020539 0.019650 0.015801 0.017488 0.011964 0.011998 0.043056 0.087115 0.011696 0.007838 0.003373 
I0428 18:23:15.909790  7412 solver.cpp:224] Iteration 10100 (4.06191 iter/s, 24.6189s/100 iters), loss = 0.487125
I0428 18:23:15.909790  7412 solver.cpp:243]     Train net output #0: loss = 0.487125 (* 1 = 0.487125 loss)
I0428 18:23:15.909790  7412 sgd_solver.cpp:137] Iteration 10100, lr = 0.01
I0428 18:23:15.919790  7412 sgd_solver.cpp:169] scale layer:0.551668 0.601362 0.642645 0.645980 0.623869 0.625884 0.624918 0.625988 0.624532 0.629486 0.629635 0.624249 0.620442 0.617751 0.616250 0.612794 0.609459 0.609503 0.612569 1.060388 
I0428 18:23:15.920790  7412 sgd_solver.cpp:200] weight diff/data:0.020892 0.011095 0.012014 0.011418 0.016381 0.014126 0.043194 0.016112 0.020628 0.033249 0.021856 0.015367 0.011000 0.012695 0.022332 0.022310 0.010162 0.016539 0.013640 0.011986 0.002633 
I0428 18:23:40.530198  7412 solver.cpp:224] Iteration 10200 (4.06159 iter/s, 24.6209s/100 iters), loss = 0.365655
I0428 18:23:40.530198  7412 solver.cpp:243]     Train net output #0: loss = 0.365655 (* 1 = 0.365655 loss)
I0428 18:23:40.531198  7412 sgd_solver.cpp:137] Iteration 10200, lr = 0.01
I0428 18:23:40.539198  7412 sgd_solver.cpp:169] scale layer:0.545904 0.597234 0.639628 0.643342 0.619175 0.623675 0.622450 0.623675 0.621787 0.627049 0.626701 0.622543 0.617790 0.615039 0.613834 0.610339 0.606691 0.606714 0.609822 1.061129 
I0428 18:23:40.541198  7412 sgd_solver.cpp:200] weight diff/data:0.007556 0.012536 0.010989 0.011455 0.016621 0.008675 0.016035 0.014397 0.016593 0.058847 0.053174 0.014003 0.016241 0.018347 0.018127 0.031597 0.012811 0.011632 0.012449 0.010268 0.001856 
I0428 18:24:05.194608  7412 solver.cpp:224] Iteration 10300 (4.05447 iter/s, 24.6642s/100 iters), loss = 0.341173
I0428 18:24:05.194608  7412 solver.cpp:243]     Train net output #0: loss = 0.341173 (* 1 = 0.341173 loss)
I0428 18:24:05.194608  7412 sgd_solver.cpp:137] Iteration 10300, lr = 0.01
I0428 18:24:05.203609  7412 sgd_solver.cpp:169] scale layer:0.544227 0.597333 0.637943 0.642092 0.615990 0.621487 0.620548 0.621159 0.619851 0.624675 0.625059 0.620634 0.615350 0.612482 0.611821 0.607288 0.603735 0.603749 0.607039 1.061845 
I0428 18:24:05.205610  7412 sgd_solver.cpp:200] weight diff/data:0.010215 0.011223 0.011678 0.014264 0.086052 0.035223 0.028492 0.015661 0.013335 0.013599 0.012559 0.019850 0.015355 0.028569 0.021190 0.024411 0.014036 0.011106 0.008331 0.010666 0.002907 
I0428 18:24:28.582947 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:24:29.805016  7412 solver.cpp:224] Iteration 10400 (4.06319 iter/s, 24.6112s/100 iters), loss = 0.443542
I0428 18:24:29.805016  7412 solver.cpp:243]     Train net output #0: loss = 0.443542 (* 1 = 0.443542 loss)
I0428 18:24:29.805016  7412 sgd_solver.cpp:137] Iteration 10400, lr = 0.01
I0428 18:24:29.813017  7412 sgd_solver.cpp:169] scale layer:0.540479 0.594571 0.636149 0.639993 0.612473 0.618590 0.617528 0.618146 0.617308 0.622606 0.623078 0.618340 0.612499 0.610323 0.608568 0.604552 0.600850 0.601001 0.604425 1.059834 
I0428 18:24:29.815017  7412 sgd_solver.cpp:200] weight diff/data:0.010213 0.011096 0.045011 0.013042 0.051562 0.012818 0.014970 0.075714 0.014750 0.013193 0.018827 0.018735 0.013914 0.014139 0.030055 0.013696 0.023997 0.018305 0.012766 0.012126 0.004766 
I0428 18:24:54.187412  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_10500.caffemodel
I0428 18:24:54.195411  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_10500.solverstate
I0428 18:24:54.198411  7412 solver.cpp:336] Iteration 10500, Testing net (#0)
I0428 18:25:02.949913 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:25:03.313933  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7914
I0428 18:25:03.313933  7412 solver.cpp:403]     Test net output #1: loss = 0.639616 (* 1 = 0.639616 loss)
I0428 18:25:03.547946  7412 solver.cpp:224] Iteration 10500 (2.96351 iter/s, 33.7438s/100 iters), loss = 0.428361
I0428 18:25:03.547946  7412 solver.cpp:243]     Train net output #0: loss = 0.428361 (* 1 = 0.428361 loss)
I0428 18:25:03.547946  7412 sgd_solver.cpp:137] Iteration 10500, lr = 0.01
I0428 18:25:03.556947  7412 sgd_solver.cpp:169] scale layer:0.542329 0.590087 0.633038 0.638045 0.609917 0.616412 0.615573 0.615734 0.614715 0.620131 0.620868 0.615844 0.611010 0.608174 0.606187 0.601955 0.598125 0.598101 0.601557 1.057675 
I0428 18:25:03.557947  7412 sgd_solver.cpp:200] weight diff/data:0.010412 0.009875 0.014050 0.017443 0.015110 0.021501 0.021344 0.015113 0.017677 0.021081 0.020482 0.062025 0.020641 0.018345 0.016520 0.014002 0.018304 0.011049 0.008992 0.018546 0.001780 
I0428 18:25:28.135352  7412 solver.cpp:224] Iteration 10600 (4.06705 iter/s, 24.5878s/100 iters), loss = 0.458506
I0428 18:25:28.135352  7412 solver.cpp:243]     Train net output #0: loss = 0.458506 (* 1 = 0.458506 loss)
I0428 18:25:28.135352  7412 sgd_solver.cpp:137] Iteration 10600, lr = 0.01
I0428 18:25:28.143353  7412 sgd_solver.cpp:169] scale layer:0.542366 0.589466 0.630685 0.637579 0.610024 0.614009 0.612703 0.613433 0.612162 0.618342 0.618679 0.613254 0.608211 0.605425 0.603691 0.598865 0.595236 0.595263 0.598777 1.058266 
I0428 18:25:28.145354  7412 sgd_solver.cpp:200] weight diff/data:0.007625 0.012814 0.011542 0.025354 0.014946 0.014979 0.014927 0.027521 0.015411 0.105559 0.015461 0.013651 0.013433 0.018633 0.047442 0.016028 0.020112 0.010454 0.017722 0.008123 0.050900 
I0428 18:25:52.756762  7412 solver.cpp:224] Iteration 10700 (4.06138 iter/s, 24.6222s/100 iters), loss = 0.327454
I0428 18:25:52.756762  7412 solver.cpp:243]     Train net output #0: loss = 0.327454 (* 1 = 0.327454 loss)
I0428 18:25:52.756762  7412 sgd_solver.cpp:137] Iteration 10700, lr = 0.01
I0428 18:25:52.766762  7412 sgd_solver.cpp:169] scale layer:0.543797 0.587778 0.630155 0.636135 0.605641 0.612130 0.610551 0.611346 0.609695 0.615811 0.616240 0.610476 0.605409 0.602845 0.600996 0.595724 0.592627 0.592410 0.595941 1.058605 
I0428 18:25:52.768761  7412 sgd_solver.cpp:200] weight diff/data:0.008992 0.038175 0.009940 0.010038 0.013143 0.018552 0.023670 0.036030 0.013662 0.031046 0.015832 0.052129 0.020402 0.012004 0.014439 0.014969 0.010213 0.015533 0.024737 0.009003 0.008674 
I0428 18:26:16.173100 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:26:17.394170  7412 solver.cpp:224] Iteration 10800 (4.0588 iter/s, 24.6378s/100 iters), loss = 0.51033
I0428 18:26:17.394170  7412 solver.cpp:243]     Train net output #0: loss = 0.51033 (* 1 = 0.51033 loss)
I0428 18:26:17.394170  7412 sgd_solver.cpp:137] Iteration 10800, lr = 0.01
I0428 18:26:17.403170  7412 sgd_solver.cpp:169] scale layer:0.543154 0.586515 0.628148 0.634444 0.605738 0.610228 0.608031 0.609008 0.608082 0.613762 0.613604 0.608552 0.603418 0.600151 0.598800 0.593376 0.589650 0.589626 0.593206 1.057610 
I0428 18:26:17.405171  7412 sgd_solver.cpp:200] weight diff/data:0.014352 0.014848 0.015429 0.015890 0.026952 0.050433 0.040115 0.015175 0.023567 0.018817 0.015771 0.018227 0.016621 0.020772 0.014219 0.029648 0.015759 0.025116 0.009251 0.007826 0.001536 
I0428 18:26:42.044580  7412 solver.cpp:224] Iteration 10900 (4.0566 iter/s, 24.6512s/100 iters), loss = 0.386439
I0428 18:26:42.045580  7412 solver.cpp:243]     Train net output #0: loss = 0.386439 (* 1 = 0.386439 loss)
I0428 18:26:42.045580  7412 sgd_solver.cpp:137] Iteration 10900, lr = 0.01
I0428 18:26:42.053580  7412 sgd_solver.cpp:169] scale layer:0.542876 0.586563 0.625865 0.632473 0.602368 0.607802 0.606885 0.606133 0.605566 0.611737 0.611554 0.605961 0.600948 0.597638 0.596706 0.590895 0.587003 0.586948 0.590469 1.057500 
I0428 18:26:42.055580  7412 sgd_solver.cpp:200] weight diff/data:0.265501 0.016330 0.013825 0.019113 0.013366 0.048272 0.017700 0.026421 0.014860 0.018189 0.016897 0.020462 0.027006 0.020634 0.028877 0.017312 0.014263 0.011245 0.023124 0.009430 0.008144 
I0428 18:27:06.392972  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_11000.caffemodel
I0428 18:27:06.400974  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_11000.solverstate
I0428 18:27:06.404973  7412 solver.cpp:336] Iteration 11000, Testing net (#0)
I0428 18:27:15.158474 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:27:15.520495  7412 solver.cpp:403]     Test net output #0: accuracy = 0.792
I0428 18:27:15.520495  7412 solver.cpp:403]     Test net output #1: loss = 0.632578 (* 1 = 0.632578 loss)
I0428 18:27:15.752508  7412 solver.cpp:224] Iteration 11000 (2.96661 iter/s, 33.7085s/100 iters), loss = 0.350429
I0428 18:27:15.752508  7412 solver.cpp:243]     Train net output #0: loss = 0.350429 (* 1 = 0.350429 loss)
I0428 18:27:15.752508  7412 sgd_solver.cpp:137] Iteration 11000, lr = 0.01
I0428 18:27:15.760509  7412 sgd_solver.cpp:169] scale layer:0.537628 0.583621 0.624425 0.629857 0.598761 0.605410 0.604443 0.604325 0.603667 0.609701 0.609305 0.603970 0.599355 0.595173 0.594345 0.588587 0.584081 0.584157 0.587838 1.057875 
I0428 18:27:15.762508  7412 sgd_solver.cpp:200] weight diff/data:0.008797 0.009618 0.011933 0.013533 0.012360 0.022749 0.012734 0.092621 0.013477 0.011839 0.014958 0.016743 0.018148 0.012318 0.016000 0.021371 0.016042 0.010636 0.015925 0.008620 0.001600 
I0428 18:27:40.359915  7412 solver.cpp:224] Iteration 11100 (4.06382 iter/s, 24.6074s/100 iters), loss = 0.357111
I0428 18:27:40.359915  7412 solver.cpp:243]     Train net output #0: loss = 0.357111 (* 1 = 0.357111 loss)
I0428 18:27:40.359915  7412 sgd_solver.cpp:137] Iteration 11100, lr = 0.01
I0428 18:27:40.367916  7412 sgd_solver.cpp:169] scale layer:0.533067 0.580375 0.621877 0.628502 0.595862 0.603735 0.602315 0.602605 0.601372 0.607700 0.607450 0.602180 0.596947 0.592118 0.591773 0.585651 0.581326 0.581166 0.585116 1.057970 
I0428 18:27:40.369916  7412 sgd_solver.cpp:200] weight diff/data:0.008601 0.013743 0.015639 0.070445 0.013214 0.012016 0.032013 0.018232 0.011852 0.026550 0.017602 0.013366 0.013604 0.014477 0.016683 0.025464 0.016210 0.198325 0.009961 0.023168 0.307284 
I0428 18:28:03.788255 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:28:05.005326  7412 solver.cpp:224] Iteration 11200 (4.05734 iter/s, 24.6467s/100 iters), loss = 0.54162
I0428 18:28:05.005326  7412 solver.cpp:243]     Train net output #0: loss = 0.54162 (* 1 = 0.54162 loss)
I0428 18:28:05.006325  7412 sgd_solver.cpp:137] Iteration 11200, lr = 0.01
I0428 18:28:05.015326  7412 sgd_solver.cpp:169] scale layer:0.531394 0.577962 0.619553 0.626682 0.592547 0.601663 0.600179 0.599695 0.599643 0.605845 0.605955 0.600301 0.594688 0.590118 0.589234 0.583223 0.578772 0.578503 0.582512 1.058070 
I0428 18:28:05.017326  7412 sgd_solver.cpp:200] weight diff/data:0.009427 0.015985 0.017975 0.011216 0.012096 0.016180 0.019580 0.020421 0.019831 0.012369 0.028184 0.015826 0.020595 0.016191 0.011547 0.025811 0.013514 0.050201 0.013909 0.006569 0.033726 
I0428 18:28:29.629734  7412 solver.cpp:224] Iteration 11300 (4.06104 iter/s, 24.6242s/100 iters), loss = 0.373706
I0428 18:28:29.629734  7412 solver.cpp:243]     Train net output #0: loss = 0.373706 (* 1 = 0.373706 loss)
I0428 18:28:29.629734  7412 sgd_solver.cpp:137] Iteration 11300, lr = 0.01
I0428 18:28:29.637734  7412 sgd_solver.cpp:169] scale layer:0.533330 0.572522 0.617215 0.623332 0.590893 0.598975 0.597805 0.597135 0.597513 0.603781 0.603520 0.597301 0.592844 0.588105 0.587188 0.580579 0.576294 0.575807 0.579845 1.055357 
I0428 18:28:29.639734  7412 sgd_solver.cpp:200] weight diff/data:0.014038 0.015286 0.017616 0.018452 0.015086 0.012335 0.018515 0.033374 0.014479 0.015731 0.014721 0.014203 0.018487 0.026502 0.045277 0.021012 0.017778 0.012848 0.019804 0.020124 0.007415 
I0428 18:28:54.252142  7412 solver.cpp:224] Iteration 11400 (4.06123 iter/s, 24.6231s/100 iters), loss = 0.430643
I0428 18:28:54.252142  7412 solver.cpp:243]     Train net output #0: loss = 0.430642 (* 1 = 0.430642 loss)
I0428 18:28:54.252142  7412 sgd_solver.cpp:137] Iteration 11400, lr = 0.01
I0428 18:28:54.261142  7412 sgd_solver.cpp:169] scale layer:0.528886 0.570123 0.615613 0.622791 0.590311 0.597427 0.595384 0.595345 0.595064 0.601020 0.601635 0.595579 0.590584 0.585762 0.584742 0.578382 0.573766 0.573223 0.577233 1.053804 
I0428 18:28:54.263142  7412 sgd_solver.cpp:200] weight diff/data:0.007348 0.010346 0.011114 0.011547 0.008253 0.014570 0.024169 0.012814 0.013023 0.013541 0.015292 1.107839 0.023385 0.014804 0.014277 0.012418 0.010823 0.010047 0.013218 0.012756 0.001801 
I0428 18:29:18.615535  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_11500.caffemodel
I0428 18:29:18.624536  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_11500.solverstate
I0428 18:29:18.627537  7412 solver.cpp:336] Iteration 11500, Testing net (#0)
I0428 18:29:27.394037 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:29:27.758059  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7945
I0428 18:29:27.758059  7412 solver.cpp:403]     Test net output #1: loss = 0.628377 (* 1 = 0.628377 loss)
I0428 18:29:27.993072  7412 solver.cpp:224] Iteration 11500 (2.96367 iter/s, 33.7419s/100 iters), loss = 0.324491
I0428 18:29:27.993072  7412 solver.cpp:243]     Train net output #0: loss = 0.324491 (* 1 = 0.324491 loss)
I0428 18:29:27.993072  7412 sgd_solver.cpp:137] Iteration 11500, lr = 0.01
I0428 18:29:28.002073  7412 sgd_solver.cpp:169] scale layer:0.526313 0.571744 0.616250 0.621313 0.588245 0.595179 0.594091 0.593572 0.593182 0.598847 0.599231 0.593619 0.587742 0.583614 0.582382 0.575554 0.570791 0.570608 0.574661 1.054951 
I0428 18:29:28.004072  7412 sgd_solver.cpp:200] weight diff/data:0.011630 0.008764 0.013342 0.010913 0.011224 0.010569 0.014573 0.012112 0.014010 0.013278 0.054204 0.015724 0.017711 0.015750 0.013718 0.052506 0.016795 0.025525 0.020835 0.010798 0.002822 
I0428 18:29:51.400410 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:29:52.612480  7412 solver.cpp:224] Iteration 11600 (4.06186 iter/s, 24.6192s/100 iters), loss = 0.467112
I0428 18:29:52.612480  7412 solver.cpp:243]     Train net output #0: loss = 0.467112 (* 1 = 0.467112 loss)
I0428 18:29:52.612480  7412 sgd_solver.cpp:137] Iteration 11600, lr = 0.01
I0428 18:29:52.621480  7412 sgd_solver.cpp:169] scale layer:0.524374 0.572536 0.613035 0.620702 0.587681 0.593138 0.591653 0.591134 0.590841 0.596829 0.596634 0.591389 0.585879 0.581481 0.579401 0.572791 0.568112 0.568005 0.572085 1.056574 
I0428 18:29:52.623481  7412 sgd_solver.cpp:200] weight diff/data:0.019206 0.018503 0.016074 0.029163 0.012871 0.040194 0.014503 0.011999 0.030653 0.013642 0.026493 0.012051 0.024270 0.018957 0.014836 0.013089 0.030430 0.009920 0.007618 0.012619 0.001955 
I0428 18:30:17.218888  7412 solver.cpp:224] Iteration 11700 (4.06376 iter/s, 24.6077s/100 iters), loss = 0.3765
I0428 18:30:17.219887  7412 solver.cpp:243]     Train net output #0: loss = 0.3765 (* 1 = 0.3765 loss)
I0428 18:30:17.219887  7412 sgd_solver.cpp:137] Iteration 11700, lr = 0.01
I0428 18:30:17.227888  7412 sgd_solver.cpp:169] scale layer:0.522798 0.570451 0.612646 0.617942 0.587161 0.590814 0.590269 0.589347 0.589156 0.594594 0.594887 0.589325 0.583914 0.580109 0.577736 0.570442 0.565881 0.565277 0.569547 1.054159 
I0428 18:30:17.229888  7412 sgd_solver.cpp:200] weight diff/data:0.013528 0.016885 0.028378 0.012405 0.067567 0.020321 0.014779 0.019700 0.016574 0.016046 0.020216 0.021414 0.031216 0.015041 0.013900 0.031245 0.017144 0.010343 0.017864 0.012177 0.002015 
I0428 18:30:41.840296  7412 solver.cpp:224] Iteration 11800 (4.06159 iter/s, 24.6209s/100 iters), loss = 0.376856
I0428 18:30:41.840296  7412 solver.cpp:243]     Train net output #0: loss = 0.376856 (* 1 = 0.376856 loss)
I0428 18:30:41.840296  7412 sgd_solver.cpp:137] Iteration 11800, lr = 0.01
I0428 18:30:41.848296  7412 sgd_solver.cpp:169] scale layer:0.521278 0.570528 0.611722 0.616952 0.588825 0.588499 0.588152 0.587078 0.586580 0.592541 0.592854 0.586806 0.581796 0.578299 0.575701 0.568442 0.563487 0.562764 0.567069 1.055715 
I0428 18:30:41.850296  7412 sgd_solver.cpp:200] weight diff/data:0.033357 0.011325 0.011306 0.099145 0.013303 0.013135 0.018297 0.016237 0.013095 0.017282 0.017030 0.024635 0.098846 0.014713 0.024116 0.022011 0.013317 0.011688 0.009202 0.063360 0.001914 
I0428 18:31:06.437702  7412 solver.cpp:224] Iteration 11900 (4.06528 iter/s, 24.5986s/100 iters), loss = 0.312209
I0428 18:31:06.437702  7412 solver.cpp:243]     Train net output #0: loss = 0.312209 (* 1 = 0.312209 loss)
I0428 18:31:06.437702  7412 sgd_solver.cpp:137] Iteration 11900, lr = 0.01
I0428 18:31:06.446703  7412 sgd_solver.cpp:169] scale layer:0.519390 0.570053 0.610017 0.614786 0.586171 0.587402 0.586472 0.585765 0.585089 0.590754 0.591255 0.584893 0.579310 0.575386 0.573386 0.565720 0.561034 0.560055 0.564502 1.053993 
I0428 18:31:06.448704  7412 sgd_solver.cpp:200] weight diff/data:0.008235 0.008963 0.010890 0.012485 0.009249 0.012828 0.014953 0.012781 0.015460 0.014493 0.015788 0.026419 0.038417 0.011407 0.012312 0.017801 0.022591 0.023857 0.007732 0.009894 0.005297 
I0428 18:31:29.832041 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:31:30.823097  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_12000.caffemodel
I0428 18:31:30.832098  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_12000.solverstate
I0428 18:31:30.835098  7412 solver.cpp:336] Iteration 12000, Testing net (#0)
I0428 18:31:39.608599 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:31:39.970620  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7948
I0428 18:31:39.970620  7412 solver.cpp:403]     Test net output #1: loss = 0.624197 (* 1 = 0.624197 loss)
I0428 18:31:40.206634  7412 solver.cpp:224] Iteration 12000 (2.96127 iter/s, 33.7693s/100 iters), loss = 0.467006
I0428 18:31:40.206634  7412 solver.cpp:243]     Train net output #0: loss = 0.467006 (* 1 = 0.467006 loss)
I0428 18:31:40.206634  7412 sgd_solver.cpp:137] Iteration 12000, lr = 0.01
I0428 18:31:40.215634  7412 sgd_solver.cpp:169] scale layer:0.515557 0.566561 0.608847 0.613409 0.584304 0.585045 0.584920 0.583543 0.583023 0.589127 0.589311 0.583384 0.577672 0.573631 0.570508 0.563313 0.558433 0.557616 0.562032 1.050415 
I0428 18:31:40.216634  7412 sgd_solver.cpp:200] weight diff/data:0.012392 0.012595 0.015957 0.010952 0.009926 0.017435 0.015021 0.033193 0.014487 0.021855 0.017145 0.021263 0.013246 0.014534 0.029251 0.014963 0.019721 0.014464 0.010790 0.018840 0.003134 
I0428 18:32:04.820042  7412 solver.cpp:224] Iteration 12100 (4.06267 iter/s, 24.6144s/100 iters), loss = 0.358534
I0428 18:32:04.820042  7412 solver.cpp:243]     Train net output #0: loss = 0.358534 (* 1 = 0.358534 loss)
I0428 18:32:04.820042  7412 sgd_solver.cpp:137] Iteration 12100, lr = 0.01
I0428 18:32:04.829042  7412 sgd_solver.cpp:169] scale layer:0.511160 0.563123 0.606123 0.611922 0.582900 0.583346 0.582842 0.581599 0.580986 0.587277 0.587421 0.580758 0.575591 0.571021 0.568469 0.561159 0.555720 0.554972 0.559613 1.051306 
I0428 18:32:04.831043  7412 sgd_solver.cpp:200] weight diff/data:0.036239 0.010556 0.009411 0.017344 0.013827 0.013517 0.015027 0.019852 0.016467 0.034243 0.015043 0.016445 0.018473 0.050788 0.024760 0.015142 0.017476 0.014380 0.021074 0.009387 0.002720 
I0428 18:32:29.453450  7412 solver.cpp:224] Iteration 12200 (4.05951 iter/s, 24.6335s/100 iters), loss = 0.397557
I0428 18:32:29.453450  7412 solver.cpp:243]     Train net output #0: loss = 0.397557 (* 1 = 0.397557 loss)
I0428 18:32:29.453450  7412 sgd_solver.cpp:137] Iteration 12200, lr = 0.01
I0428 18:32:29.461452  7412 sgd_solver.cpp:169] scale layer:0.506896 0.561084 0.605306 0.610788 0.580950 0.581328 0.580895 0.579982 0.579551 0.585446 0.586130 0.579083 0.574069 0.569147 0.566957 0.558897 0.553147 0.552498 0.557073 1.048665 
I0428 18:32:29.463451  7412 sgd_solver.cpp:200] weight diff/data:0.019130 0.015150 0.014852 0.013592 0.011473 0.015853 0.014588 0.014739 0.034146 0.013572 0.024466 0.014429 0.011438 0.018528 0.027779 0.013498 0.014237 0.009096 0.009067 0.010470 0.001856 
I0428 18:32:54.050858  7412 solver.cpp:224] Iteration 12300 (4.06534 iter/s, 24.5982s/100 iters), loss = 0.302708
I0428 18:32:54.050858  7412 solver.cpp:243]     Train net output #0: loss = 0.302708 (* 1 = 0.302708 loss)
I0428 18:32:54.050858  7412 sgd_solver.cpp:137] Iteration 12300, lr = 0.01
I0428 18:32:54.059859  7412 sgd_solver.cpp:169] scale layer:0.506758 0.559652 0.602056 0.608413 0.578169 0.579101 0.579613 0.578136 0.576852 0.583178 0.584760 0.577299 0.571850 0.566863 0.564538 0.556298 0.550580 0.549881 0.554764 1.050440 
I0428 18:32:54.061858  7412 sgd_solver.cpp:200] weight diff/data:0.018951 0.009297 0.019970 0.009324 0.011248 0.042511 0.014434 0.012204 0.013766 0.011636 0.021003 0.013307 0.014395 0.012191 0.013735 0.014245 0.010816 0.009274 0.014026 0.008734 0.002212 
I0428 18:33:17.441195 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:33:18.664265  7412 solver.cpp:224] Iteration 12400 (4.06272 iter/s, 24.6141s/100 iters), loss = 0.56597
I0428 18:33:18.665266  7412 solver.cpp:243]     Train net output #0: loss = 0.56597 (* 1 = 0.56597 loss)
I0428 18:33:18.665266  7412 sgd_solver.cpp:137] Iteration 12400, lr = 0.01
I0428 18:33:18.673266  7412 sgd_solver.cpp:169] scale layer:0.511356 0.558178 0.599341 0.607416 0.577018 0.577537 0.577597 0.576573 0.575480 0.581489 0.582642 0.576051 0.570017 0.564234 0.562044 0.554139 0.548231 0.547377 0.552298 1.049894 
I0428 18:33:18.675266  7412 sgd_solver.cpp:200] weight diff/data:0.013333 0.008656 0.012123 0.012582 0.013186 0.038719 0.023466 0.019328 0.022114 0.016111 0.014837 0.020545 0.014282 0.014788 0.014390 0.037737 0.013486 0.012503 0.018081 0.013321 0.001886 
I0428 18:33:43.054661  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_12500.caffemodel
I0428 18:33:43.062661  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_12500.solverstate
I0428 18:33:43.066661  7412 solver.cpp:336] Iteration 12500, Testing net (#0)
I0428 18:33:51.837163 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:33:52.201184  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7909
I0428 18:33:52.201184  7412 solver.cpp:403]     Test net output #1: loss = 0.642116 (* 1 = 0.642116 loss)
I0428 18:33:52.437197  7412 solver.cpp:224] Iteration 12500 (2.96091 iter/s, 33.7734s/100 iters), loss = 0.371425
I0428 18:33:52.437197  7412 solver.cpp:243]     Train net output #0: loss = 0.371425 (* 1 = 0.371425 loss)
I0428 18:33:52.437197  7412 sgd_solver.cpp:137] Iteration 12500, lr = 0.01
I0428 18:33:52.446197  7412 sgd_solver.cpp:169] scale layer:0.510371 0.556051 0.600487 0.605402 0.573484 0.576408 0.577232 0.573919 0.573644 0.579504 0.580975 0.574190 0.568661 0.562779 0.559626 0.551419 0.545677 0.544786 0.549969 1.046503 
I0428 18:33:52.448197  7412 sgd_solver.cpp:200] weight diff/data:0.012519 0.009971 0.010297 0.013885 0.013633 0.015152 0.013327 0.018256 0.012550 0.017072 0.068353 0.027966 0.022847 0.017656 0.011567 0.013373 0.011960 0.010739 0.007707 0.016079 0.001697 
I0428 18:34:17.029603  7412 solver.cpp:224] Iteration 12600 (4.0663 iter/s, 24.5924s/100 iters), loss = 0.3351
I0428 18:34:17.029603  7412 solver.cpp:243]     Train net output #0: loss = 0.3351 (* 1 = 0.3351 loss)
I0428 18:34:17.029603  7412 sgd_solver.cpp:137] Iteration 12600, lr = 0.01
I0428 18:34:17.037605  7412 sgd_solver.cpp:169] scale layer:0.509682 0.555649 0.598548 0.604432 0.574413 0.575005 0.575171 0.572395 0.572364 0.577922 0.580421 0.572025 0.566099 0.560820 0.557178 0.549326 0.543365 0.542552 0.547594 1.046309 
I0428 18:34:17.039604  7412 sgd_solver.cpp:200] weight diff/data:0.009210 0.012870 0.373060 0.031757 0.017540 0.019326 0.017008 0.049176 0.013186 0.017017 0.016844 0.034062 0.028911 0.044207 0.020205 0.013970 0.016598 0.016231 0.014133 0.010172 0.002563 
I0428 18:34:41.644011  7412 solver.cpp:224] Iteration 12700 (4.06253 iter/s, 24.6152s/100 iters), loss = 0.340432
I0428 18:34:41.644011  7412 solver.cpp:243]     Train net output #0: loss = 0.340432 (* 1 = 0.340432 loss)
I0428 18:34:41.644011  7412 sgd_solver.cpp:137] Iteration 12700, lr = 0.01
I0428 18:34:41.652012  7412 sgd_solver.cpp:169] scale layer:0.509557 0.553057 0.596746 0.603366 0.571847 0.572789 0.573315 0.570933 0.570154 0.576536 0.578020 0.570618 0.564070 0.558946 0.555201 0.546984 0.540879 0.539869 0.545134 1.048889 
I0428 18:34:41.654012  7412 sgd_solver.cpp:200] weight diff/data:0.009744 0.014693 0.022510 0.011920 0.014971 0.900835 0.016525 0.054789 0.030632 0.016458 0.011766 0.015476 0.013468 0.033916 0.017524 0.012795 0.027812 0.010549 0.015527 0.010345 0.008446 
I0428 18:35:05.031349 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:35:06.257419  7412 solver.cpp:224] Iteration 12800 (4.06272 iter/s, 24.614s/100 iters), loss = 0.344557
I0428 18:35:06.257419  7412 solver.cpp:243]     Train net output #0: loss = 0.344557 (* 1 = 0.344557 loss)
I0428 18:35:06.257419  7412 sgd_solver.cpp:137] Iteration 12800, lr = 0.01
I0428 18:35:06.266420  7412 sgd_solver.cpp:169] scale layer:0.506836 0.549821 0.594405 0.602553 0.570378 0.570476 0.571552 0.569047 0.568550 0.575175 0.576285 0.570251 0.562387 0.556449 0.552789 0.544926 0.538504 0.537442 0.542826 1.046130 
I0428 18:35:06.268420  7412 sgd_solver.cpp:200] weight diff/data:0.016201 0.017822 0.012470 0.012055 0.020962 0.040544 0.020338 0.012105 0.017124 0.017404 0.016736 0.025737 0.012631 0.016624 0.021319 0.015764 0.021002 0.011726 0.011481 0.009652 0.001612 
I0428 18:35:30.857826  7412 solver.cpp:224] Iteration 12900 (4.06491 iter/s, 24.6008s/100 iters), loss = 0.287975
I0428 18:35:30.857826  7412 solver.cpp:243]     Train net output #0: loss = 0.287975 (* 1 = 0.287975 loss)
I0428 18:35:30.857826  7412 sgd_solver.cpp:137] Iteration 12900, lr = 0.01
I0428 18:35:30.866827  7412 sgd_solver.cpp:169] scale layer:0.506320 0.547695 0.594074 0.601693 0.569201 0.568717 0.569916 0.567151 0.566464 0.572938 0.574483 0.568114 0.561287 0.554466 0.551536 0.542315 0.536373 0.535076 0.540508 1.044796 
I0428 18:35:30.868827  7412 sgd_solver.cpp:200] weight diff/data:0.013929 0.010490 0.010119 0.172431 0.013815 0.024094 0.014423 0.015455 0.047927 0.025431 0.014400 0.020102 0.017792 0.014993 0.014024 0.019509 0.015624 0.015086 0.011153 0.012224 0.001530 
I0428 18:35:55.219220  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_13000.caffemodel
I0428 18:35:55.228220  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_13000.solverstate
I0428 18:35:55.231220  7412 solver.cpp:336] Iteration 13000, Testing net (#0)
I0428 18:36:03.988721 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:36:04.349742  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8008
I0428 18:36:04.350742  7412 solver.cpp:403]     Test net output #1: loss = 0.626634 (* 1 = 0.626634 loss)
I0428 18:36:04.583755  7412 solver.cpp:224] Iteration 13000 (2.96504 iter/s, 33.7264s/100 iters), loss = 0.352926
I0428 18:36:04.583755  7412 solver.cpp:243]     Train net output #0: loss = 0.352926 (* 1 = 0.352926 loss)
I0428 18:36:04.583755  7412 sgd_solver.cpp:137] Iteration 13000, lr = 0.01
I0428 18:36:04.591756  7412 sgd_solver.cpp:169] scale layer:0.505424 0.550104 0.592825 0.600889 0.570211 0.568015 0.567970 0.566054 0.565184 0.570937 0.572734 0.565841 0.558650 0.552312 0.549332 0.539935 0.534124 0.532747 0.538125 1.045041 
I0428 18:36:04.593756  7412 sgd_solver.cpp:200] weight diff/data:0.013269 0.008386 0.013398 0.011177 1.994408 0.017081 0.015692 0.132500 0.018522 0.018735 0.013717 0.032002 0.012450 0.022563 0.030411 0.015739 0.016642 0.024301 0.013452 0.009988 0.002312 
I0428 18:36:29.212164  7412 solver.cpp:224] Iteration 13100 (4.0603 iter/s, 24.6287s/100 iters), loss = 0.272851
I0428 18:36:29.212164  7412 solver.cpp:243]     Train net output #0: loss = 0.272851 (* 1 = 0.272851 loss)
I0428 18:36:29.212164  7412 sgd_solver.cpp:137] Iteration 13100, lr = 0.01
I0428 18:36:29.220165  7412 sgd_solver.cpp:169] scale layer:0.505113 0.549246 0.592055 0.599523 0.568102 0.566095 0.565552 0.564323 0.563367 0.569461 0.571656 0.564410 0.556253 0.550251 0.547543 0.537942 0.531630 0.530146 0.535687 1.046934 
I0428 18:36:29.222164  7412 sgd_solver.cpp:200] weight diff/data:0.008596 0.025178 0.010509 0.009126 0.016005 0.009985 0.017012 0.092577 0.013653 0.017571 0.011538 0.021646 0.018602 0.014008 0.057120 0.016220 0.017583 0.012639 0.011384 0.012232 0.004559 
I0428 18:36:52.581501 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:36:53.800570  7412 solver.cpp:224] Iteration 13200 (4.06676 iter/s, 24.5896s/100 iters), loss = 0.523114
I0428 18:36:53.801570  7412 solver.cpp:243]     Train net output #0: loss = 0.523114 (* 1 = 0.523114 loss)
I0428 18:36:53.801570  7412 sgd_solver.cpp:137] Iteration 13200, lr = 0.01
I0428 18:36:53.809571  7412 sgd_solver.cpp:169] scale layer:0.502650 0.546191 0.591432 0.600170 0.567391 0.563970 0.564460 0.562292 0.562147 0.568214 0.569828 0.562717 0.554293 0.548110 0.544819 0.535297 0.529171 0.527873 0.533501 1.044961 
I0428 18:36:53.811571  7412 sgd_solver.cpp:200] weight diff/data:0.012656 0.007217 0.014814 0.016366 0.014156 0.037619 0.015249 0.014963 0.014496 0.034215 0.016813 0.033964 0.024004 0.016476 0.012744 0.017707 0.013232 0.012433 0.015086 0.013492 0.004238 
I0428 18:37:18.392977  7412 solver.cpp:224] Iteration 13300 (4.06635 iter/s, 24.5921s/100 iters), loss = 0.286907
I0428 18:37:18.392977  7412 solver.cpp:243]     Train net output #0: loss = 0.286907 (* 1 = 0.286907 loss)
I0428 18:37:18.392977  7412 sgd_solver.cpp:137] Iteration 13300, lr = 0.01
I0428 18:37:18.400977  7412 sgd_solver.cpp:169] scale layer:0.498631 0.542771 0.589032 0.595927 0.567884 0.561913 0.562954 0.560123 0.560088 0.566379 0.567809 0.560446 0.553082 0.546905 0.542894 0.533022 0.527065 0.525445 0.531296 1.044482 
I0428 18:37:18.402977  7412 sgd_solver.cpp:200] weight diff/data:0.010920 0.008546 0.012753 0.015312 0.008378 0.015981 0.019774 0.036227 0.017229 0.016760 0.026285 0.013161 0.032489 0.013721 0.017379 0.013821 0.010821 0.010670 0.008210 0.009203 0.001876 
I0428 18:37:42.973383  7412 solver.cpp:224] Iteration 13400 (4.0681 iter/s, 24.5815s/100 iters), loss = 0.378879
I0428 18:37:42.973383  7412 solver.cpp:243]     Train net output #0: loss = 0.378879 (* 1 = 0.378879 loss)
I0428 18:37:42.973383  7412 sgd_solver.cpp:137] Iteration 13400, lr = 0.01
I0428 18:37:42.982383  7412 sgd_solver.cpp:169] scale layer:0.497797 0.539655 0.586465 0.595922 0.567919 0.560139 0.560616 0.558434 0.557153 0.564813 0.566406 0.559691 0.550845 0.544755 0.541094 0.531418 0.524353 0.523150 0.529042 1.043601 
I0428 18:37:42.984383  7412 sgd_solver.cpp:200] weight diff/data:0.017801 0.015402 0.025843 0.013092 0.010335 0.016632 0.038979 0.014913 0.012555 0.017103 0.019025 0.014034 0.012740 0.017407 0.012749 0.015308 0.014974 0.015247 0.010624 0.009744 0.001783 
I0428 18:38:07.370779  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_13500.caffemodel
I0428 18:38:07.378779  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_13500.solverstate
I0428 18:38:07.381779  7412 solver.cpp:336] Iteration 13500, Testing net (#0)
I0428 18:38:16.134279 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:38:16.498301  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7901
I0428 18:38:16.498301  7412 solver.cpp:403]     Test net output #1: loss = 0.66414 (* 1 = 0.66414 loss)
I0428 18:38:16.731314  7412 solver.cpp:224] Iteration 13500 (2.96222 iter/s, 33.7584s/100 iters), loss = 0.220391
I0428 18:38:16.731314  7412 solver.cpp:243]     Train net output #0: loss = 0.220391 (* 1 = 0.220391 loss)
I0428 18:38:16.731314  7412 sgd_solver.cpp:137] Iteration 13500, lr = 0.01
I0428 18:38:16.739315  7412 sgd_solver.cpp:169] scale layer:0.497363 0.537755 0.584476 0.594456 0.562922 0.560103 0.558829 0.556751 0.556634 0.563614 0.564985 0.558027 0.548998 0.542638 0.539391 0.528587 0.522378 0.520584 0.526860 1.045569 
I0428 18:38:16.741314  7412 sgd_solver.cpp:200] weight diff/data:0.010996 0.010327 0.013279 0.009927 0.011863 0.015590 0.080312 0.013009 0.016327 0.024538 0.011936 0.014650 0.018714 0.019824 0.126762 0.023313 0.012015 0.011047 0.009486 0.008064 0.002532 
I0428 18:38:40.115651 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:38:41.338721  7412 solver.cpp:224] Iteration 13600 (4.06371 iter/s, 24.6081s/100 iters), loss = 0.534651
I0428 18:38:41.338721  7412 solver.cpp:243]     Train net output #0: loss = 0.534651 (* 1 = 0.534651 loss)
I0428 18:38:41.338721  7412 sgd_solver.cpp:137] Iteration 13600, lr = 0.01
I0428 18:38:41.347723  7412 sgd_solver.cpp:169] scale layer:0.500239 0.534711 0.582976 0.593983 0.560186 0.558370 0.557174 0.554833 0.554743 0.561872 0.563425 0.556852 0.547258 0.540014 0.536994 0.526922 0.520553 0.518371 0.524654 1.044085 
I0428 18:38:41.349722  7412 sgd_solver.cpp:200] weight diff/data:0.018068 0.010353 0.015122 0.010071 0.015178 0.011863 0.022019 0.024593 0.017877 0.011872 0.016927 0.014289 0.022589 0.015713 0.027526 0.031752 0.019984 0.013080 0.026920 0.060344 0.001882 
I0428 18:39:05.954129  7412 solver.cpp:224] Iteration 13700 (4.06242 iter/s, 24.6159s/100 iters), loss = 0.346154
I0428 18:39:05.954129  7412 solver.cpp:243]     Train net output #0: loss = 0.346154 (* 1 = 0.346154 loss)
I0428 18:39:05.954129  7412 sgd_solver.cpp:137] Iteration 13700, lr = 0.01
I0428 18:39:05.962129  7412 sgd_solver.cpp:169] scale layer:0.499427 0.536380 0.582904 0.593438 0.559781 0.556661 0.556471 0.553512 0.553535 0.560763 0.561148 0.555277 0.545790 0.539398 0.535807 0.524848 0.518209 0.515939 0.522356 1.042725 
I0428 18:39:05.964130  7412 sgd_solver.cpp:200] weight diff/data:0.008924 0.010387 0.053331 0.016149 0.010741 0.015434 0.023213 0.017878 0.016440 0.012569 0.016419 0.019403 0.020344 0.016398 0.013659 0.023034 0.011494 0.022316 0.011801 0.008891 0.001655 
I0428 18:39:30.573537  7412 solver.cpp:224] Iteration 13800 (4.0618 iter/s, 24.6196s/100 iters), loss = 0.328282
I0428 18:39:30.573537  7412 solver.cpp:243]     Train net output #0: loss = 0.328282 (* 1 = 0.328282 loss)
I0428 18:39:30.573537  7412 sgd_solver.cpp:137] Iteration 13800, lr = 0.01
I0428 18:39:30.582538  7412 sgd_solver.cpp:169] scale layer:0.502705 0.533009 0.581515 0.591287 0.561511 0.555590 0.554978 0.552669 0.551883 0.558841 0.560301 0.553902 0.543859 0.537694 0.534210 0.522835 0.515935 0.513631 0.520011 1.043296 
I0428 18:39:30.585538  7412 sgd_solver.cpp:200] weight diff/data:0.009589 0.011153 0.014566 0.012418 0.010899 0.018043 0.017103 0.013191 0.012789 0.013098 0.016221 0.016423 0.170432 0.140134 0.022333 0.016149 0.013974 0.014557 0.009375 0.011217 0.002331 
I0428 18:39:55.186945  7412 solver.cpp:224] Iteration 13900 (4.06264 iter/s, 24.6145s/100 iters), loss = 0.287931
I0428 18:39:55.187945  7412 solver.cpp:243]     Train net output #0: loss = 0.287931 (* 1 = 0.287931 loss)
I0428 18:39:55.187945  7412 sgd_solver.cpp:137] Iteration 13900, lr = 0.01
I0428 18:39:55.195945  7412 sgd_solver.cpp:169] scale layer:0.502865 0.526577 0.579422 0.589749 0.560503 0.554280 0.553678 0.551313 0.550276 0.557575 0.559423 0.552059 0.541742 0.535358 0.532534 0.520468 0.513436 0.511185 0.517835 1.044951 
I0428 18:39:55.198946  7412 sgd_solver.cpp:200] weight diff/data:0.010851 0.011659 0.011481 0.009582 0.014760 0.012331 0.014907 0.010667 0.015091 0.012032 0.016053 0.029865 0.017357 0.015409 0.016973 0.015354 0.020280 0.012566 0.016551 0.023198 0.002344 
I0428 18:40:18.561282 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:40:19.545338  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_14000.caffemodel
I0428 18:40:19.553339  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_14000.solverstate
I0428 18:40:19.556339  7412 solver.cpp:336] Iteration 14000, Testing net (#0)
I0428 18:40:28.323840 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:40:28.688861  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7852
I0428 18:40:28.688861  7412 solver.cpp:403]     Test net output #1: loss = 0.672719 (* 1 = 0.672719 loss)
I0428 18:40:28.923876  7412 solver.cpp:224] Iteration 14000 (2.96406 iter/s, 33.7375s/100 iters), loss = 0.381629
I0428 18:40:28.923876  7412 solver.cpp:243]     Train net output #0: loss = 0.381629 (* 1 = 0.381629 loss)
I0428 18:40:28.923876  7412 sgd_solver.cpp:137] Iteration 14000, lr = 0.01
I0428 18:40:28.932875  7412 sgd_solver.cpp:169] scale layer:0.501943 0.527624 0.579607 0.588867 0.558524 0.552451 0.551906 0.550455 0.548110 0.556610 0.557532 0.550675 0.540241 0.533930 0.529672 0.518697 0.511159 0.509323 0.515707 1.043292 
I0428 18:40:28.933876  7412 sgd_solver.cpp:200] weight diff/data:0.008523 0.009837 0.020194 0.009442 0.017144 0.024603 0.201882 0.018096 0.053175 0.340922 0.018512 0.019847 0.013564 0.021495 0.014665 0.011774 0.010764 0.009098 0.010526 0.008461 0.002691 
I0428 18:40:53.694291  7412 solver.cpp:224] Iteration 14100 (4.03704 iter/s, 24.7706s/100 iters), loss = 0.240305
I0428 18:40:53.694291  7412 solver.cpp:243]     Train net output #0: loss = 0.240305 (* 1 = 0.240305 loss)
I0428 18:40:53.694291  7412 sgd_solver.cpp:137] Iteration 14100, lr = 0.01
I0428 18:40:53.703292  7412 sgd_solver.cpp:169] scale layer:0.502417 0.524592 0.577440 0.587109 0.559051 0.550885 0.551257 0.547572 0.546525 0.555179 0.555990 0.549302 0.538822 0.532827 0.527733 0.517073 0.509170 0.506896 0.513649 1.042572 
I0428 18:40:53.705292  7412 sgd_solver.cpp:200] weight diff/data:0.011299 0.026704 0.017694 0.057434 0.011281 0.013237 0.051963 0.011904 0.016318 0.043283 0.014483 0.015520 0.011881 0.013166 0.013595 0.015675 0.014443 0.012727 0.011771 0.016073 0.001809 
I0428 18:41:18.300699  7412 solver.cpp:224] Iteration 14200 (4.06388 iter/s, 24.607s/100 iters), loss = 0.338959
I0428 18:41:18.300699  7412 solver.cpp:243]     Train net output #0: loss = 0.338959 (* 1 = 0.338959 loss)
I0428 18:41:18.300699  7412 sgd_solver.cpp:137] Iteration 14200, lr = 0.01
I0428 18:41:18.309700  7412 sgd_solver.cpp:169] scale layer:0.499526 0.525991 0.577303 0.587011 0.555924 0.549504 0.549954 0.546385 0.544588 0.552979 0.554995 0.547211 0.537165 0.531192 0.526280 0.514826 0.507058 0.504721 0.511381 1.043698 
I0428 18:41:18.312700  7412 sgd_solver.cpp:200] weight diff/data:0.028232 0.012565 0.021546 0.015352 0.011861 0.015579 0.016145 0.014323 0.012377 0.020115 0.016098 0.011993 0.017054 0.012686 0.013703 0.014241 0.022235 0.012229 0.025307 0.017105 0.001263 
I0428 18:41:42.886106  7412 solver.cpp:224] Iteration 14300 (4.06747 iter/s, 24.5853s/100 iters), loss = 0.264899
I0428 18:41:42.886106  7412 solver.cpp:243]     Train net output #0: loss = 0.264899 (* 1 = 0.264899 loss)
I0428 18:41:42.886106  7412 sgd_solver.cpp:137] Iteration 14300, lr = 0.01
I0428 18:41:42.894105  7412 sgd_solver.cpp:169] scale layer:0.496420 0.523593 0.575977 0.584606 0.552617 0.548585 0.549534 0.545024 0.543469 0.552334 0.553931 0.546165 0.535735 0.529305 0.524193 0.512684 0.505179 0.502318 0.509206 1.044275 
I0428 18:41:42.896106  7412 sgd_solver.cpp:200] weight diff/data:0.011529 0.009950 0.012688 0.010639 0.018386 0.016974 0.014736 0.016451 0.014026 0.157509 0.015085 0.015932 0.013404 0.015135 0.010687 0.093255 0.012671 0.015497 0.012223 0.011857 0.001488 
I0428 18:42:06.276443 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:42:07.490512  7412 solver.cpp:224] Iteration 14400 (4.06413 iter/s, 24.6055s/100 iters), loss = 0.438126
I0428 18:42:07.490512  7412 solver.cpp:243]     Train net output #0: loss = 0.438126 (* 1 = 0.438126 loss)
I0428 18:42:07.490512  7412 sgd_solver.cpp:137] Iteration 14400, lr = 0.01
I0428 18:42:07.500514  7412 sgd_solver.cpp:169] scale layer:0.498922 0.523433 0.574329 0.585789 0.554639 0.546397 0.547708 0.543346 0.542840 0.550725 0.553426 0.545386 0.533536 0.527266 0.522158 0.510784 0.502954 0.500393 0.507090 1.042952 
I0428 18:42:07.502513  7412 sgd_solver.cpp:200] weight diff/data:0.023328 0.017823 0.017666 0.038285 0.012887 0.013913 0.033343 0.016816 0.016018 0.014124 0.012697 0.023181 0.012824 0.030161 0.013516 0.017385 0.013538 0.011561 0.009066 0.008816 0.001325 
I0428 18:42:31.870908  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_14500.caffemodel
I0428 18:42:31.878907  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_14500.solverstate
I0428 18:42:31.881907  7412 solver.cpp:336] Iteration 14500, Testing net (#0)
I0428 18:42:40.645409 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:42:41.010430  7412 solver.cpp:403]     Test net output #0: accuracy = 0.796
I0428 18:42:41.010430  7412 solver.cpp:403]     Test net output #1: loss = 0.648565 (* 1 = 0.648565 loss)
I0428 18:42:41.242444  7412 solver.cpp:224] Iteration 14500 (2.96274 iter/s, 33.7525s/100 iters), loss = 0.416488
I0428 18:42:41.242444  7412 solver.cpp:243]     Train net output #0: loss = 0.416488 (* 1 = 0.416488 loss)
I0428 18:42:41.242444  7412 sgd_solver.cpp:137] Iteration 14500, lr = 0.01
I0428 18:42:41.251443  7412 sgd_solver.cpp:169] scale layer:0.496294 0.525468 0.575264 0.585641 0.553668 0.544545 0.546475 0.541216 0.541830 0.548927 0.551374 0.543916 0.531834 0.525943 0.520614 0.508656 0.500737 0.498104 0.505028 1.042667 
I0428 18:42:41.252444  7412 sgd_solver.cpp:200] weight diff/data:0.011272 0.087312 0.019152 0.045929 0.438176 0.013876 0.021284 0.017605 0.011525 0.013249 0.037126 0.023001 0.053518 0.016435 0.014914 0.015733 0.010390 0.012576 0.010744 0.010566 0.001759 
I0428 18:43:05.868851  7412 solver.cpp:224] Iteration 14600 (4.06058 iter/s, 24.6271s/100 iters), loss = 0.300608
I0428 18:43:05.868851  7412 solver.cpp:243]     Train net output #0: loss = 0.300608 (* 1 = 0.300608 loss)
I0428 18:43:05.868851  7412 sgd_solver.cpp:137] Iteration 14600, lr = 0.01
I0428 18:43:05.877852  7412 sgd_solver.cpp:169] scale layer:0.492559 0.523967 0.574861 0.583766 0.555072 0.544153 0.544370 0.541062 0.539935 0.547109 0.550331 0.542332 0.530973 0.524000 0.519080 0.507223 0.498471 0.495875 0.502834 1.042264 
I0428 18:43:05.879853  7412 sgd_solver.cpp:200] weight diff/data:0.013902 0.010281 0.051898 0.011247 0.019142 0.014767 0.014392 0.014912 0.011595 0.013731 0.019871 0.014631 0.023670 0.038071 0.011961 0.022360 0.014427 0.010862 0.049287 0.008163 0.001373 
I0428 18:43:30.479259  7412 solver.cpp:224] Iteration 14700 (4.06324 iter/s, 24.6109s/100 iters), loss = 0.314891
I0428 18:43:30.479259  7412 solver.cpp:243]     Train net output #0: loss = 0.314891 (* 1 = 0.314891 loss)
I0428 18:43:30.479259  7412 sgd_solver.cpp:137] Iteration 14700, lr = 0.01
I0428 18:43:30.488260  7412 sgd_solver.cpp:169] scale layer:0.493280 0.520068 0.570597 0.582608 0.551773 0.542858 0.543957 0.539935 0.540068 0.546094 0.548361 0.541007 0.529468 0.522178 0.517379 0.505519 0.496286 0.493560 0.500884 1.042313 
I0428 18:43:30.490260  7412 sgd_solver.cpp:200] weight diff/data:0.007520 0.019060 0.020320 0.014253 0.012091 0.017880 0.026442 0.014111 0.016910 0.013378 0.015985 0.023784 0.022101 0.028140 0.013851 0.021560 0.010772 0.010119 0.015255 0.011893 0.001441 
I0428 18:43:53.880597 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:43:55.099668  7412 solver.cpp:224] Iteration 14800 (4.06154 iter/s, 24.6212s/100 iters), loss = 0.381885
I0428 18:43:55.099668  7412 solver.cpp:243]     Train net output #0: loss = 0.381885 (* 1 = 0.381885 loss)
I0428 18:43:55.099668  7412 sgd_solver.cpp:137] Iteration 14800, lr = 0.01
I0428 18:43:55.108669  7412 sgd_solver.cpp:169] scale layer:0.488734 0.524080 0.569165 0.580861 0.554536 0.540501 0.542141 0.538550 0.537683 0.545253 0.547416 0.539886 0.527365 0.520177 0.515536 0.503650 0.494224 0.491518 0.499001 1.042762 
I0428 18:43:55.111668  7412 sgd_solver.cpp:200] weight diff/data:0.012344 0.008752 0.016118 0.018229 0.020819 0.037487 0.018503 0.028341 0.016872 0.011488 0.025720 0.013241 0.021094 0.018330 0.012794 0.015445 0.014971 0.014323 0.061186 0.026664 0.001520 
I0428 18:44:19.702075  7412 solver.cpp:224] Iteration 14900 (4.06453 iter/s, 24.6031s/100 iters), loss = 0.325683
I0428 18:44:19.703074  7412 solver.cpp:243]     Train net output #0: loss = 0.325683 (* 1 = 0.325683 loss)
I0428 18:44:19.703074  7412 sgd_solver.cpp:137] Iteration 14900, lr = 0.01
I0428 18:44:19.712075  7412 sgd_solver.cpp:169] scale layer:0.490628 0.522305 0.570526 0.580017 0.554421 0.539450 0.541296 0.537260 0.536650 0.544159 0.546374 0.537764 0.526345 0.519693 0.513674 0.502119 0.492306 0.489482 0.496995 1.042434 
I0428 18:44:19.714076  7412 sgd_solver.cpp:200] weight diff/data:0.009341 0.021423 0.012914 0.013537 0.015090 0.017239 0.041787 0.024994 0.025183 0.047241 0.014675 0.014044 0.014996 0.012518 0.018850 0.022622 0.014369 0.020847 0.009676 0.047218 0.001240 
I0428 18:44:44.035466  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_15000.caffemodel
I0428 18:44:44.043467  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_15000.solverstate
I0428 18:44:44.046468  7412 solver.cpp:336] Iteration 15000, Testing net (#0)
I0428 18:44:52.813968 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:44:53.175989  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7978
I0428 18:44:53.175989  7412 solver.cpp:403]     Test net output #1: loss = 0.651817 (* 1 = 0.651817 loss)
I0428 18:44:53.412003  7412 solver.cpp:224] Iteration 15000 (2.96649 iter/s, 33.7099s/100 iters), loss = 0.386521
I0428 18:44:53.412003  7412 solver.cpp:243]     Train net output #0: loss = 0.386521 (* 1 = 0.386521 loss)
I0428 18:44:53.412003  7412 sgd_solver.cpp:137] Iteration 15000, lr = 0.01
I0428 18:44:53.420003  7412 sgd_solver.cpp:169] scale layer:0.486866 0.519830 0.570054 0.580159 0.553591 0.539203 0.540178 0.536438 0.536842 0.542018 0.545168 0.536557 0.525662 0.517531 0.511817 0.500461 0.490300 0.487291 0.494661 1.041806 
I0428 18:44:53.422003  7412 sgd_solver.cpp:200] weight diff/data:0.010934 0.015123 0.018601 0.009478 0.016874 0.019609 0.013737 0.018811 0.019672 0.017760 0.016360 0.013207 0.014158 0.014106 0.013655 0.061796 0.012135 0.025331 0.018846 0.029940 0.002349 
I0428 18:45:18.054412  7412 solver.cpp:224] Iteration 15100 (4.05785 iter/s, 24.6436s/100 iters), loss = 0.314867
I0428 18:45:18.054412  7412 solver.cpp:243]     Train net output #0: loss = 0.314867 (* 1 = 0.314867 loss)
I0428 18:45:18.055413  7412 sgd_solver.cpp:137] Iteration 15100, lr = 0.01
I0428 18:45:18.063412  7412 sgd_solver.cpp:169] scale layer:0.485883 0.519011 0.568502 0.578764 0.548850 0.537551 0.537898 0.535161 0.536234 0.540934 0.544003 0.535190 0.524338 0.516157 0.510439 0.498358 0.487976 0.485100 0.492907 1.042515 
I0428 18:45:18.065413  7412 sgd_solver.cpp:200] weight diff/data:0.035809 0.010374 0.014912 0.010256 0.013621 0.015280 0.012282 0.023957 0.015287 0.010510 0.012557 0.010347 0.011511 0.009778 0.019162 0.010030 0.012743 0.027225 0.011629 0.011978 0.001433 
I0428 18:45:41.448750 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:45:42.674820  7412 solver.cpp:224] Iteration 15200 (4.06163 iter/s, 24.6206s/100 iters), loss = 0.461917
I0428 18:45:42.674820  7412 solver.cpp:243]     Train net output #0: loss = 0.461917 (* 1 = 0.461917 loss)
I0428 18:45:42.674820  7412 sgd_solver.cpp:137] Iteration 15200, lr = 0.01
I0428 18:45:42.683821  7412 sgd_solver.cpp:169] scale layer:0.490697 0.519259 0.568265 0.577959 0.547170 0.535747 0.537280 0.533240 0.535260 0.539587 0.542117 0.533289 0.522341 0.514138 0.508788 0.496731 0.486125 0.482963 0.490852 1.042185 
I0428 18:45:42.685822  7412 sgd_solver.cpp:200] weight diff/data:0.021837 0.013011 0.016755 0.010682 0.021353 0.009864 0.022020 0.013899 0.020782 0.014679 0.013023 0.027508 0.019820 0.063774 0.035633 0.021043 0.029030 0.017766 0.012889 0.035907 0.002233 
I0428 18:46:07.304229  7412 solver.cpp:224] Iteration 15300 (4.06006 iter/s, 24.6302s/100 iters), loss = 0.32925
I0428 18:46:07.304229  7412 solver.cpp:243]     Train net output #0: loss = 0.32925 (* 1 = 0.32925 loss)
I0428 18:46:07.305229  7412 sgd_solver.cpp:137] Iteration 15300, lr = 0.01
I0428 18:46:07.313230  7412 sgd_solver.cpp:169] scale layer:0.491195 0.516416 0.568255 0.578273 0.546382 0.535523 0.536433 0.532001 0.533601 0.539074 0.541264 0.532799 0.520759 0.512497 0.507046 0.494448 0.484469 0.481062 0.488951 1.039570 
I0428 18:46:07.315229  7412 sgd_solver.cpp:200] weight diff/data:0.010160 0.026364 0.012077 0.011716 0.012552 0.016066 0.015431 0.030473 0.030543 0.020364 0.013904 0.017774 0.016712 0.034443 0.024555 0.153384 0.018378 0.021494 0.011544 0.012807 0.001194 
I0428 18:46:31.900636  7412 solver.cpp:224] Iteration 15400 (4.06566 iter/s, 24.5962s/100 iters), loss = 0.348703
I0428 18:46:31.900636  7412 solver.cpp:243]     Train net output #0: loss = 0.348703 (* 1 = 0.348703 loss)
I0428 18:46:31.900636  7412 sgd_solver.cpp:137] Iteration 15400, lr = 0.01
I0428 18:46:31.908637  7412 sgd_solver.cpp:169] scale layer:0.486415 0.515047 0.567814 0.574424 0.545110 0.533886 0.534864 0.532171 0.531163 0.537971 0.540074 0.531395 0.519433 0.511064 0.505458 0.492918 0.482764 0.479111 0.486772 1.040200 
I0428 18:46:31.911636  7412 sgd_solver.cpp:200] weight diff/data:0.008533 0.013128 0.023368 0.020105 0.019124 0.014519 0.012767 0.023001 0.019110 0.015767 0.019613 0.039076 0.017274 0.017832 0.011637 0.019420 0.035770 0.021139 0.019322 0.009346 0.000879 
I0428 18:46:56.264029  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_15500.caffemodel
I0428 18:46:56.274030  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_15500.solverstate
I0428 18:46:56.277030  7412 solver.cpp:336] Iteration 15500, Testing net (#0)
I0428 18:47:05.049532 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:47:05.412552  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7943
I0428 18:47:05.412552  7412 solver.cpp:403]     Test net output #1: loss = 0.649344 (* 1 = 0.649344 loss)
I0428 18:47:05.644567  7412 solver.cpp:224] Iteration 15500 (2.96336 iter/s, 33.7455s/100 iters), loss = 0.319241
I0428 18:47:05.645566  7412 solver.cpp:243]     Train net output #0: loss = 0.31924 (* 1 = 0.31924 loss)
I0428 18:47:05.645566  7412 sgd_solver.cpp:137] Iteration 15500, lr = 0.01
I0428 18:47:05.653566  7412 sgd_solver.cpp:169] scale layer:0.481699 0.514893 0.565434 0.573340 0.545084 0.532529 0.534780 0.531673 0.530020 0.536522 0.539100 0.529687 0.518324 0.509296 0.504616 0.490838 0.480700 0.476786 0.484609 1.040211 
I0428 18:47:05.654567  7412 sgd_solver.cpp:200] weight diff/data:0.009735 0.020706 0.056000 0.013517 0.009510 0.023559 0.012964 0.017495 0.018987 0.020909 0.014023 0.012829 0.015048 0.015238 0.013532 0.015443 0.011052 0.012464 0.008879 0.016866 0.001549 
I0428 18:47:29.046905 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:47:30.265974  7412 solver.cpp:224] Iteration 15600 (4.06146 iter/s, 24.6217s/100 iters), loss = 0.402338
I0428 18:47:30.265974  7412 solver.cpp:243]     Train net output #0: loss = 0.402338 (* 1 = 0.402338 loss)
I0428 18:47:30.265974  7412 sgd_solver.cpp:137] Iteration 15600, lr = 0.01
I0428 18:47:30.274974  7412 sgd_solver.cpp:169] scale layer:0.481090 0.513287 0.564564 0.573859 0.542910 0.531462 0.533163 0.530077 0.529088 0.535719 0.538334 0.529675 0.516831 0.507512 0.503066 0.488976 0.479032 0.474795 0.482691 1.039906 
I0428 18:47:30.276975  7412 sgd_solver.cpp:200] weight diff/data:0.010823 0.011520 0.039616 0.014046 0.010348 0.013812 0.015185 0.022404 0.011610 0.014243 0.015055 0.014259 0.020932 0.024294 0.012554 0.014815 0.014354 0.016317 0.010754 0.010891 0.001604 
I0428 18:47:54.864382  7412 solver.cpp:224] Iteration 15700 (4.06529 iter/s, 24.5985s/100 iters), loss = 0.257787
I0428 18:47:54.864382  7412 solver.cpp:243]     Train net output #0: loss = 0.257787 (* 1 = 0.257787 loss)
I0428 18:47:54.864382  7412 sgd_solver.cpp:137] Iteration 15700, lr = 0.01
I0428 18:47:54.872381  7412 sgd_solver.cpp:169] scale layer:0.479894 0.514025 0.564089 0.574411 0.541657 0.530713 0.532111 0.529003 0.527839 0.534489 0.536536 0.528066 0.515492 0.506193 0.501938 0.487517 0.477538 0.472873 0.480709 1.037948 
I0428 18:47:54.874382  7412 sgd_solver.cpp:200] weight diff/data:0.012788 0.007578 0.012192 0.032288 0.012390 0.051225 0.019571 0.016079 0.014577 0.013278 0.012921 0.013188 0.012489 0.021748 0.012958 0.014913 0.010092 0.013675 0.017005 0.009792 0.001455 
I0428 18:48:19.454787  7412 solver.cpp:224] Iteration 15800 (4.06646 iter/s, 24.5914s/100 iters), loss = 0.388561
I0428 18:48:19.455787  7412 solver.cpp:243]     Train net output #0: loss = 0.388561 (* 1 = 0.388561 loss)
I0428 18:48:19.455787  7412 sgd_solver.cpp:137] Iteration 15800, lr = 0.01
I0428 18:48:19.463788  7412 sgd_solver.cpp:169] scale layer:0.480582 0.513606 0.564011 0.573928 0.542299 0.529634 0.531330 0.528116 0.526440 0.533506 0.535782 0.525929 0.515082 0.504302 0.500900 0.486017 0.475462 0.470946 0.478680 1.038463 
I0428 18:48:19.465788  7412 sgd_solver.cpp:200] weight diff/data:0.019887 0.028168 0.010532 0.012020 0.047447 0.016603 0.014531 0.025584 0.010689 0.015634 0.014101 0.013228 0.024871 0.015071 0.016019 0.014823 0.014378 0.021756 0.013731 0.008025 0.001213 
I0428 18:48:44.130199  7412 solver.cpp:224] Iteration 15900 (4.05252 iter/s, 24.676s/100 iters), loss = 0.25806
I0428 18:48:44.131199  7412 solver.cpp:243]     Train net output #0: loss = 0.25806 (* 1 = 0.25806 loss)
I0428 18:48:44.131199  7412 sgd_solver.cpp:137] Iteration 15900, lr = 0.01
I0428 18:48:44.139199  7412 sgd_solver.cpp:169] scale layer:0.476743 0.513064 0.561059 0.572604 0.545178 0.528723 0.530727 0.527064 0.525427 0.532267 0.535219 0.525459 0.513680 0.503678 0.498682 0.484371 0.473514 0.468876 0.476864 1.038227 
I0428 18:48:44.141199  7412 sgd_solver.cpp:200] weight diff/data:0.011555 0.008800 0.014208 0.017403 0.012444 0.014182 0.011335 0.015077 0.015113 0.017009 0.024524 0.013143 0.011283 0.010160 0.021307 0.031044 0.013907 0.013199 0.011122 0.008805 0.001369 
I0428 18:49:07.641544 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:49:08.622601  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_16000.caffemodel
I0428 18:49:08.631600  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_16000.solverstate
I0428 18:49:08.634600  7412 solver.cpp:336] Iteration 16000, Testing net (#0)
I0428 18:49:17.456105 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:49:17.818126  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7936
I0428 18:49:17.818126  7412 solver.cpp:403]     Test net output #1: loss = 0.660645 (* 1 = 0.660645 loss)
I0428 18:49:18.054139  7412 solver.cpp:224] Iteration 16000 (2.94774 iter/s, 33.9243s/100 iters), loss = 0.426459
I0428 18:49:18.054139  7412 solver.cpp:243]     Train net output #0: loss = 0.426459 (* 1 = 0.426459 loss)
I0428 18:49:18.054139  7412 sgd_solver.cpp:137] Iteration 16000, lr = 0.01
I0428 18:49:18.062140  7412 sgd_solver.cpp:169] scale layer:0.476983 0.511975 0.559759 0.572566 0.545875 0.528773 0.529152 0.525850 0.524552 0.531248 0.534121 0.524873 0.511782 0.502285 0.497239 0.483128 0.471799 0.467019 0.475207 1.036644 
I0428 18:49:18.064141  7412 sgd_solver.cpp:200] weight diff/data:0.011470 0.010306 0.015278 0.017537 0.012231 0.017345 0.025668 0.025084 0.013745 0.014775 0.017147 0.014779 0.022527 0.044390 0.065170 0.014836 0.021263 0.024118 0.011290 0.018857 0.001400 
I0428 18:49:42.746551  7412 solver.cpp:224] Iteration 16100 (4.04981 iter/s, 24.6925s/100 iters), loss = 0.372755
I0428 18:49:42.746551  7412 solver.cpp:243]     Train net output #0: loss = 0.372755 (* 1 = 0.372755 loss)
I0428 18:49:42.746551  7412 sgd_solver.cpp:137] Iteration 16100, lr = 0.01
I0428 18:49:42.755553  7412 sgd_solver.cpp:169] scale layer:0.484001 0.509522 0.559117 0.573277 0.544629 0.528266 0.528040 0.524362 0.524130 0.531289 0.532639 0.523961 0.511875 0.500747 0.496261 0.480968 0.469901 0.465221 0.473172 1.031619 
I0428 18:49:42.757552  7412 sgd_solver.cpp:200] weight diff/data:0.011107 0.013424 0.018617 0.025210 0.014131 0.014517 0.014290 0.011360 0.017304 0.013984 0.014155 0.016494 0.029263 0.025968 0.015899 0.013675 0.014310 0.012175 0.010271 0.007903 0.002857 
I0428 18:50:07.453965  7412 solver.cpp:224] Iteration 16200 (4.04721 iter/s, 24.7084s/100 iters), loss = 0.446632
I0428 18:50:07.453965  7412 solver.cpp:243]     Train net output #0: loss = 0.446632 (* 1 = 0.446632 loss)
I0428 18:50:07.453965  7412 sgd_solver.cpp:137] Iteration 16200, lr = 0.01
I0428 18:50:07.463965  7412 sgd_solver.cpp:169] scale layer:0.481589 0.508668 0.558856 0.573022 0.537060 0.527312 0.527198 0.523400 0.523205 0.530547 0.532408 0.522699 0.511316 0.499881 0.494720 0.480079 0.467897 0.463380 0.471218 1.031911 
I0428 18:50:07.465965  7412 sgd_solver.cpp:200] weight diff/data:0.011781 0.013372 0.013659 0.010454 0.015337 0.025849 0.028409 0.016816 0.012515 0.013557 0.015849 0.013290 0.013125 0.039767 0.014333 0.014691 0.013065 0.013990 0.020031 0.011219 0.001116 
I0428 18:50:32.128376  7412 solver.cpp:224] Iteration 16300 (4.0528 iter/s, 24.6743s/100 iters), loss = 0.237574
I0428 18:50:32.128376  7412 solver.cpp:243]     Train net output #0: loss = 0.237573 (* 1 = 0.237573 loss)
I0428 18:50:32.128376  7412 sgd_solver.cpp:137] Iteration 16300, lr = 0.01
I0428 18:50:32.137377  7412 sgd_solver.cpp:169] scale layer:0.479384 0.507080 0.557625 0.572239 0.536720 0.525417 0.526114 0.523292 0.521938 0.529648 0.532017 0.521624 0.509251 0.498859 0.493566 0.477642 0.465702 0.461200 0.469571 1.034921 
I0428 18:50:32.139377  7412 sgd_solver.cpp:200] weight diff/data:0.009249 0.047677 0.011856 0.239225 0.007464 0.013987 0.011749 0.018229 0.013277 0.012919 0.028237 0.013872 0.013575 0.015934 0.018819 0.010761 0.009920 0.015959 0.015368 0.018500 0.001140 
I0428 18:50:55.615720 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:50:56.834789  7412 solver.cpp:224] Iteration 16400 (4.04736 iter/s, 24.7075s/100 iters), loss = 0.349567
I0428 18:50:56.835789  7412 solver.cpp:243]     Train net output #0: loss = 0.349567 (* 1 = 0.349567 loss)
I0428 18:50:56.835789  7412 sgd_solver.cpp:137] Iteration 16400, lr = 0.01
I0428 18:50:56.843791  7412 sgd_solver.cpp:169] scale layer:0.476286 0.508176 0.555438 0.571028 0.537043 0.524911 0.525993 0.522517 0.520884 0.528615 0.531512 0.521076 0.507913 0.497511 0.491626 0.476753 0.463634 0.459337 0.468024 1.035124 
I0428 18:50:56.845790  7412 sgd_solver.cpp:200] weight diff/data:0.179745 0.012701 0.018450 0.012005 0.012506 0.054115 0.016152 0.152498 0.012562 0.025181 0.048178 0.020118 0.019235 0.015470 0.015386 0.019570 0.010773 0.026949 0.010322 0.021590 0.001215 
I0428 18:51:21.290189  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_16500.caffemodel
I0428 18:51:21.298188  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_16500.solverstate
I0428 18:51:21.301188  7412 solver.cpp:336] Iteration 16500, Testing net (#0)
I0428 18:51:30.084692 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:51:30.449712  7412 solver.cpp:403]     Test net output #0: accuracy = 0.796
I0428 18:51:30.449712  7412 solver.cpp:403]     Test net output #1: loss = 0.65352 (* 1 = 0.65352 loss)
I0428 18:51:30.684725  7412 solver.cpp:224] Iteration 16500 (2.95415 iter/s, 33.8506s/100 iters), loss = 0.283354
I0428 18:51:30.685725  7412 solver.cpp:243]     Train net output #0: loss = 0.283354 (* 1 = 0.283354 loss)
I0428 18:51:30.685725  7412 sgd_solver.cpp:137] Iteration 16500, lr = 0.01
I0428 18:51:30.694726  7412 sgd_solver.cpp:169] scale layer:0.476104 0.507083 0.554834 0.570512 0.534964 0.523205 0.525437 0.520661 0.520473 0.528048 0.530237 0.519935 0.507510 0.496046 0.490158 0.475421 0.462290 0.457360 0.466284 1.032933 
I0428 18:51:30.696727  7412 sgd_solver.cpp:200] weight diff/data:0.009637 0.012021 0.013089 0.052828 0.025363 0.012928 0.046980 0.012416 0.018224 0.023647 0.024421 0.012816 0.013347 0.020467 0.021028 0.011669 0.016175 0.012379 0.014438 0.018121 0.001337 
I0428 18:51:55.269131  7412 solver.cpp:224] Iteration 16600 (4.06763 iter/s, 24.5843s/100 iters), loss = 0.323975
I0428 18:51:55.269131  7412 solver.cpp:243]     Train net output #0: loss = 0.323975 (* 1 = 0.323975 loss)
I0428 18:51:55.269131  7412 sgd_solver.cpp:137] Iteration 16600, lr = 0.01
I0428 18:51:55.277132  7412 sgd_solver.cpp:169] scale layer:0.474068 0.504676 0.556150 0.571195 0.538114 0.522314 0.524053 0.519588 0.519516 0.527497 0.529424 0.519636 0.505252 0.495361 0.489583 0.473936 0.460418 0.455557 0.464340 1.035119 
I0428 18:51:55.279132  7412 sgd_solver.cpp:200] weight diff/data:0.015162 0.024448 0.016639 0.009607 0.011699 0.012529 0.014837 0.015252 0.019226 0.017921 0.014641 0.023554 0.016177 0.024907 0.021815 0.013950 0.020617 0.017248 0.015028 0.018767 0.001092 
I0428 18:52:19.866539  7412 solver.cpp:224] Iteration 16700 (4.06534 iter/s, 24.5982s/100 iters), loss = 0.19389
I0428 18:52:19.866539  7412 solver.cpp:243]     Train net output #0: loss = 0.19389 (* 1 = 0.19389 loss)
I0428 18:52:19.866539  7412 sgd_solver.cpp:137] Iteration 16700, lr = 0.01
I0428 18:52:19.875540  7412 sgd_solver.cpp:169] scale layer:0.470833 0.504270 0.555035 0.570922 0.537348 0.521721 0.523728 0.520180 0.518155 0.526407 0.529109 0.518436 0.504491 0.494186 0.488265 0.471881 0.458969 0.453647 0.462473 1.033571 
I0428 18:52:19.877539  7412 sgd_solver.cpp:200] weight diff/data:0.012395 0.008180 0.012437 0.015627 0.010727 0.011310 0.024460 0.010869 0.017069 0.013355 0.011925 0.012452 0.012847 0.012405 0.010330 0.015678 0.050036 0.016354 0.008867 0.014582 0.000972 
I0428 18:52:43.241875 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:52:44.461946  7412 solver.cpp:224] Iteration 16800 (4.06571 iter/s, 24.596s/100 iters), loss = 0.389932
I0428 18:52:44.462945  7412 solver.cpp:243]     Train net output #0: loss = 0.389932 (* 1 = 0.389932 loss)
I0428 18:52:44.462945  7412 sgd_solver.cpp:137] Iteration 16800, lr = 0.01
I0428 18:52:44.471946  7412 sgd_solver.cpp:169] scale layer:0.474497 0.505937 0.553928 0.571962 0.540332 0.521097 0.523130 0.519316 0.517603 0.526218 0.527906 0.517689 0.503153 0.492717 0.486801 0.470770 0.456994 0.452115 0.460721 1.032732 
I0428 18:52:44.473947  7412 sgd_solver.cpp:200] weight diff/data:0.012349 0.015189 0.013261 0.019614 0.018607 0.022525 0.072646 0.014381 0.017336 0.014975 0.043935 0.019179 0.018582 0.020248 0.012779 0.012220 0.012900 0.015376 0.012148 0.010462 0.001494 
I0428 18:53:09.075353  7412 solver.cpp:224] Iteration 16900 (4.06287 iter/s, 24.6131s/100 iters), loss = 0.292284
I0428 18:53:09.075353  7412 solver.cpp:243]     Train net output #0: loss = 0.292284 (* 1 = 0.292284 loss)
I0428 18:53:09.075353  7412 sgd_solver.cpp:137] Iteration 16900, lr = 0.01
I0428 18:53:09.083353  7412 sgd_solver.cpp:169] scale layer:0.472979 0.504414 0.551752 0.570120 0.537222 0.520731 0.523321 0.518158 0.516887 0.524759 0.526763 0.516229 0.502880 0.492257 0.485805 0.469286 0.455492 0.450355 0.459017 1.031071 
I0428 18:53:09.085353  7412 sgd_solver.cpp:200] weight diff/data:0.014562 0.010837 0.277813 0.016384 0.038157 0.076248 0.013560 0.030614 0.012916 0.016594 0.022278 0.011707 0.015137 0.013757 0.016997 0.092855 0.021187 0.015298 0.011779 0.032207 0.001299 
I0428 18:53:33.413745  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_17000.caffemodel
I0428 18:53:33.421746  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_17000.solverstate
I0428 18:53:33.424746  7412 solver.cpp:336] Iteration 17000, Testing net (#0)
I0428 18:53:42.177247 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:53:42.537267  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8025
I0428 18:53:42.537267  7412 solver.cpp:403]     Test net output #1: loss = 0.633998 (* 1 = 0.633998 loss)
I0428 18:53:42.772280  7412 solver.cpp:224] Iteration 17000 (2.96754 iter/s, 33.6979s/100 iters), loss = 0.311884
I0428 18:53:42.772280  7412 solver.cpp:243]     Train net output #0: loss = 0.311884 (* 1 = 0.311884 loss)
I0428 18:53:42.772280  7412 sgd_solver.cpp:137] Iteration 17000, lr = 0.01
I0428 18:53:42.782281  7412 sgd_solver.cpp:169] scale layer:0.467619 0.503090 0.552467 0.569368 0.535457 0.518978 0.522433 0.518064 0.516163 0.523822 0.525261 0.515492 0.501901 0.491062 0.484301 0.468082 0.453927 0.448610 0.457296 1.032063 
I0428 18:53:42.783282  7412 sgd_solver.cpp:200] weight diff/data:0.008220 0.010145 0.011660 0.017999 0.013961 0.022999 0.013759 0.024658 0.014684 0.034310 0.015821 0.012414 0.016560 0.012981 0.014811 0.049136 0.010656 0.015373 0.009664 0.010822 0.001096 
I0428 18:54:07.379688  7412 solver.cpp:224] Iteration 17100 (4.0638 iter/s, 24.6075s/100 iters), loss = 0.225805
I0428 18:54:07.379688  7412 solver.cpp:243]     Train net output #0: loss = 0.225805 (* 1 = 0.225805 loss)
I0428 18:54:07.379688  7412 sgd_solver.cpp:137] Iteration 17100, lr = 0.01
I0428 18:54:07.388689  7412 sgd_solver.cpp:169] scale layer:0.467989 0.499014 0.551174 0.568295 0.533252 0.518897 0.521327 0.516730 0.514794 0.522130 0.524355 0.514927 0.500236 0.489782 0.482472 0.466025 0.452301 0.446756 0.455440 1.032449 
I0428 18:54:07.390688  7412 sgd_solver.cpp:200] weight diff/data:0.020909 0.012812 0.014287 0.011367 0.654293 0.013149 0.019892 0.013291 0.021869 0.013308 0.045962 0.052995 0.011654 0.017040 0.018504 0.020566 0.051180 0.015903 0.013802 0.034105 0.001236 
I0428 18:54:30.755025 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:54:31.976094  7412 solver.cpp:224] Iteration 17200 (4.0656 iter/s, 24.5966s/100 iters), loss = 0.438974
I0428 18:54:31.976094  7412 solver.cpp:243]     Train net output #0: loss = 0.438974 (* 1 = 0.438974 loss)
I0428 18:54:31.976094  7412 sgd_solver.cpp:137] Iteration 17200, lr = 0.01
I0428 18:54:31.985095  7412 sgd_solver.cpp:169] scale layer:0.469508 0.500292 0.550956 0.565630 0.533060 0.517760 0.519775 0.515306 0.513654 0.521481 0.524494 0.514032 0.499326 0.488150 0.481224 0.465468 0.450941 0.445090 0.453792 1.032631 
I0428 18:54:31.987095  7412 sgd_solver.cpp:200] weight diff/data:0.009952 0.015098 0.012823 0.017064 0.013117 0.016165 0.019452 0.013034 0.013508 0.012840 0.014870 0.017323 0.025675 0.030183 0.016752 0.013756 0.172008 0.016850 0.009005 0.011952 0.001036 
I0428 18:54:56.662506  7412 solver.cpp:224] Iteration 17300 (4.05074 iter/s, 24.6868s/100 iters), loss = 0.224792
I0428 18:54:56.662506  7412 solver.cpp:243]     Train net output #0: loss = 0.224792 (* 1 = 0.224792 loss)
I0428 18:54:56.662506  7412 sgd_solver.cpp:137] Iteration 17300, lr = 0.01
I0428 18:54:56.670507  7412 sgd_solver.cpp:169] scale layer:0.469217 0.500719 0.550032 0.565829 0.538503 0.516862 0.520102 0.514155 0.513548 0.520564 0.523310 0.513263 0.498415 0.487168 0.480517 0.463757 0.449903 0.443083 0.452248 1.033517 
I0428 18:54:56.673507  7412 sgd_solver.cpp:200] weight diff/data:0.020225 0.011682 0.012861 0.042095 0.014412 0.024997 0.014366 0.014377 0.015289 0.013812 0.013687 0.013822 0.021430 0.013457 0.014554 0.019291 0.019981 0.015517 0.021025 0.016393 0.000991 
I0428 18:55:21.309916  7412 solver.cpp:224] Iteration 17400 (4.05708 iter/s, 24.6483s/100 iters), loss = 0.425817
I0428 18:55:21.309916  7412 solver.cpp:243]     Train net output #0: loss = 0.425817 (* 1 = 0.425817 loss)
I0428 18:55:21.309916  7412 sgd_solver.cpp:137] Iteration 17400, lr = 0.01
I0428 18:55:21.318917  7412 sgd_solver.cpp:169] scale layer:0.467046 0.500244 0.550123 0.565431 0.535158 0.516160 0.519382 0.513871 0.512660 0.519164 0.522048 0.512841 0.498114 0.486395 0.479967 0.463101 0.448845 0.441483 0.450549 1.034302 
I0428 18:55:21.320917  7412 sgd_solver.cpp:200] weight diff/data:0.049454 0.028077 0.019238 0.014238 0.022148 0.014021 0.014267 0.014547 0.012970 0.014513 0.017914 0.016850 0.012154 0.014248 0.014126 0.012151 0.011478 0.015896 0.032195 0.018501 0.000923 
I0428 18:55:45.687310  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_17500.caffemodel
I0428 18:55:45.696311  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_17500.solverstate
I0428 18:55:45.699311  7412 solver.cpp:336] Iteration 17500, Testing net (#0)
I0428 18:55:54.440811 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:55:54.805832  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8064
I0428 18:55:54.805832  7412 solver.cpp:403]     Test net output #1: loss = 0.63101 (* 1 = 0.63101 loss)
I0428 18:55:55.043846  7412 solver.cpp:224] Iteration 17500 (2.96429 iter/s, 33.7348s/100 iters), loss = 0.301903
I0428 18:55:55.043846  7412 solver.cpp:243]     Train net output #0: loss = 0.301902 (* 1 = 0.301902 loss)
I0428 18:55:55.043846  7412 sgd_solver.cpp:137] Iteration 17500, lr = 0.01
I0428 18:55:55.052846  7412 sgd_solver.cpp:169] scale layer:0.463578 0.500843 0.549371 0.565320 0.533445 0.515435 0.518481 0.513033 0.512274 0.518533 0.521817 0.511353 0.496945 0.484750 0.478785 0.462022 0.446877 0.439608 0.448800 1.037604 
I0428 18:55:55.053846  7412 sgd_solver.cpp:200] weight diff/data:0.015441 0.008916 0.011441 0.012264 0.013122 0.014439 0.043113 0.015162 0.013066 0.012046 0.024746 0.018777 0.061267 0.013731 0.012608 0.017348 0.016862 0.012897 0.266515 0.011493 0.001284 
I0428 18:56:18.437185 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:56:19.658253  7412 solver.cpp:224] Iteration 17600 (4.06262 iter/s, 24.6147s/100 iters), loss = 0.37127
I0428 18:56:19.658253  7412 solver.cpp:243]     Train net output #0: loss = 0.37127 (* 1 = 0.37127 loss)
I0428 18:56:19.658253  7412 sgd_solver.cpp:137] Iteration 17600, lr = 0.01
I0428 18:56:19.667254  7412 sgd_solver.cpp:169] scale layer:0.464844 0.498134 0.547028 0.566891 0.536543 0.514278 0.516310 0.511759 0.511808 0.517971 0.521632 0.509971 0.495518 0.483875 0.477879 0.460588 0.445210 0.438366 0.447005 1.034446 
I0428 18:56:19.669255  7412 sgd_solver.cpp:200] weight diff/data:0.027830 0.076779 0.018023 0.016309 0.011996 0.016726 0.017336 0.012127 0.021167 0.015304 0.016283 0.025199 0.016553 0.013590 0.012951 0.017178 0.026813 0.010076 0.020621 0.012675 0.001100 
I0428 18:56:44.273661  7412 solver.cpp:224] Iteration 17700 (4.06235 iter/s, 24.6163s/100 iters), loss = 0.265194
I0428 18:56:44.273661  7412 solver.cpp:243]     Train net output #0: loss = 0.265194 (* 1 = 0.265194 loss)
I0428 18:56:44.273661  7412 sgd_solver.cpp:137] Iteration 17700, lr = 0.01
I0428 18:56:44.282662  7412 sgd_solver.cpp:169] scale layer:0.466110 0.498648 0.549215 0.565480 0.535627 0.513813 0.515752 0.511323 0.511639 0.518313 0.520920 0.509612 0.493960 0.482725 0.476898 0.459231 0.443822 0.436773 0.445761 1.033255 
I0428 18:56:44.284662  7412 sgd_solver.cpp:200] weight diff/data:0.017051 0.016106 0.013625 0.010775 0.009245 0.012885 0.016884 0.010170 0.043121 0.063400 0.014993 0.015632 0.011676 0.011273 0.016552 0.023839 0.070498 0.015341 0.043029 0.012037 0.001075 
I0428 18:57:08.863068  7412 solver.cpp:224] Iteration 17800 (4.0668 iter/s, 24.5894s/100 iters), loss = 0.319718
I0428 18:57:08.863068  7412 solver.cpp:243]     Train net output #0: loss = 0.319718 (* 1 = 0.319718 loss)
I0428 18:57:08.863068  7412 sgd_solver.cpp:137] Iteration 17800, lr = 0.01
I0428 18:57:08.872068  7412 sgd_solver.cpp:169] scale layer:0.464162 0.495136 0.546338 0.565696 0.533523 0.511559 0.515685 0.510735 0.510364 0.518013 0.519925 0.509119 0.493725 0.481989 0.475999 0.457419 0.442180 0.435051 0.444368 1.032189 
I0428 18:57:08.874068  7412 sgd_solver.cpp:200] weight diff/data:0.010573 0.328425 0.027079 0.012655 0.015179 0.017363 0.021986 0.011089 0.013099 0.015640 0.015002 0.031347 0.019384 0.013012 0.012485 0.018993 0.018246 0.012172 0.015960 0.010261 0.000925 
I0428 18:57:33.466475  7412 solver.cpp:224] Iteration 17900 (4.06426 iter/s, 24.6047s/100 iters), loss = 0.281772
I0428 18:57:33.467475  7412 solver.cpp:243]     Train net output #0: loss = 0.281771 (* 1 = 0.281771 loss)
I0428 18:57:33.467475  7412 sgd_solver.cpp:137] Iteration 17900, lr = 0.01
I0428 18:57:33.475476  7412 sgd_solver.cpp:169] scale layer:0.463275 0.495651 0.547160 0.565360 0.534170 0.511196 0.514574 0.510509 0.510973 0.516885 0.519813 0.508135 0.492815 0.481685 0.475596 0.456264 0.440661 0.433533 0.442573 1.029912 
I0428 18:57:33.477476  7412 sgd_solver.cpp:200] weight diff/data:0.010517 0.007853 0.012720 0.013378 0.010224 0.010650 0.015426 0.011889 0.015693 0.061553 0.025291 0.010491 0.017526 0.014446 0.024016 0.011929 0.043220 0.047751 0.013252 0.009894 0.001180 
I0428 18:57:56.870815 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:57:57.853870  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_18000.caffemodel
I0428 18:57:57.861871  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_18000.solverstate
I0428 18:57:57.865871  7412 solver.cpp:336] Iteration 18000, Testing net (#0)
I0428 18:58:06.655374 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:58:07.019394  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8012
I0428 18:58:07.019394  7412 solver.cpp:403]     Test net output #1: loss = 0.641851 (* 1 = 0.641851 loss)
I0428 18:58:07.249408  7412 solver.cpp:224] Iteration 18000 (2.96001 iter/s, 33.7837s/100 iters), loss = 0.417833
I0428 18:58:07.250407  7412 solver.cpp:243]     Train net output #0: loss = 0.417833 (* 1 = 0.417833 loss)
I0428 18:58:07.250407  7412 sgd_solver.cpp:137] Iteration 18000, lr = 0.01
I0428 18:58:07.259408  7412 sgd_solver.cpp:169] scale layer:0.462982 0.495934 0.546517 0.565584 0.529810 0.511118 0.514235 0.509508 0.509598 0.515981 0.519950 0.509007 0.491668 0.480034 0.473911 0.454802 0.439262 0.432034 0.440919 1.028166 
I0428 18:58:07.260408  7412 sgd_solver.cpp:200] weight diff/data:0.012904 0.014122 0.015109 0.014759 0.025961 0.012574 0.468319 0.028258 0.013664 0.016395 0.019792 0.015695 0.014796 0.084856 0.010614 0.012409 0.013646 0.011047 0.011490 0.010789 0.001041 
I0428 18:58:31.912818  7412 solver.cpp:224] Iteration 18100 (4.05461 iter/s, 24.6633s/100 iters), loss = 0.293417
I0428 18:58:31.912818  7412 solver.cpp:243]     Train net output #0: loss = 0.293417 (* 1 = 0.293417 loss)
I0428 18:58:31.912818  7412 sgd_solver.cpp:137] Iteration 18100, lr = 0.01
I0428 18:58:31.921819  7412 sgd_solver.cpp:169] scale layer:0.468484 0.495531 0.544112 0.567183 0.531019 0.510664 0.513210 0.509109 0.508969 0.515066 0.519003 0.507936 0.491603 0.479738 0.473359 0.453498 0.438321 0.430478 0.439347 1.026596 
I0428 18:58:31.923820  7412 sgd_solver.cpp:200] weight diff/data:0.021952 0.011952 0.013045 0.011845 0.013445 0.031921 0.018672 0.017546 0.013837 0.011577 0.053721 0.083813 0.017071 0.026790 0.011745 0.013570 0.014277 0.020687 0.009168 0.013483 0.001197 
I0428 18:58:56.541227  7412 solver.cpp:224] Iteration 18200 (4.06022 iter/s, 24.6292s/100 iters), loss = 0.39312
I0428 18:58:56.541227  7412 solver.cpp:243]     Train net output #0: loss = 0.39312 (* 1 = 0.39312 loss)
I0428 18:58:56.541227  7412 sgd_solver.cpp:137] Iteration 18200, lr = 0.01
I0428 18:58:56.551228  7412 sgd_solver.cpp:169] scale layer:0.467409 0.496129 0.545687 0.565843 0.535877 0.509513 0.513025 0.508892 0.508429 0.514194 0.518713 0.507808 0.492243 0.478621 0.472330 0.452529 0.436513 0.429122 0.437691 1.026474 
I0428 18:58:56.553227  7412 sgd_solver.cpp:200] weight diff/data:0.009211 0.012484 0.032382 0.013339 0.016948 0.013758 0.012791 0.035464 0.017184 0.013537 0.015297 0.015685 0.021840 0.015280 0.015572 0.025027 0.012785 0.034836 0.011666 0.009802 0.001190 
I0428 18:59:21.228639  7412 solver.cpp:224] Iteration 18300 (4.05067 iter/s, 24.6873s/100 iters), loss = 0.271973
I0428 18:59:21.228639  7412 solver.cpp:243]     Train net output #0: loss = 0.271973 (* 1 = 0.271973 loss)
I0428 18:59:21.228639  7412 sgd_solver.cpp:137] Iteration 18300, lr = 0.01
I0428 18:59:21.236639  7412 sgd_solver.cpp:169] scale layer:0.463938 0.493316 0.544859 0.565859 0.534015 0.509183 0.513092 0.508090 0.508406 0.513340 0.518014 0.507982 0.490975 0.477248 0.471064 0.451367 0.435281 0.427179 0.436149 1.027116 
I0428 18:59:21.238639  7412 sgd_solver.cpp:200] weight diff/data:0.008014 0.022332 0.013169 0.014347 0.013671 0.011995 0.034256 0.011734 0.020085 0.014490 0.016587 0.014417 0.011939 0.017518 0.012091 0.013675 0.016953 0.015758 0.012334 0.010957 0.001253 
I0428 18:59:44.683980 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 18:59:45.907050  7412 solver.cpp:224] Iteration 18400 (4.05193 iter/s, 24.6796s/100 iters), loss = 0.393618
I0428 18:59:45.907050  7412 solver.cpp:243]     Train net output #0: loss = 0.393618 (* 1 = 0.393618 loss)
I0428 18:59:45.907050  7412 sgd_solver.cpp:137] Iteration 18400, lr = 0.01
I0428 18:59:45.916051  7412 sgd_solver.cpp:169] scale layer:0.461921 0.491815 0.542179 0.564077 0.531040 0.509501 0.512931 0.506936 0.507423 0.512179 0.516827 0.506676 0.488962 0.475633 0.469587 0.449427 0.433073 0.425388 0.434608 1.028172 
I0428 18:59:45.918051  7412 sgd_solver.cpp:200] weight diff/data:0.022317 0.010134 0.014113 0.017157 0.015372 0.015594 0.017760 0.011082 0.120936 0.015899 0.021632 0.013840 0.013046 0.015658 0.013197 0.014431 0.013256 0.013988 0.015142 0.011889 0.000883 
I0428 19:00:10.335448  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_18500.caffemodel
I0428 19:00:10.344449  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_18500.solverstate
I0428 19:00:10.347448  7412 solver.cpp:336] Iteration 18500, Testing net (#0)
I0428 19:00:19.139951 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:00:19.504972  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7939
I0428 19:00:19.504972  7412 solver.cpp:403]     Test net output #1: loss = 0.669824 (* 1 = 0.669824 loss)
I0428 19:00:19.739985  7412 solver.cpp:224] Iteration 18500 (2.95564 iter/s, 33.8337s/100 iters), loss = 0.294167
I0428 19:00:19.739985  7412 solver.cpp:243]     Train net output #0: loss = 0.294167 (* 1 = 0.294167 loss)
I0428 19:00:19.739985  7412 sgd_solver.cpp:137] Iteration 18500, lr = 0.01
I0428 19:00:19.748986  7412 sgd_solver.cpp:169] scale layer:0.464860 0.490645 0.542770 0.564209 0.529567 0.509163 0.511004 0.506001 0.505980 0.512130 0.515951 0.505716 0.487417 0.474467 0.468410 0.448890 0.432347 0.424177 0.433196 1.029769 
I0428 19:00:19.750986  7412 sgd_solver.cpp:200] weight diff/data:0.032088 0.064126 0.015904 0.022906 0.021931 0.020102 0.015986 0.018531 0.014821 0.018921 0.015312 0.011877 0.014781 0.021572 0.013672 0.013401 0.023832 0.013358 0.012542 0.008737 0.000975 
I0428 19:00:44.389395  7412 solver.cpp:224] Iteration 18600 (4.05681 iter/s, 24.6499s/100 iters), loss = 0.291534
I0428 19:00:44.389395  7412 solver.cpp:243]     Train net output #0: loss = 0.291534 (* 1 = 0.291534 loss)
I0428 19:00:44.389395  7412 sgd_solver.cpp:137] Iteration 18600, lr = 0.01
I0428 19:00:44.398396  7412 sgd_solver.cpp:169] scale layer:0.459666 0.492378 0.542648 0.564023 0.526862 0.507672 0.510340 0.506474 0.505968 0.510858 0.515685 0.503762 0.487074 0.473375 0.468233 0.447076 0.430846 0.422636 0.431891 1.029756 
I0428 19:00:44.400396  7412 sgd_solver.cpp:200] weight diff/data:0.012358 0.010781 0.009507 0.011458 0.010441 0.030745 0.012061 0.014847 0.021700 0.013659 0.061262 0.050120 0.011607 0.011198 0.142365 0.012789 0.015804 0.009868 0.012968 0.023286 0.001173 
I0428 19:01:09.056807  7412 solver.cpp:224] Iteration 18700 (4.05382 iter/s, 24.6681s/100 iters), loss = 0.203663
I0428 19:01:09.057806  7412 solver.cpp:243]     Train net output #0: loss = 0.203663 (* 1 = 0.203663 loss)
I0428 19:01:09.057806  7412 sgd_solver.cpp:137] Iteration 18700, lr = 0.01
I0428 19:01:09.066807  7412 sgd_solver.cpp:169] scale layer:0.454970 0.491221 0.541975 0.563722 0.527082 0.505930 0.510545 0.505083 0.505417 0.509915 0.515045 0.503893 0.486841 0.473464 0.467622 0.446399 0.429681 0.420621 0.430394 1.029712 
I0428 19:01:09.068807  7412 sgd_solver.cpp:200] weight diff/data:0.012572 0.029708 0.013640 0.039084 0.245675 0.012757 0.014761 0.020610 0.012655 0.011656 0.011702 0.010847 0.010500 0.011555 0.023405 0.044615 0.013057 0.014759 0.011628 0.012881 0.001096 
I0428 19:01:32.480146 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:01:33.704216  7412 solver.cpp:224] Iteration 18800 (4.0572 iter/s, 24.6475s/100 iters), loss = 0.333448
I0428 19:01:33.704216  7412 solver.cpp:243]     Train net output #0: loss = 0.333447 (* 1 = 0.333447 loss)
I0428 19:01:33.705216  7412 sgd_solver.cpp:137] Iteration 18800, lr = 0.01
I0428 19:01:33.714216  7412 sgd_solver.cpp:169] scale layer:0.454461 0.493106 0.540801 0.564076 0.526647 0.504717 0.509666 0.504036 0.505756 0.508915 0.514324 0.503579 0.485868 0.472532 0.465890 0.444952 0.427895 0.419068 0.428811 1.029408 
I0428 19:01:33.716217  7412 sgd_solver.cpp:200] weight diff/data:0.016463 0.029690 0.012987 0.010337 0.010792 0.038682 0.014449 0.042462 0.032678 0.016405 0.022222 0.013605 0.017321 0.038959 0.014328 0.012650 0.011645 0.012349 0.008590 0.013829 0.000907 
I0428 19:01:58.325624  7412 solver.cpp:224] Iteration 18900 (4.06153 iter/s, 24.6213s/100 iters), loss = 0.246215
I0428 19:01:58.325624  7412 solver.cpp:243]     Train net output #0: loss = 0.246215 (* 1 = 0.246215 loss)
I0428 19:01:58.325624  7412 sgd_solver.cpp:137] Iteration 18900, lr = 0.01
I0428 19:01:58.334625  7412 sgd_solver.cpp:169] scale layer:0.460982 0.492981 0.542742 0.561366 0.527572 0.505562 0.510273 0.504460 0.505930 0.508374 0.514168 0.503497 0.486186 0.472019 0.464787 0.443776 0.426858 0.417955 0.427568 1.029450 
I0428 19:01:58.337625  7412 sgd_solver.cpp:200] weight diff/data:0.062401 0.023634 0.012938 0.011555 0.025204 0.034988 0.017398 0.016554 0.010830 0.016237 0.023295 0.010748 0.012090 0.011158 0.028478 0.016078 0.013546 0.012713 0.010648 0.008835 0.000952 
I0428 19:02:22.719019  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_19000.caffemodel
I0428 19:02:22.727020  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_19000.solverstate
I0428 19:02:22.730020  7412 solver.cpp:336] Iteration 19000, Testing net (#0)
I0428 19:02:31.495522 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:02:31.857542  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8038
I0428 19:02:31.857542  7412 solver.cpp:403]     Test net output #1: loss = 0.633556 (* 1 = 0.633556 loss)
I0428 19:02:32.094557  7412 solver.cpp:224] Iteration 19000 (2.96121 iter/s, 33.77s/100 iters), loss = 0.314743
I0428 19:02:32.094557  7412 solver.cpp:243]     Train net output #0: loss = 0.314743 (* 1 = 0.314743 loss)
I0428 19:02:32.094557  7412 sgd_solver.cpp:137] Iteration 19000, lr = 0.01
I0428 19:02:32.103556  7412 sgd_solver.cpp:169] scale layer:0.459039 0.492086 0.542140 0.560772 0.528224 0.504941 0.509914 0.503991 0.504507 0.508650 0.513711 0.503446 0.485560 0.470672 0.464328 0.442843 0.426180 0.416479 0.425958 1.030523 
I0428 19:02:32.105556  7412 sgd_solver.cpp:200] weight diff/data:0.047740 0.012155 0.015869 0.013681 0.011581 0.016042 0.013829 0.023900 0.021160 0.012930 0.013590 0.012107 0.071132 0.026193 0.011060 0.032097 0.021854 0.012531 0.013108 0.009597 0.001148 
I0428 19:02:56.710964  7412 solver.cpp:224] Iteration 19100 (4.06231 iter/s, 24.6165s/100 iters), loss = 0.276641
I0428 19:02:56.710964  7412 solver.cpp:243]     Train net output #0: loss = 0.276641 (* 1 = 0.276641 loss)
I0428 19:02:56.710964  7412 sgd_solver.cpp:137] Iteration 19100, lr = 0.01
I0428 19:02:56.719964  7412 sgd_solver.cpp:169] scale layer:0.458032 0.487592 0.543019 0.562053 0.525950 0.504171 0.508906 0.503176 0.503188 0.507517 0.513925 0.502966 0.484453 0.470551 0.463685 0.441101 0.424647 0.414904 0.424391 1.029988 
I0428 19:02:56.720964  7412 sgd_solver.cpp:200] weight diff/data:0.171459 0.012595 0.035727 0.023388 0.210898 0.015130 0.092161 0.016151 0.016790 0.014389 0.017231 0.023347 0.148845 0.014303 0.015714 0.037370 0.038070 0.016107 0.014441 0.017903 0.001751 
I0428 19:03:20.105303 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:03:21.325371  7412 solver.cpp:224] Iteration 19200 (4.06249 iter/s, 24.6154s/100 iters), loss = 0.472948
I0428 19:03:21.326372  7412 solver.cpp:243]     Train net output #0: loss = 0.472948 (* 1 = 0.472948 loss)
I0428 19:03:21.326372  7412 sgd_solver.cpp:137] Iteration 19200, lr = 0.01
I0428 19:03:21.334372  7412 sgd_solver.cpp:169] scale layer:0.453599 0.492134 0.540026 0.561187 0.529351 0.502097 0.508308 0.503524 0.501876 0.507630 0.513217 0.502627 0.484563 0.469203 0.462488 0.440926 0.423113 0.413504 0.423174 1.029761 
I0428 19:03:21.337373  7412 sgd_solver.cpp:200] weight diff/data:0.012560 0.068854 0.302757 0.016980 0.034253 0.050590 0.031056 0.027299 0.017025 0.014676 0.031393 0.016767 0.029983 0.014747 0.013609 0.015386 0.021410 0.019551 0.062075 0.048273 0.001128 
I0428 19:03:45.967782  7412 solver.cpp:224] Iteration 19300 (4.05812 iter/s, 24.642s/100 iters), loss = 0.307264
I0428 19:03:45.967782  7412 solver.cpp:243]     Train net output #0: loss = 0.307264 (* 1 = 0.307264 loss)
I0428 19:03:45.967782  7412 sgd_solver.cpp:137] Iteration 19300, lr = 0.01
I0428 19:03:45.976781  7412 sgd_solver.cpp:169] scale layer:0.456668 0.492476 0.538828 0.561857 0.523949 0.502712 0.507682 0.503183 0.501574 0.507887 0.511698 0.502612 0.483637 0.468615 0.461560 0.439863 0.421691 0.412027 0.421737 1.029144 
I0428 19:03:45.978782  7412 sgd_solver.cpp:200] weight diff/data:0.014032 0.011239 0.041731 0.016484 0.019480 0.020434 0.014505 0.015669 0.027898 0.020283 0.034955 0.012274 0.040522 0.013139 0.027298 0.017564 0.014941 0.022304 0.077030 0.012997 0.001631 
I0428 19:04:10.537186  7412 solver.cpp:224] Iteration 19400 (4.07004 iter/s, 24.5698s/100 iters), loss = 0.322871
I0428 19:04:10.537186  7412 solver.cpp:243]     Train net output #0: loss = 0.322871 (* 1 = 0.322871 loss)
I0428 19:04:10.537186  7412 sgd_solver.cpp:137] Iteration 19400, lr = 0.01
I0428 19:04:10.545187  7412 sgd_solver.cpp:169] scale layer:0.460806 0.496681 0.540046 0.561803 0.523145 0.501836 0.506663 0.501934 0.500990 0.507221 0.511678 0.501013 0.483633 0.467888 0.460776 0.439118 0.420522 0.410663 0.419970 1.027352 
I0428 19:04:10.547188  7412 sgd_solver.cpp:200] weight diff/data:0.009111 0.010442 0.008520 0.014418 0.023626 0.017654 0.015829 0.014410 0.013235 0.014900 0.012960 0.013309 0.013441 0.010739 0.011407 0.014912 0.023960 0.012189 0.017038 0.020480 0.008582 
I0428 19:04:34.926581  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_19500.caffemodel
I0428 19:04:34.934582  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_19500.solverstate
I0428 19:04:34.937582  7412 solver.cpp:336] Iteration 19500, Testing net (#0)
I0428 19:04:43.717084 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:04:44.085105  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8023
I0428 19:04:44.085105  7412 solver.cpp:403]     Test net output #1: loss = 0.636566 (* 1 = 0.636566 loss)
I0428 19:04:44.320118  7412 solver.cpp:224] Iteration 19500 (2.95992 iter/s, 33.7847s/100 iters), loss = 0.273596
I0428 19:04:44.321120  7412 solver.cpp:243]     Train net output #0: loss = 0.273596 (* 1 = 0.273596 loss)
I0428 19:04:44.321120  7412 sgd_solver.cpp:137] Iteration 19500, lr = 0.01
I0428 19:04:44.330119  7412 sgd_solver.cpp:169] scale layer:0.461935 0.493023 0.539473 0.561882 0.523125 0.500993 0.506980 0.501730 0.499948 0.506855 0.511927 0.500342 0.482316 0.467541 0.460608 0.437939 0.419896 0.409503 0.418834 1.029645 
I0428 19:04:44.331120  7412 sgd_solver.cpp:200] weight diff/data:0.006544 0.010556 0.009755 0.031505 0.012634 0.016938 0.011832 0.017494 0.024173 0.020627 0.024218 0.019711 0.011598 0.014199 0.015830 0.010875 0.019696 0.012146 0.013452 0.017890 0.001130 
I0428 19:05:07.777460 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:05:09.001530  7412 solver.cpp:224] Iteration 19600 (4.05174 iter/s, 24.6807s/100 iters), loss = 0.338106
I0428 19:05:09.001530  7412 solver.cpp:243]     Train net output #0: loss = 0.338106 (* 1 = 0.338106 loss)
I0428 19:05:09.001530  7412 sgd_solver.cpp:137] Iteration 19600, lr = 0.01
I0428 19:05:09.010531  7412 sgd_solver.cpp:169] scale layer:0.458827 0.492799 0.538441 0.561971 0.524878 0.499702 0.505569 0.500768 0.499846 0.505751 0.511810 0.499851 0.481211 0.466255 0.459212 0.436596 0.418488 0.407988 0.417300 1.028335 
I0428 19:05:09.012531  7412 sgd_solver.cpp:200] weight diff/data:0.007456 0.085580 0.010574 0.010924 0.009854 0.026200 0.013867 0.012873 0.020814 0.036658 0.043778 0.015632 0.013377 0.014760 0.012530 0.011836 0.013617 0.011966 0.021880 0.017743 0.001671 
I0428 19:05:33.619938  7412 solver.cpp:224] Iteration 19700 (4.06181 iter/s, 24.6196s/100 iters), loss = 0.257766
I0428 19:05:33.620939  7412 solver.cpp:243]     Train net output #0: loss = 0.257766 (* 1 = 0.257766 loss)
I0428 19:05:33.620939  7412 sgd_solver.cpp:137] Iteration 19700, lr = 0.01
I0428 19:05:33.628939  7412 sgd_solver.cpp:169] scale layer:0.458843 0.491087 0.540205 0.562069 0.525117 0.500370 0.505387 0.499716 0.499419 0.505053 0.510640 0.499612 0.481542 0.465615 0.458300 0.435930 0.417504 0.407017 0.416212 1.027608 
I0428 19:05:33.630939  7412 sgd_solver.cpp:200] weight diff/data:0.019522 0.009688 0.013602 0.026611 0.010017 0.015868 0.018729 0.039499 0.017032 0.014210 0.016494 0.011294 0.018455 0.012114 0.013292 0.025105 0.013018 0.018001 0.014885 0.011429 0.001637 
I0428 19:05:58.230346  7412 solver.cpp:224] Iteration 19800 (4.0633 iter/s, 24.6105s/100 iters), loss = 0.314918
I0428 19:05:58.230346  7412 solver.cpp:243]     Train net output #0: loss = 0.314918 (* 1 = 0.314918 loss)
I0428 19:05:58.230346  7412 sgd_solver.cpp:137] Iteration 19800, lr = 0.01
I0428 19:05:58.239346  7412 sgd_solver.cpp:169] scale layer:0.458618 0.489540 0.539201 0.560898 0.524094 0.499381 0.504764 0.499918 0.500119 0.504073 0.510263 0.498695 0.480673 0.464858 0.456796 0.435058 0.416615 0.405506 0.414855 1.030039 
I0428 19:05:58.241348  7412 sgd_solver.cpp:200] weight diff/data:0.007953 0.041089 0.048421 0.018842 0.010907 0.012212 0.016208 0.024030 0.017152 0.012584 0.014515 0.061812 0.013298 0.073848 0.016990 0.013631 0.013585 0.014104 0.009045 0.013368 0.000856 
I0428 19:06:22.811753  7412 solver.cpp:224] Iteration 19900 (4.06814 iter/s, 24.5812s/100 iters), loss = 0.345805
I0428 19:06:22.811753  7412 solver.cpp:243]     Train net output #0: loss = 0.345804 (* 1 = 0.345804 loss)
I0428 19:06:22.811753  7412 sgd_solver.cpp:137] Iteration 19900, lr = 0.01
I0428 19:06:22.819752  7412 sgd_solver.cpp:169] scale layer:0.455796 0.483683 0.538923 0.559527 0.521917 0.498991 0.504593 0.499103 0.499289 0.503894 0.509131 0.497660 0.479386 0.465037 0.456048 0.433590 0.415560 0.403838 0.413488 1.029148 
I0428 19:06:22.821753  7412 sgd_solver.cpp:200] weight diff/data:0.007685 0.011341 0.017637 0.012700 0.014895 0.019918 0.018280 0.013338 0.015252 0.030024 0.013843 0.023373 0.015778 0.020105 0.013835 0.017391 0.029934 0.013438 0.023661 0.013028 0.001554 
I0428 19:06:46.237092 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:06:47.222148  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_20000.caffemodel
I0428 19:06:47.231149  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_20000.solverstate
I0428 19:06:47.234149  7412 solver.cpp:336] Iteration 20000, Testing net (#0)
I0428 19:06:55.995651 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:06:56.356672  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8016
I0428 19:06:56.356672  7412 solver.cpp:403]     Test net output #1: loss = 0.633975 (* 1 = 0.633975 loss)
I0428 19:06:56.590684  7412 solver.cpp:224] Iteration 20000 (2.96033 iter/s, 33.78s/100 iters), loss = 0.273364
I0428 19:06:56.590684  7412 solver.cpp:243]     Train net output #0: loss = 0.273364 (* 1 = 0.273364 loss)
I0428 19:06:56.590684  7412 sgd_solver.cpp:137] Iteration 20000, lr = 0.01
I0428 19:06:56.599684  7412 sgd_solver.cpp:169] scale layer:0.452909 0.483437 0.535448 0.558195 0.518122 0.499531 0.502738 0.498241 0.498516 0.503539 0.508776 0.497321 0.478508 0.463934 0.455801 0.433348 0.413864 0.402704 0.412130 1.028026 
I0428 19:06:56.600685  7412 sgd_solver.cpp:200] weight diff/data:0.006019 0.019365 0.028278 0.017029 0.014265 0.024948 0.013920 0.014200 0.017175 0.018105 0.013726 0.075205 0.035897 0.039744 0.011923 0.019361 0.016826 0.024170 0.035976 0.009630 0.002397 
I0428 19:07:21.240094  7412 solver.cpp:224] Iteration 20100 (4.05686 iter/s, 24.6496s/100 iters), loss = 0.222423
I0428 19:07:21.240094  7412 solver.cpp:243]     Train net output #0: loss = 0.222423 (* 1 = 0.222423 loss)
I0428 19:07:21.240094  7412 sgd_solver.cpp:137] Iteration 20100, lr = 0.01
I0428 19:07:21.249094  7412 sgd_solver.cpp:169] scale layer:0.450265 0.484650 0.534553 0.558711 0.520146 0.499646 0.501951 0.497634 0.498820 0.503680 0.508115 0.496969 0.479021 0.463418 0.455028 0.432557 0.413108 0.400925 0.410962 1.027151 
I0428 19:07:21.250094  7412 sgd_solver.cpp:200] weight diff/data:0.006152 0.010424 0.020417 0.058859 0.009433 0.016168 0.012377 0.014018 0.012743 0.013394 0.010351 0.013488 0.011701 0.016499 0.013372 0.017365 0.012110 0.017706 0.008933 0.009134 0.001200 
I0428 19:07:45.847501  7412 solver.cpp:224] Iteration 20200 (4.06375 iter/s, 24.6078s/100 iters), loss = 0.278572
I0428 19:07:45.847501  7412 solver.cpp:243]     Train net output #0: loss = 0.278572 (* 1 = 0.278572 loss)
I0428 19:07:45.847501  7412 sgd_solver.cpp:137] Iteration 20200, lr = 0.01
I0428 19:07:45.856503  7412 sgd_solver.cpp:169] scale layer:0.450407 0.486671 0.536955 0.556484 0.521500 0.498717 0.502352 0.498496 0.498578 0.501830 0.507858 0.496466 0.478585 0.462609 0.454626 0.432082 0.412142 0.399892 0.409710 1.028120 
I0428 19:07:45.859503  7412 sgd_solver.cpp:200] weight diff/data:0.016377 0.011136 0.019507 0.010467 0.017710 0.013468 0.015302 0.014577 0.010618 0.012367 0.069357 0.015504 0.070646 0.052886 0.041454 0.011303 0.026459 0.017520 0.010307 0.020451 0.001551 
I0428 19:08:10.456909  7412 solver.cpp:224] Iteration 20300 (4.06349 iter/s, 24.6094s/100 iters), loss = 0.250436
I0428 19:08:10.456909  7412 solver.cpp:243]     Train net output #0: loss = 0.250436 (* 1 = 0.250436 loss)
I0428 19:08:10.456909  7412 sgd_solver.cpp:137] Iteration 20300, lr = 0.01
I0428 19:08:10.464910  7412 sgd_solver.cpp:169] scale layer:0.448734 0.488462 0.535073 0.557824 0.522318 0.498086 0.503210 0.497478 0.499070 0.501728 0.507702 0.495660 0.478406 0.462080 0.453998 0.430543 0.410334 0.398469 0.408469 1.029691 
I0428 19:08:10.466909  7412 sgd_solver.cpp:200] weight diff/data:0.014253 0.092248 0.023359 0.011003 0.008884 0.013005 0.370208 0.013699 0.015149 0.013595 0.014056 0.016580 0.014052 0.013118 0.017577 0.011984 0.011711 0.016588 0.013778 0.015532 0.001498 
I0428 19:08:33.837246 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:08:35.049315  7412 solver.cpp:224] Iteration 20400 (4.06631 iter/s, 24.5923s/100 iters), loss = 0.283412
I0428 19:08:35.049315  7412 solver.cpp:243]     Train net output #0: loss = 0.283412 (* 1 = 0.283412 loss)
I0428 19:08:35.049315  7412 sgd_solver.cpp:137] Iteration 20400, lr = 0.01
I0428 19:08:35.057317  7412 sgd_solver.cpp:169] scale layer:0.449093 0.489222 0.534828 0.558920 0.523864 0.496945 0.501825 0.496988 0.497146 0.501562 0.507174 0.495103 0.477296 0.461473 0.453237 0.429808 0.409403 0.397219 0.407256 1.029463 
I0428 19:08:35.059316  7412 sgd_solver.cpp:200] weight diff/data:0.009682 0.010614 0.011152 0.011313 0.013671 0.013480 0.017346 0.021235 0.020438 0.010918 0.013822 0.012559 0.014091 0.017222 0.013613 0.018876 0.027467 0.013371 0.011867 0.009789 0.001280 
I0428 19:08:59.423710  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_20500.caffemodel
I0428 19:08:59.431710  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_20500.solverstate
I0428 19:08:59.434710  7412 solver.cpp:336] Iteration 20500, Testing net (#0)
I0428 19:09:08.202213 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:09:08.564232  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7922
I0428 19:09:08.564232  7412 solver.cpp:403]     Test net output #1: loss = 0.676667 (* 1 = 0.676667 loss)
I0428 19:09:08.798246  7412 solver.cpp:224] Iteration 20500 (2.96294 iter/s, 33.7503s/100 iters), loss = 0.253862
I0428 19:09:08.798246  7412 solver.cpp:243]     Train net output #0: loss = 0.253862 (* 1 = 0.253862 loss)
I0428 19:09:08.798246  7412 sgd_solver.cpp:137] Iteration 20500, lr = 0.01
I0428 19:09:08.807246  7412 sgd_solver.cpp:169] scale layer:0.447860 0.483917 0.533461 0.558200 0.520411 0.496553 0.501972 0.496845 0.497072 0.501876 0.506059 0.495274 0.476834 0.460016 0.453307 0.429554 0.408108 0.396032 0.406193 1.028649 
I0428 19:09:08.809247  7412 sgd_solver.cpp:200] weight diff/data:0.006929 0.009714 0.019475 0.014440 0.021368 0.012271 0.013927 0.044504 0.019597 0.037526 0.018565 0.024433 0.013852 0.162677 0.020103 0.018515 0.016115 0.013895 0.013374 0.010303 0.000801 
I0428 19:09:33.445657  7412 solver.cpp:224] Iteration 20600 (4.05718 iter/s, 24.6477s/100 iters), loss = 0.305725
I0428 19:09:33.445657  7412 solver.cpp:243]     Train net output #0: loss = 0.305725 (* 1 = 0.305725 loss)
I0428 19:09:33.445657  7412 sgd_solver.cpp:137] Iteration 20600, lr = 0.01
I0428 19:09:33.453656  7412 sgd_solver.cpp:169] scale layer:0.451571 0.481987 0.533785 0.559370 0.520266 0.496678 0.500459 0.497425 0.497671 0.502400 0.507480 0.493928 0.477021 0.459498 0.452336 0.428606 0.407619 0.394643 0.404960 1.025642 
I0428 19:09:33.455657  7412 sgd_solver.cpp:200] weight diff/data:0.009613 0.011158 0.015207 0.008522 0.018037 0.020848 0.018777 0.015105 0.013697 0.024943 0.015585 0.014049 0.022799 0.011301 0.041403 0.014612 0.017236 0.012380 0.009792 0.009402 0.000846 
I0428 19:09:58.017061  7412 solver.cpp:224] Iteration 20700 (4.06973 iter/s, 24.5717s/100 iters), loss = 0.255295
I0428 19:09:58.017061  7412 solver.cpp:243]     Train net output #0: loss = 0.255295 (* 1 = 0.255295 loss)
I0428 19:09:58.017061  7412 sgd_solver.cpp:137] Iteration 20700, lr = 0.01
I0428 19:09:58.027062  7412 sgd_solver.cpp:169] scale layer:0.455897 0.485534 0.535439 0.560376 0.520593 0.495626 0.501079 0.497066 0.496978 0.502046 0.506524 0.493132 0.476399 0.459539 0.450940 0.427107 0.406205 0.393846 0.403477 1.025825 
I0428 19:09:58.029062  7412 sgd_solver.cpp:200] weight diff/data:0.005733 0.008107 0.034071 0.008443 0.012049 0.016888 0.014631 0.010864 0.012636 0.010490 0.018807 0.009321 0.020410 0.017676 0.017259 0.021113 0.017342 0.009890 0.010855 0.114322 0.001294 
I0428 19:10:21.422400 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:10:22.644470  7412 solver.cpp:224] Iteration 20800 (4.06046 iter/s, 24.6277s/100 iters), loss = 0.351195
I0428 19:10:22.644470  7412 solver.cpp:243]     Train net output #0: loss = 0.351195 (* 1 = 0.351195 loss)
I0428 19:10:22.644470  7412 sgd_solver.cpp:137] Iteration 20800, lr = 0.01
I0428 19:10:22.652470  7412 sgd_solver.cpp:169] scale layer:0.453041 0.482262 0.533828 0.559781 0.522317 0.495863 0.499682 0.496391 0.495501 0.501321 0.505480 0.493932 0.475801 0.457956 0.449768 0.426351 0.405042 0.392166 0.402363 1.025084 
I0428 19:10:22.654470  7412 sgd_solver.cpp:200] weight diff/data:0.012770 0.011855 0.020035 0.009782 0.017619 0.035069 0.016846 0.014806 0.031226 0.015429 0.013468 0.013668 0.018504 0.016850 0.014703 0.015532 0.754621 0.011051 0.015373 0.021227 0.001292 
I0428 19:10:47.279880  7412 solver.cpp:224] Iteration 20900 (4.0591 iter/s, 24.636s/100 iters), loss = 0.299543
I0428 19:10:47.279880  7412 solver.cpp:243]     Train net output #0: loss = 0.299543 (* 1 = 0.299543 loss)
I0428 19:10:47.279880  7412 sgd_solver.cpp:137] Iteration 20900, lr = 0.01
I0428 19:10:47.287879  7412 sgd_solver.cpp:169] scale layer:0.455874 0.481692 0.533090 0.556996 0.524041 0.496439 0.499353 0.495288 0.495181 0.501341 0.505881 0.492979 0.474763 0.457259 0.449438 0.425009 0.404613 0.391361 0.401237 1.026565 
I0428 19:10:47.289880  7412 sgd_solver.cpp:200] weight diff/data:0.008382 0.049985 0.010825 0.021557 0.010384 0.015048 0.023540 0.018458 0.014983 0.014013 0.020796 0.012551 0.016599 0.014189 0.011121 0.019200 0.012406 0.013698 0.012141 0.012767 0.003451 
I0428 19:11:11.656273  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_21000.caffemodel
I0428 19:11:11.664273  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_21000.solverstate
I0428 19:11:11.667273  7412 solver.cpp:336] Iteration 21000, Testing net (#0)
I0428 19:11:20.425774 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:11:20.787796  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8016
I0428 19:11:20.787796  7412 solver.cpp:403]     Test net output #1: loss = 0.656107 (* 1 = 0.656107 loss)
I0428 19:11:21.021809  7412 solver.cpp:224] Iteration 21000 (2.9636 iter/s, 33.7427s/100 iters), loss = 0.328162
I0428 19:11:21.021809  7412 solver.cpp:243]     Train net output #0: loss = 0.328162 (* 1 = 0.328162 loss)
I0428 19:11:21.021809  7412 sgd_solver.cpp:137] Iteration 21000, lr = 0.01
I0428 19:11:21.029809  7412 sgd_solver.cpp:169] scale layer:0.449617 0.482852 0.533539 0.558487 0.522476 0.496134 0.498321 0.495317 0.495122 0.500323 0.504971 0.491693 0.473599 0.455675 0.449410 0.423467 0.403533 0.389990 0.400004 1.027091 
I0428 19:11:21.031810  7412 sgd_solver.cpp:200] weight diff/data:0.006513 0.015802 0.009837 0.012261 0.035316 0.014072 0.019692 0.014181 0.020205 0.017750 0.015011 0.017026 0.020461 0.014914 0.013349 0.019442 0.013554 0.023928 0.009256 0.010113 0.008077 
I0428 19:11:45.624217  7412 solver.cpp:224] Iteration 21100 (4.06443 iter/s, 24.6037s/100 iters), loss = 0.30111
I0428 19:11:45.625216  7412 solver.cpp:243]     Train net output #0: loss = 0.30111 (* 1 = 0.30111 loss)
I0428 19:11:45.625216  7412 sgd_solver.cpp:137] Iteration 21100, lr = 0.01
I0428 19:11:45.634217  7412 sgd_solver.cpp:169] scale layer:0.450774 0.481283 0.533759 0.558551 0.522619 0.495094 0.498026 0.494429 0.493939 0.499654 0.504958 0.491769 0.474862 0.456080 0.448716 0.421721 0.402655 0.388782 0.398668 1.028284 
I0428 19:11:45.636216  7412 sgd_solver.cpp:200] weight diff/data:0.010670 0.011251 0.013721 0.009342 0.013223 0.012592 0.014656 0.017550 0.012014 0.018485 0.013385 0.032521 0.011877 0.017934 0.013095 0.033959 0.019226 0.014192 0.028587 0.010681 0.002702 
I0428 19:12:09.016554 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:12:10.232623  7412 solver.cpp:224] Iteration 21200 (4.0636 iter/s, 24.6087s/100 iters), loss = 0.375389
I0428 19:12:10.232623  7412 solver.cpp:243]     Train net output #0: loss = 0.375389 (* 1 = 0.375389 loss)
I0428 19:12:10.233623  7412 sgd_solver.cpp:137] Iteration 21200, lr = 0.01
I0428 19:12:10.241624  7412 sgd_solver.cpp:169] scale layer:0.453386 0.480911 0.533418 0.560172 0.520796 0.493736 0.498016 0.494970 0.493565 0.499239 0.504214 0.491100 0.473780 0.454652 0.447347 0.420777 0.401459 0.387456 0.397361 1.028517 
I0428 19:12:10.243624  7412 sgd_solver.cpp:200] weight diff/data:0.008344 0.020849 0.010636 0.012661 0.014090 0.011721 0.013309 0.012917 0.018060 0.021843 0.013972 0.018991 0.012803 0.025416 0.017441 0.019169 0.014362 0.012902 0.008477 0.011475 0.001046 
I0428 19:12:34.853031  7412 solver.cpp:224] Iteration 21300 (4.06165 iter/s, 24.6205s/100 iters), loss = 0.294968
I0428 19:12:34.853031  7412 solver.cpp:243]     Train net output #0: loss = 0.294968 (* 1 = 0.294968 loss)
I0428 19:12:34.853031  7412 sgd_solver.cpp:137] Iteration 21300, lr = 0.01
I0428 19:12:34.862032  7412 sgd_solver.cpp:169] scale layer:0.448688 0.479673 0.531515 0.558725 0.520370 0.493966 0.498843 0.493421 0.493097 0.499296 0.504745 0.491181 0.474352 0.454348 0.446623 0.420466 0.401367 0.386468 0.396288 1.026000 
I0428 19:12:34.864032  7412 sgd_solver.cpp:200] weight diff/data:0.008290 0.013264 0.026179 0.018401 0.013474 0.021903 0.015181 0.058401 0.015347 0.017847 0.016588 0.016322 0.013132 0.015795 0.023174 0.013846 0.013339 0.019668 0.011175 0.016584 0.002300 
I0428 19:12:59.482440  7412 solver.cpp:224] Iteration 21400 (4.06013 iter/s, 24.6297s/100 iters), loss = 0.252328
I0428 19:12:59.482440  7412 solver.cpp:243]     Train net output #0: loss = 0.252328 (* 1 = 0.252328 loss)
I0428 19:12:59.482440  7412 sgd_solver.cpp:137] Iteration 21400, lr = 0.01
I0428 19:12:59.490442  7412 sgd_solver.cpp:169] scale layer:0.448480 0.478699 0.531655 0.557759 0.519967 0.492237 0.498513 0.493975 0.492389 0.498569 0.504709 0.489646 0.473423 0.455227 0.446019 0.419655 0.400140 0.385603 0.395006 1.026562 
I0428 19:12:59.492441  7412 sgd_solver.cpp:200] weight diff/data:0.022299 0.012540 0.011841 0.013743 0.015544 0.012189 0.013081 0.020663 0.014114 0.013494 0.012080 0.016911 0.013705 0.010363 0.028562 0.014899 0.016640 0.016330 0.012036 0.011797 0.001332 
I0428 19:13:23.851835  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_21500.caffemodel
I0428 19:13:23.859834  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_21500.solverstate
I0428 19:13:23.862835  7412 solver.cpp:336] Iteration 21500, Testing net (#0)
I0428 19:13:32.609335 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:13:32.972357  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8028
I0428 19:13:32.972357  7412 solver.cpp:403]     Test net output #1: loss = 0.641696 (* 1 = 0.641696 loss)
I0428 19:13:33.208369  7412 solver.cpp:224] Iteration 21500 (2.965 iter/s, 33.7268s/100 iters), loss = 0.35486
I0428 19:13:33.208369  7412 solver.cpp:243]     Train net output #0: loss = 0.35486 (* 1 = 0.35486 loss)
I0428 19:13:33.208369  7412 sgd_solver.cpp:137] Iteration 21500, lr = 0.01
I0428 19:13:33.217370  7412 sgd_solver.cpp:169] scale layer:0.452611 0.476656 0.532225 0.557479 0.522253 0.491764 0.498879 0.494049 0.492545 0.497902 0.503798 0.489835 0.473701 0.454802 0.445785 0.418492 0.398686 0.384464 0.393976 1.026413 
I0428 19:13:33.218370  7412 sgd_solver.cpp:200] weight diff/data:0.006172 0.007540 0.012825 0.014431 0.029999 0.010321 0.014822 0.013952 0.011234 0.039313 0.011965 0.016645 0.012188 0.020068 0.044820 0.014112 0.017449 0.010995 0.010872 0.016193 0.001856 
I0428 19:13:56.615708 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:13:57.836778  7412 solver.cpp:224] Iteration 21600 (4.06026 iter/s, 24.629s/100 iters), loss = 0.279817
I0428 19:13:57.836778  7412 solver.cpp:243]     Train net output #0: loss = 0.279817 (* 1 = 0.279817 loss)
I0428 19:13:57.836778  7412 sgd_solver.cpp:137] Iteration 21600, lr = 0.01
I0428 19:13:57.844779  7412 sgd_solver.cpp:169] scale layer:0.453183 0.479925 0.532491 0.555555 0.525755 0.491720 0.499373 0.493628 0.491379 0.497936 0.503939 0.489045 0.473239 0.453384 0.445075 0.418327 0.397811 0.383283 0.392984 1.025151 
I0428 19:13:57.846778  7412 sgd_solver.cpp:200] weight diff/data:0.009180 0.014241 0.019075 0.014295 0.036275 0.015997 0.012749 0.013558 0.015518 0.013975 0.058292 0.021063 0.011921 0.024300 0.012110 0.015914 0.016720 0.019484 0.011709 0.027100 0.001436 
I0428 19:14:22.464186  7412 solver.cpp:224] Iteration 21700 (4.06036 iter/s, 24.6283s/100 iters), loss = 0.190873
I0428 19:14:22.464186  7412 solver.cpp:243]     Train net output #0: loss = 0.190873 (* 1 = 0.190873 loss)
I0428 19:14:22.464186  7412 sgd_solver.cpp:137] Iteration 21700, lr = 0.01
I0428 19:14:22.474187  7412 sgd_solver.cpp:169] scale layer:0.451107 0.478965 0.532511 0.556359 0.521349 0.492494 0.498940 0.491684 0.491317 0.496795 0.502489 0.489227 0.473979 0.453639 0.444200 0.417877 0.397195 0.381994 0.392027 1.025746 
I0428 19:14:22.476187  7412 sgd_solver.cpp:200] weight diff/data:0.007182 0.008824 0.011937 0.008950 0.013698 0.056790 0.016850 0.015845 0.014276 0.017024 0.014863 0.020812 0.015010 0.018428 0.016573 0.010456 0.017021 0.021045 0.032245 0.009671 0.001678 
I0428 19:14:47.128597  7412 solver.cpp:224] Iteration 21800 (4.05437 iter/s, 24.6647s/100 iters), loss = 0.319951
I0428 19:14:47.128597  7412 solver.cpp:243]     Train net output #0: loss = 0.319951 (* 1 = 0.319951 loss)
I0428 19:14:47.128597  7412 sgd_solver.cpp:137] Iteration 21800, lr = 0.01
I0428 19:14:47.137598  7412 sgd_solver.cpp:169] scale layer:0.453044 0.478539 0.532848 0.555871 0.519615 0.492917 0.499849 0.492051 0.490675 0.496331 0.503191 0.489293 0.473422 0.453673 0.443897 0.417061 0.396003 0.381068 0.390973 1.026687 
I0428 19:14:47.139598  7412 sgd_solver.cpp:200] weight diff/data:0.006948 0.014312 0.014547 0.008492 0.010626 0.010455 0.012126 0.247703 0.015587 0.013181 0.045938 0.011159 0.015282 0.013228 0.013097 0.010911 0.026450 0.026384 0.012904 0.014981 0.002573 
I0428 19:15:11.801008  7412 solver.cpp:224] Iteration 21900 (4.05314 iter/s, 24.6722s/100 iters), loss = 0.218242
I0428 19:15:11.801008  7412 solver.cpp:243]     Train net output #0: loss = 0.218242 (* 1 = 0.218242 loss)
I0428 19:15:11.801008  7412 sgd_solver.cpp:137] Iteration 21900, lr = 0.01
I0428 19:15:11.809010  7412 sgd_solver.cpp:169] scale layer:0.447471 0.477043 0.532093 0.556862 0.520203 0.491488 0.499124 0.491438 0.490736 0.495830 0.501770 0.489461 0.471895 0.452805 0.443402 0.415550 0.395220 0.379683 0.389810 1.030328 
I0428 19:15:11.811009  7412 sgd_solver.cpp:200] weight diff/data:0.008697 0.012801 0.017886 0.026587 0.010984 0.013242 0.051536 0.012410 0.013850 0.029106 0.014211 0.017228 0.017651 0.010361 0.014469 0.013204 0.025469 0.014372 0.014764 0.012956 0.001795 
I0428 19:15:35.253350 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:15:36.229406  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_22000.caffemodel
I0428 19:15:36.238406  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_22000.solverstate
I0428 19:15:36.241406  7412 solver.cpp:336] Iteration 22000, Testing net (#0)
I0428 19:15:45.003907 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:15:45.369928  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7991
I0428 19:15:45.370929  7412 solver.cpp:403]     Test net output #1: loss = 0.646996 (* 1 = 0.646996 loss)
I0428 19:15:45.601943  7412 solver.cpp:224] Iteration 22000 (2.95842 iter/s, 33.8018s/100 iters), loss = 0.210931
I0428 19:15:45.601943  7412 solver.cpp:243]     Train net output #0: loss = 0.210931 (* 1 = 0.210931 loss)
I0428 19:15:45.601943  7412 sgd_solver.cpp:137] Iteration 22000, lr = 0.01
I0428 19:15:45.610942  7412 sgd_solver.cpp:169] scale layer:0.446967 0.478047 0.530801 0.557243 0.521443 0.492196 0.498000 0.492258 0.490155 0.495993 0.502107 0.487899 0.470873 0.452266 0.442942 0.413826 0.394542 0.379051 0.388708 1.027281 
I0428 19:15:45.611943  7412 sgd_solver.cpp:200] weight diff/data:0.008696 0.011813 0.013095 0.010875 0.010561 0.011181 0.011308 0.013857 0.016070 0.011308 0.013641 0.045636 0.017957 0.013858 0.031334 0.022622 0.044129 0.011826 0.015191 0.012951 0.001914 
I0428 19:16:10.274353  7412 solver.cpp:224] Iteration 22100 (4.05299 iter/s, 24.6731s/100 iters), loss = 0.286357
I0428 19:16:10.274353  7412 solver.cpp:243]     Train net output #0: loss = 0.286356 (* 1 = 0.286356 loss)
I0428 19:16:10.274353  7412 sgd_solver.cpp:137] Iteration 22100, lr = 0.01
I0428 19:16:10.283354  7412 sgd_solver.cpp:169] scale layer:0.456271 0.477729 0.529392 0.555058 0.522148 0.492258 0.496819 0.490925 0.489802 0.496475 0.502426 0.487951 0.470031 0.452668 0.442183 0.413868 0.393694 0.378609 0.387703 1.026693 
I0428 19:16:10.286355  7412 sgd_solver.cpp:200] weight diff/data:0.016084 0.014161 0.012947 0.012068 0.015597 0.014827 0.014876 0.019344 0.017003 0.023555 0.013167 0.012932 0.013897 0.011151 0.014132 0.014387 0.024271 0.018380 0.016505 0.015825 0.002657 
I0428 19:16:34.855759  7412 solver.cpp:224] Iteration 22200 (4.06813 iter/s, 24.5813s/100 iters), loss = 0.413888
I0428 19:16:34.855759  7412 solver.cpp:243]     Train net output #0: loss = 0.413888 (* 1 = 0.413888 loss)
I0428 19:16:34.855759  7412 sgd_solver.cpp:137] Iteration 22200, lr = 0.01
I0428 19:16:34.864759  7412 sgd_solver.cpp:169] scale layer:0.455008 0.476976 0.530870 0.555912 0.520003 0.491489 0.498308 0.491821 0.489696 0.496028 0.502830 0.487361 0.470100 0.452037 0.441539 0.413872 0.392704 0.377266 0.386750 1.025791 
I0428 19:16:34.866760  7412 sgd_solver.cpp:200] weight diff/data:0.240154 0.017147 0.009900 0.010700 0.016229 0.014554 0.012016 0.039701 0.024610 0.015742 5.283805 0.012784 0.023917 0.013044 0.018380 0.019515 0.013513 0.017044 0.029884 0.013210 0.010709 
I0428 19:16:59.475167  7412 solver.cpp:224] Iteration 22300 (4.06172 iter/s, 24.6201s/100 iters), loss = 0.231086
I0428 19:16:59.475167  7412 solver.cpp:243]     Train net output #0: loss = 0.231086 (* 1 = 0.231086 loss)
I0428 19:16:59.475167  7412 sgd_solver.cpp:137] Iteration 22300, lr = 0.01
I0428 19:16:59.484167  7412 sgd_solver.cpp:169] scale layer:0.450615 0.475267 0.529575 0.556468 0.519329 0.490626 0.497748 0.490699 0.491216 0.494745 0.502186 0.487569 0.469985 0.451900 0.440915 0.412917 0.392542 0.376367 0.385993 1.026586 
I0428 19:16:59.486168  7412 sgd_solver.cpp:200] weight diff/data:0.006951 0.093641 0.018021 0.049348 0.029547 0.014381 0.013080 0.032999 0.015393 0.009681 0.013139 0.012794 0.017651 0.020332 0.013591 0.013008 0.016813 0.013399 0.011434 0.012910 0.002422 
I0428 19:17:22.858505 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:17:24.077574  7412 solver.cpp:224] Iteration 22400 (4.06464 iter/s, 24.6024s/100 iters), loss = 0.433572
I0428 19:17:24.077574  7412 solver.cpp:243]     Train net output #0: loss = 0.433572 (* 1 = 0.433572 loss)
I0428 19:17:24.077574  7412 sgd_solver.cpp:137] Iteration 22400, lr = 0.01
I0428 19:17:24.085575  7412 sgd_solver.cpp:169] scale layer:0.449568 0.476559 0.527407 0.557073 0.521225 0.489782 0.496947 0.490886 0.490491 0.494257 0.502820 0.488365 0.468997 0.450892 0.441150 0.412058 0.391129 0.375426 0.384884 1.026252 
I0428 19:17:24.087575  7412 sgd_solver.cpp:200] weight diff/data:0.011880 0.022260 0.013977 0.027732 0.015467 0.015385 0.045013 0.037860 0.016746 0.015650 0.023887 0.015052 0.024121 0.023205 0.012312 0.014807 0.015710 0.017470 0.018657 0.009770 0.001397 
I0428 19:17:48.456969  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_22500.caffemodel
I0428 19:17:48.465970  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_22500.solverstate
I0428 19:17:48.468969  7412 solver.cpp:336] Iteration 22500, Testing net (#0)
I0428 19:17:57.215471 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:17:57.575490  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7947
I0428 19:17:57.575490  7412 solver.cpp:403]     Test net output #1: loss = 0.680406 (* 1 = 0.680406 loss)
I0428 19:17:57.809504  7412 solver.cpp:224] Iteration 22500 (2.96447 iter/s, 33.7328s/100 iters), loss = 0.248985
I0428 19:17:57.809504  7412 solver.cpp:243]     Train net output #0: loss = 0.248985 (* 1 = 0.248985 loss)
I0428 19:17:57.809504  7412 sgd_solver.cpp:137] Iteration 22500, lr = 0.01
I0428 19:17:57.819504  7412 sgd_solver.cpp:169] scale layer:0.454412 0.476418 0.530240 0.555389 0.518565 0.490218 0.496911 0.490249 0.490876 0.493699 0.502723 0.487379 0.469966 0.451450 0.440340 0.413266 0.390159 0.374316 0.384045 1.024919 
I0428 19:17:57.821504  7412 sgd_solver.cpp:200] weight diff/data:0.007870 1.411422 0.017554 0.010954 0.013073 0.012946 0.020474 0.016233 0.025270 0.015646 0.020100 0.025857 0.019702 0.018654 0.033453 0.025593 0.022134 0.011120 0.012419 0.012618 0.001457 
I0428 19:18:22.431912  7412 solver.cpp:224] Iteration 22600 (4.06123 iter/s, 24.6231s/100 iters), loss = 0.350143
I0428 19:18:22.432912  7412 solver.cpp:243]     Train net output #0: loss = 0.350143 (* 1 = 0.350143 loss)
I0428 19:18:22.432912  7412 sgd_solver.cpp:137] Iteration 22600, lr = 0.01
I0428 19:18:22.441912  7412 sgd_solver.cpp:169] scale layer:0.450954 0.476629 0.530090 0.556024 0.518660 0.490083 0.496390 0.491542 0.491658 0.493405 0.502422 0.487688 0.469449 0.450662 0.439761 0.413103 0.389270 0.373198 0.383178 1.022919 
I0428 19:18:22.443912  7412 sgd_solver.cpp:200] weight diff/data:0.007878 0.021469 0.013322 0.060122 0.015262 0.012637 0.017944 0.011284 0.014335 0.144158 0.012922 0.598056 0.020815 0.018481 0.010972 0.010185 0.013568 0.296219 0.008789 0.010338 0.002602 
I0428 19:18:47.022318  7412 solver.cpp:224] Iteration 22700 (4.06657 iter/s, 24.5907s/100 iters), loss = 0.19147
I0428 19:18:47.022318  7412 solver.cpp:243]     Train net output #0: loss = 0.19147 (* 1 = 0.19147 loss)
I0428 19:18:47.023319  7412 sgd_solver.cpp:137] Iteration 22700, lr = 0.01
I0428 19:18:47.032320  7412 sgd_solver.cpp:169] scale layer:0.449460 0.478989 0.528637 0.555733 0.518199 0.490588 0.496479 0.491254 0.490966 0.493705 0.501630 0.487215 0.468558 0.449983 0.439322 0.411855 0.389674 0.371788 0.381930 1.023844 
I0428 19:18:47.034319  7412 sgd_solver.cpp:200] weight diff/data:0.013301 0.009023 0.016600 0.013424 0.020796 0.015691 0.027455 0.031869 0.024906 0.016400 0.035048 0.011500 0.014087 0.017407 0.013753 0.041836 0.026450 0.023031 0.014538 0.016218 0.001714 
I0428 19:19:10.440659 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:19:11.661728  7412 solver.cpp:224] Iteration 22800 (4.05846 iter/s, 24.6399s/100 iters), loss = 0.315866
I0428 19:19:11.662729  7412 solver.cpp:243]     Train net output #0: loss = 0.315866 (* 1 = 0.315866 loss)
I0428 19:19:11.662729  7412 sgd_solver.cpp:137] Iteration 22800, lr = 0.01
I0428 19:19:11.671728  7412 sgd_solver.cpp:169] scale layer:0.446160 0.475566 0.528257 0.556333 0.522623 0.488878 0.495367 0.490043 0.490911 0.493413 0.501912 0.487592 0.468656 0.449401 0.438651 0.411575 0.389087 0.370742 0.381253 1.023948 
I0428 19:19:11.673728  7412 sgd_solver.cpp:200] weight diff/data:0.006092 0.010935 0.011041 0.011719 0.016300 0.012523 0.013958 0.031285 0.018245 0.012619 0.069309 0.012836 0.017454 0.017350 0.016340 0.033477 0.014328 0.012321 0.015268 0.014807 0.001644 
I0428 19:19:36.361140  7412 solver.cpp:224] Iteration 22900 (4.04868 iter/s, 24.6994s/100 iters), loss = 0.273251
I0428 19:19:36.361140  7412 solver.cpp:243]     Train net output #0: loss = 0.273251 (* 1 = 0.273251 loss)
I0428 19:19:36.361140  7412 sgd_solver.cpp:137] Iteration 22900, lr = 0.01
I0428 19:19:36.369141  7412 sgd_solver.cpp:169] scale layer:0.448608 0.477918 0.526893 0.556240 0.520122 0.489840 0.495947 0.489458 0.490511 0.494324 0.501263 0.486878 0.468294 0.448691 0.438898 0.411238 0.388423 0.369987 0.379964 1.022999 
I0428 19:19:36.372141  7412 sgd_solver.cpp:200] weight diff/data:0.008683 0.013218 0.048106 0.012456 0.010114 0.015262 0.012981 0.039153 0.019164 0.026158 0.013312 0.016202 0.011599 0.010884 0.013894 0.027729 0.010719 0.016008 0.016151 0.017938 0.000962 
I0428 19:20:00.759536  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_23000.caffemodel
I0428 19:20:00.767536  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_23000.solverstate
I0428 19:20:00.770536  7412 solver.cpp:336] Iteration 23000, Testing net (#0)
I0428 19:20:09.524037 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:20:09.885058  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8011
I0428 19:20:09.885058  7412 solver.cpp:403]     Test net output #1: loss = 0.666157 (* 1 = 0.666157 loss)
I0428 19:20:10.116071  7412 solver.cpp:224] Iteration 23000 (2.96247 iter/s, 33.7556s/100 iters), loss = 0.29342
I0428 19:20:10.116071  7412 solver.cpp:243]     Train net output #0: loss = 0.29342 (* 1 = 0.29342 loss)
I0428 19:20:10.116071  7412 sgd_solver.cpp:137] Iteration 23000, lr = 0.01
I0428 19:20:10.124071  7412 sgd_solver.cpp:169] scale layer:0.448427 0.475696 0.527215 0.556159 0.520711 0.489381 0.495808 0.489996 0.489803 0.494237 0.501489 0.487301 0.468093 0.448924 0.437995 0.410828 0.387165 0.368847 0.378972 1.022093 
I0428 19:20:10.126072  7412 sgd_solver.cpp:200] weight diff/data:0.007786 0.023041 0.113227 0.013264 0.009916 0.020185 0.013228 0.017923 0.012519 0.011118 0.028210 0.012036 0.011177 0.010939 0.015281 0.011644 0.011880 0.011097 0.008577 0.032842 0.001784 
I0428 19:20:34.724479  7412 solver.cpp:224] Iteration 23100 (4.06347 iter/s, 24.6095s/100 iters), loss = 0.19482
I0428 19:20:34.725479  7412 solver.cpp:243]     Train net output #0: loss = 0.19482 (* 1 = 0.19482 loss)
I0428 19:20:34.725479  7412 sgd_solver.cpp:137] Iteration 23100, lr = 0.01
I0428 19:20:34.734479  7412 sgd_solver.cpp:169] scale layer:0.444923 0.472060 0.525585 0.557138 0.516545 0.488998 0.495572 0.490164 0.489403 0.493728 0.502302 0.487953 0.468000 0.448375 0.437833 0.409192 0.386672 0.367499 0.377948 1.023981 
I0428 19:20:34.736479  7412 sgd_solver.cpp:200] weight diff/data:0.012515 0.017405 0.011356 0.015131 0.008549 0.011131 0.022037 0.012659 0.012063 0.014629 0.010929 0.009581 0.010729 0.013441 0.021291 0.016459 0.011266 0.011804 0.010657 0.011484 0.001611 
I0428 19:20:58.134819 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:20:59.355887  7412 solver.cpp:224] Iteration 23200 (4.05991 iter/s, 24.6311s/100 iters), loss = 0.331305
I0428 19:20:59.355887  7412 solver.cpp:243]     Train net output #0: loss = 0.331305 (* 1 = 0.331305 loss)
I0428 19:20:59.355887  7412 sgd_solver.cpp:137] Iteration 23200, lr = 0.01
I0428 19:20:59.364888  7412 sgd_solver.cpp:169] scale layer:0.441947 0.475295 0.524761 0.556213 0.516009 0.488654 0.495263 0.489100 0.488463 0.493099 0.501621 0.487219 0.466861 0.447878 0.437361 0.409711 0.385478 0.366705 0.377136 1.023353 
I0428 19:20:59.366888  7412 sgd_solver.cpp:200] weight diff/data:0.011001 0.011628 0.020778 0.013366 0.015295 0.014407 0.024566 0.012658 0.014457 1.654256 0.027859 0.014990 0.020075 0.012847 0.040878 0.012244 0.027866 0.016478 0.012953 0.030187 0.001413 
I0428 19:21:23.933293  7412 solver.cpp:224] Iteration 23300 (4.06867 iter/s, 24.5781s/100 iters), loss = 0.27035
I0428 19:21:23.933293  7412 solver.cpp:243]     Train net output #0: loss = 0.27035 (* 1 = 0.27035 loss)
I0428 19:21:23.933293  7412 sgd_solver.cpp:137] Iteration 23300, lr = 0.01
I0428 19:21:23.941294  7412 sgd_solver.cpp:169] scale layer:0.444607 0.474076 0.527931 0.554837 0.517984 0.488637 0.496345 0.488893 0.489721 0.493294 0.500823 0.487830 0.466716 0.446895 0.436213 0.409965 0.385166 0.365911 0.376574 1.021849 
I0428 19:21:23.943295  7412 sgd_solver.cpp:200] weight diff/data:0.023514 0.038071 0.032639 0.018269 0.017548 0.014226 0.017535 0.013138 0.029423 0.022089 0.013711 0.012701 0.018137 0.023558 0.013658 0.015484 0.023351 0.034181 0.012139 0.013199 0.001114 
I0428 19:21:48.535701  7412 solver.cpp:224] Iteration 23400 (4.06453 iter/s, 24.6031s/100 iters), loss = 0.325782
I0428 19:21:48.535701  7412 solver.cpp:243]     Train net output #0: loss = 0.325782 (* 1 = 0.325782 loss)
I0428 19:21:48.535701  7412 sgd_solver.cpp:137] Iteration 23400, lr = 0.01
I0428 19:21:48.544701  7412 sgd_solver.cpp:169] scale layer:0.443365 0.476826 0.527400 0.555438 0.514775 0.488310 0.494916 0.488770 0.489481 0.493277 0.500670 0.487020 0.466890 0.446919 0.436373 0.409416 0.384576 0.365196 0.375460 1.022237 
I0428 19:21:48.545701  7412 sgd_solver.cpp:200] weight diff/data:0.008688 0.014943 0.010279 0.012460 0.015730 0.031334 0.021132 0.096611 0.013234 0.014789 0.013651 0.017314 0.011032 0.018932 0.012809 0.207859 0.029181 0.012229 0.009820 0.027456 0.001284 
I0428 19:22:12.874092  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_23500.caffemodel
I0428 19:22:12.882093  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_23500.solverstate
I0428 19:22:12.885093  7412 solver.cpp:336] Iteration 23500, Testing net (#0)
I0428 19:22:21.640594 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:22:22.000614  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8042
I0428 19:22:22.000614  7412 solver.cpp:403]     Test net output #1: loss = 0.648237 (* 1 = 0.648237 loss)
I0428 19:22:22.233628  7412 solver.cpp:224] Iteration 23500 (2.9675 iter/s, 33.6984s/100 iters), loss = 0.232236
I0428 19:22:22.233628  7412 solver.cpp:243]     Train net output #0: loss = 0.232236 (* 1 = 0.232236 loss)
I0428 19:22:22.233628  7412 sgd_solver.cpp:137] Iteration 23500, lr = 0.01
I0428 19:22:22.241628  7412 sgd_solver.cpp:169] scale layer:0.445924 0.470369 0.528584 0.556817 0.516036 0.488288 0.495438 0.488891 0.488815 0.491814 0.500694 0.486984 0.466385 0.447517 0.436528 0.409105 0.383565 0.364433 0.374478 1.022337 
I0428 19:22:22.243628  7412 sgd_solver.cpp:200] weight diff/data:0.012541 0.010098 0.021252 0.011642 0.010757 0.013275 0.013200 0.014700 0.018901 0.017772 0.018174 0.034815 0.015261 0.021023 0.013700 0.011589 0.049180 0.021129 0.014834 0.012411 0.003256 
I0428 19:22:45.672968 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:22:46.902040  7412 solver.cpp:224] Iteration 23600 (4.05359 iter/s, 24.6695s/100 iters), loss = 0.396065
I0428 19:22:46.902040  7412 solver.cpp:243]     Train net output #0: loss = 0.396065 (* 1 = 0.396065 loss)
I0428 19:22:46.902040  7412 sgd_solver.cpp:137] Iteration 23600, lr = 0.01
I0428 19:22:46.911039  7412 sgd_solver.cpp:169] scale layer:0.442558 0.472346 0.527314 0.556607 0.515480 0.487912 0.493950 0.488904 0.488167 0.492737 0.499783 0.486705 0.465247 0.447026 0.435291 0.408662 0.383449 0.363829 0.373130 1.021028 
I0428 19:22:46.913039  7412 sgd_solver.cpp:200] weight diff/data:0.015652 0.017159 0.015183 0.011255 0.015653 0.012946 0.025066 0.020826 0.024144 0.016347 0.014653 0.012803 0.019423 0.016933 0.016905 0.015263 0.015043 0.016709 0.010847 0.014011 0.001508 
I0428 19:23:11.537448  7412 solver.cpp:224] Iteration 23700 (4.05913 iter/s, 24.6358s/100 iters), loss = 0.251047
I0428 19:23:11.537448  7412 solver.cpp:243]     Train net output #0: loss = 0.251047 (* 1 = 0.251047 loss)
I0428 19:23:11.537448  7412 sgd_solver.cpp:137] Iteration 23700, lr = 0.01
I0428 19:23:11.546448  7412 sgd_solver.cpp:169] scale layer:0.446758 0.473260 0.527299 0.553567 0.515556 0.488610 0.493969 0.488164 0.487600 0.492661 0.499713 0.486456 0.464512 0.446202 0.436365 0.408328 0.384157 0.363329 0.372566 1.019007 
I0428 19:23:11.548449  7412 sgd_solver.cpp:200] weight diff/data:0.016963 1.420555 0.017689 0.013800 0.011229 0.033469 0.020709 0.015159 0.025913 0.012287 0.013392 0.022788 0.014941 0.020907 0.011419 0.031185 0.010343 0.012333 0.011647 0.011846 0.003021 
I0428 19:23:36.141855  7412 solver.cpp:224] Iteration 23800 (4.06426 iter/s, 24.6047s/100 iters), loss = 0.199487
I0428 19:23:36.141855  7412 solver.cpp:243]     Train net output #0: loss = 0.199487 (* 1 = 0.199487 loss)
I0428 19:23:36.141855  7412 sgd_solver.cpp:137] Iteration 23800, lr = 0.01
I0428 19:23:36.150856  7412 sgd_solver.cpp:169] scale layer:0.449957 0.473651 0.527892 0.554601 0.515779 0.487644 0.493789 0.488883 0.487640 0.491825 0.498864 0.485454 0.464784 0.445435 0.436358 0.407204 0.383482 0.362757 0.371599 1.018026 
I0428 19:23:36.152856  7412 sgd_solver.cpp:200] weight diff/data:0.006182 0.012191 0.029329 0.017061 0.018282 0.011246 0.015365 0.011580 0.013215 0.012759 0.021233 0.019257 0.018148 0.013115 0.014037 0.015071 0.012228 0.012443 0.010707 0.042166 0.001233 
I0428 19:24:00.767263  7412 solver.cpp:224] Iteration 23900 (4.06078 iter/s, 24.6258s/100 iters), loss = 0.214296
I0428 19:24:00.767263  7412 solver.cpp:243]     Train net output #0: loss = 0.214296 (* 1 = 0.214296 loss)
I0428 19:24:00.767263  7412 sgd_solver.cpp:137] Iteration 23900, lr = 0.01
I0428 19:24:00.777264  7412 sgd_solver.cpp:169] scale layer:0.445745 0.467749 0.529577 0.555851 0.511626 0.486082 0.493680 0.488346 0.486890 0.491774 0.498979 0.486094 0.464271 0.445800 0.435972 0.407707 0.382044 0.362062 0.370950 1.017954 
I0428 19:24:00.779264  7412 sgd_solver.cpp:200] weight diff/data:0.010252 0.011988 0.034575 0.012127 0.008927 0.012204 0.037958 0.014001 0.015206 0.008770 0.017321 0.012506 0.013156 0.013250 0.014956 0.016467 0.013295 0.011864 0.014323 0.011397 0.001911 
I0428 19:24:24.160603 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:24:25.143658  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_24000.caffemodel
I0428 19:24:25.152658  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_24000.solverstate
I0428 19:24:25.155658  7412 solver.cpp:336] Iteration 24000, Testing net (#0)
I0428 19:24:33.914160 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:24:34.279181  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8033
I0428 19:24:34.279181  7412 solver.cpp:403]     Test net output #1: loss = 0.667539 (* 1 = 0.667539 loss)
I0428 19:24:34.515194  7412 solver.cpp:224] Iteration 24000 (2.96313 iter/s, 33.7481s/100 iters), loss = 0.288959
I0428 19:24:34.515194  7412 solver.cpp:243]     Train net output #0: loss = 0.288959 (* 1 = 0.288959 loss)
I0428 19:24:34.515194  7412 sgd_solver.cpp:137] Iteration 24000, lr = 0.01
I0428 19:24:34.523195  7412 sgd_solver.cpp:169] scale layer:0.449029 0.469922 0.528507 0.555570 0.515988 0.486939 0.492981 0.488146 0.485881 0.491423 0.498702 0.486215 0.464355 0.445394 0.435141 0.407013 0.380843 0.361410 0.370050 1.019011 
I0428 19:24:34.525194  7412 sgd_solver.cpp:200] weight diff/data:0.009382 0.013441 0.013460 0.010719 0.014585 0.010240 0.016323 0.012440 0.016602 0.014304 0.015164 0.013791 0.012863 0.017187 0.110736 0.011353 0.013122 0.012495 0.009197 0.021954 0.001334 
I0428 19:24:59.148603  7412 solver.cpp:224] Iteration 24100 (4.05935 iter/s, 24.6345s/100 iters), loss = 0.348549
I0428 19:24:59.149603  7412 solver.cpp:243]     Train net output #0: loss = 0.348548 (* 1 = 0.348548 loss)
I0428 19:24:59.149603  7412 sgd_solver.cpp:137] Iteration 24100, lr = 0.01
I0428 19:24:59.157603  7412 sgd_solver.cpp:169] scale layer:0.454350 0.474624 0.526667 0.554286 0.516181 0.487393 0.493464 0.486864 0.486596 0.491686 0.499728 0.486137 0.465605 0.445352 0.435575 0.406522 0.380453 0.360960 0.369222 1.019659 
I0428 19:24:59.159603  7412 sgd_solver.cpp:200] weight diff/data:0.014533 0.014638 0.011173 0.011069 0.017580 0.024876 0.025045 0.012978 0.013088 0.012563 0.012948 0.011478 0.034298 0.016123 0.018200 0.019440 0.019790 0.011493 0.014442 0.014468 0.001531 
I0428 19:25:23.765012  7412 solver.cpp:224] Iteration 24200 (4.0623 iter/s, 24.6166s/100 iters), loss = 0.278844
I0428 19:25:23.765012  7412 solver.cpp:243]     Train net output #0: loss = 0.278844 (* 1 = 0.278844 loss)
I0428 19:25:23.766011  7412 sgd_solver.cpp:137] Iteration 24200, lr = 0.01
I0428 19:25:23.775012  7412 sgd_solver.cpp:169] scale layer:0.450622 0.474671 0.528626 0.555807 0.518515 0.486977 0.495071 0.488612 0.487307 0.491617 0.499775 0.485912 0.465450 0.445737 0.434930 0.405677 0.380307 0.359945 0.368538 1.017994 
I0428 19:25:23.777011  7412 sgd_solver.cpp:200] weight diff/data:0.025406 0.009777 0.008568 0.014534 0.012307 0.014100 0.014128 0.034438 0.015436 0.013805 0.042810 0.029362 0.015536 0.016610 0.016987 0.014682 0.015435 0.010619 0.013006 0.014741 0.001336 
I0428 19:25:48.393419  7412 solver.cpp:224] Iteration 24300 (4.06036 iter/s, 24.6283s/100 iters), loss = 0.242507
I0428 19:25:48.393419  7412 solver.cpp:243]     Train net output #0: loss = 0.242507 (* 1 = 0.242507 loss)
I0428 19:25:48.393419  7412 sgd_solver.cpp:137] Iteration 24300, lr = 0.01
I0428 19:25:48.401420  7412 sgd_solver.cpp:169] scale layer:0.453115 0.474798 0.529201 0.555749 0.518578 0.486384 0.495248 0.489497 0.488039 0.491720 0.500134 0.485227 0.465607 0.445339 0.434394 0.405210 0.380201 0.358834 0.367795 1.017026 
I0428 19:25:48.404420  7412 sgd_solver.cpp:200] weight diff/data:0.030939 0.012302 0.032534 0.009705 0.011909 0.013968 0.013442 0.055952 0.012394 0.010585 0.014943 0.012540 0.015550 0.015340 0.013203 0.026044 0.017639 0.019265 0.012746 0.017673 0.002185 
I0428 19:26:11.831760 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:26:13.048830  7412 solver.cpp:224] Iteration 24400 (4.05585 iter/s, 24.6557s/100 iters), loss = 0.396998
I0428 19:26:13.048830  7412 solver.cpp:243]     Train net output #0: loss = 0.396998 (* 1 = 0.396998 loss)
I0428 19:26:13.048830  7412 sgd_solver.cpp:137] Iteration 24400, lr = 0.01
I0428 19:26:13.057831  7412 sgd_solver.cpp:169] scale layer:0.449685 0.471826 0.528948 0.554942 0.519217 0.486025 0.494836 0.488894 0.487254 0.491602 0.500824 0.484682 0.464127 0.445668 0.434470 0.405294 0.379167 0.358205 0.366918 1.015039 
I0428 19:26:13.059830  7412 sgd_solver.cpp:200] weight diff/data:0.014506 0.013941 0.014767 0.009196 0.014858 0.022790 0.014684 0.016807 0.029037 0.013698 0.016210 0.012208 0.014354 0.016240 0.011476 0.017862 0.015558 0.020718 0.013011 0.024567 0.001314 
I0428 19:26:37.395222  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_24500.caffemodel
I0428 19:26:37.403223  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_24500.solverstate
I0428 19:26:37.406224  7412 solver.cpp:336] Iteration 24500, Testing net (#0)
I0428 19:26:46.160723 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:26:46.519744  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7997
I0428 19:26:46.519744  7412 solver.cpp:403]     Test net output #1: loss = 0.655214 (* 1 = 0.655214 loss)
I0428 19:26:46.752758  7412 solver.cpp:224] Iteration 24500 (2.96698 iter/s, 33.7043s/100 iters), loss = 0.230159
I0428 19:26:46.752758  7412 solver.cpp:243]     Train net output #0: loss = 0.230159 (* 1 = 0.230159 loss)
I0428 19:26:46.752758  7412 sgd_solver.cpp:137] Iteration 24500, lr = 0.01
I0428 19:26:46.760758  7412 sgd_solver.cpp:169] scale layer:0.451431 0.471422 0.525883 0.554920 0.516965 0.485988 0.493948 0.489237 0.488833 0.492198 0.500119 0.485246 0.464524 0.445495 0.434359 0.404497 0.378679 0.357922 0.366503 1.013809 
I0428 19:26:46.762758  7412 sgd_solver.cpp:200] weight diff/data:0.007891 0.011100 0.010931 0.018773 0.010506 0.037271 0.013094 0.014903 0.014624 0.013258 0.014968 0.012243 0.013051 0.011835 0.011643 0.011463 0.016608 0.012590 0.021209 0.013827 0.001990 
I0428 19:27:11.366165  7412 solver.cpp:224] Iteration 24600 (4.06267 iter/s, 24.6143s/100 iters), loss = 0.35672
I0428 19:27:11.367166  7412 solver.cpp:243]     Train net output #0: loss = 0.35672 (* 1 = 0.35672 loss)
I0428 19:27:11.367166  7412 sgd_solver.cpp:137] Iteration 24600, lr = 0.01
I0428 19:27:11.375166  7412 sgd_solver.cpp:169] scale layer:0.447837 0.474858 0.527162 0.555476 0.513692 0.486601 0.494765 0.489400 0.488475 0.491829 0.499067 0.484974 0.464316 0.445805 0.433881 0.404376 0.378185 0.356326 0.365671 1.014006 
I0428 19:27:11.377166  7412 sgd_solver.cpp:200] weight diff/data:0.005746 0.014697 0.011515 0.021671 0.018537 0.022426 0.014289 0.012435 0.023916 0.013470 0.013781 0.017943 0.058447 0.013027 0.014283 0.029471 0.014988 0.010437 0.012676 0.014787 0.006420 
I0428 19:27:36.047577  7412 solver.cpp:224] Iteration 24700 (4.05165 iter/s, 24.6813s/100 iters), loss = 0.241657
I0428 19:27:36.047577  7412 solver.cpp:243]     Train net output #0: loss = 0.241657 (* 1 = 0.241657 loss)
I0428 19:27:36.047577  7412 sgd_solver.cpp:137] Iteration 24700, lr = 0.01
I0428 19:27:36.056577  7412 sgd_solver.cpp:169] scale layer:0.443250 0.472959 0.524847 0.554580 0.511632 0.486016 0.495593 0.489036 0.488813 0.491844 0.499060 0.484089 0.464207 0.445020 0.433039 0.403397 0.377542 0.354550 0.364632 1.016622 
I0428 19:27:36.058578  7412 sgd_solver.cpp:200] weight diff/data:0.021096 0.017003 0.009547 0.012662 0.049519 0.012414 0.041796 0.014361 0.021579 0.011913 0.023975 0.013063 0.015675 0.011809 0.021969 0.011684 0.035438 0.123399 0.013114 0.014859 0.003287 
I0428 19:27:59.556921 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:00.780992  7412 solver.cpp:224] Iteration 24800 (4.04313 iter/s, 24.7333s/100 iters), loss = 0.386647
I0428 19:28:00.780992  7412 solver.cpp:243]     Train net output #0: loss = 0.386647 (* 1 = 0.386647 loss)
I0428 19:28:00.780992  7412 sgd_solver.cpp:137] Iteration 24800, lr = 0.01
I0428 19:28:00.789993  7412 sgd_solver.cpp:169] scale layer:0.447017 0.473211 0.525252 0.555321 0.511119 0.485941 0.495415 0.488664 0.488525 0.491714 0.498292 0.484307 0.463720 0.444760 0.432345 0.403044 0.376269 0.354424 0.364021 1.016672 
I0428 19:28:00.790992  7412 sgd_solver.cpp:200] weight diff/data:0.007505 0.022350 0.051249 0.009924 0.009771 0.036849 0.018963 0.019158 0.015441 0.074511 0.016152 0.015884 0.017808 0.015734 0.015545 0.047567 0.018603 0.039699 0.011105 0.013710 0.001104 
I0428 19:28:25.495405  7412 solver.cpp:224] Iteration 24900 (4.0462 iter/s, 24.7145s/100 iters), loss = 0.313219
I0428 19:28:25.495405  7412 solver.cpp:243]     Train net output #0: loss = 0.313219 (* 1 = 0.313219 loss)
I0428 19:28:25.495405  7412 sgd_solver.cpp:137] Iteration 24900, lr = 0.01
I0428 19:28:25.503407  7412 sgd_solver.cpp:169] scale layer:0.449428 0.471720 0.525481 0.551510 0.508220 0.485250 0.495803 0.488053 0.488562 0.491445 0.498374 0.484202 0.462334 0.444163 0.433192 0.403348 0.376367 0.354179 0.363605 1.015758 
I0428 19:28:25.505406  7412 sgd_solver.cpp:200] weight diff/data:0.013887 0.013165 0.017909 0.011739 0.019401 0.013267 0.019718 0.012818 0.025388 0.032690 0.013907 0.011628 0.023472 0.010879 0.016122 0.011448 0.017522 0.014393 0.010539 0.017160 0.001133 
I0428 19:28:49.984807  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_25000.caffemodel
I0428 19:28:49.993806  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_25000.solverstate
I0428 19:28:49.996806  7412 solver.cpp:336] Iteration 25000, Testing net (#0)
I0428 19:28:58.791309 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:59.154330  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8087
I0428 19:28:59.154330  7412 solver.cpp:403]     Test net output #1: loss = 0.636379 (* 1 = 0.636379 loss)
I0428 19:28:59.389344  7412 solver.cpp:224] Iteration 25000 (2.95029 iter/s, 33.895s/100 iters), loss = 0.317174
I0428 19:28:59.389344  7412 solver.cpp:243]     Train net output #0: loss = 0.317174 (* 1 = 0.317174 loss)
I0428 19:28:59.389344  7412 sgd_solver.cpp:137] Iteration 25000, lr = 0.01
I0428 19:28:59.398344  7412 sgd_solver.cpp:169] scale layer:0.452053 0.473550 0.526416 0.550753 0.510164 0.485609 0.495017 0.487476 0.488342 0.491322 0.498809 0.483991 0.463444 0.444066 0.432425 0.402441 0.375766 0.353162 0.362760 1.017407 
I0428 19:28:59.399344  7412 sgd_solver.cpp:200] weight diff/data:0.007924 0.015480 0.012901 0.020005 0.011864 0.015903 0.016220 0.017237 0.011487 0.012780 0.014780 0.017411 0.016411 0.015445 0.020251 0.022134 0.028057 0.015570 0.012882 0.254116 0.001928 
I0428 19:29:24.098757  7412 solver.cpp:224] Iteration 25100 (4.04702 iter/s, 24.7095s/100 iters), loss = 0.197641
I0428 19:29:24.098757  7412 solver.cpp:243]     Train net output #0: loss = 0.19764 (* 1 = 0.19764 loss)
I0428 19:29:24.098757  7412 sgd_solver.cpp:137] Iteration 25100, lr = 0.01
I0428 19:29:24.108758  7412 sgd_solver.cpp:169] scale layer:0.447557 0.476652 0.525846 0.551486 0.514263 0.484276 0.494431 0.487124 0.488414 0.491448 0.498109 0.482944 0.461623 0.443476 0.431959 0.402601 0.375252 0.352217 0.361676 1.018792 
I0428 19:29:24.110759  7412 sgd_solver.cpp:200] weight diff/data:0.007432 0.018181 0.019780 0.321289 0.010956 0.012190 0.013707 0.023997 0.011986 0.027975 0.012172 0.027392 0.020357 0.018455 0.021411 0.011914 0.013896 0.017829 0.017316 0.012094 0.003126 
I0428 19:29:47.577100 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:29:48.804170  7412 solver.cpp:224] Iteration 25200 (4.04762 iter/s, 24.7059s/100 iters), loss = 0.34119
I0428 19:29:48.805171  7412 solver.cpp:243]     Train net output #0: loss = 0.34119 (* 1 = 0.34119 loss)
I0428 19:29:48.805171  7412 sgd_solver.cpp:137] Iteration 25200, lr = 0.01
I0428 19:29:48.813171  7412 sgd_solver.cpp:169] scale layer:0.446416 0.477191 0.523910 0.550019 0.515856 0.484245 0.494625 0.486817 0.488833 0.491638 0.498023 0.483535 0.462575 0.443537 0.431515 0.401925 0.374649 0.351269 0.360692 1.016178 
I0428 19:29:48.815171  7412 sgd_solver.cpp:200] weight diff/data:0.010986 0.043440 0.016281 0.016734 0.016076 0.019486 0.016483 0.032953 0.014624 0.703237 0.013343 0.014350 0.015527 0.015842 0.014899 0.023438 0.010908 0.015731 0.010837 0.012953 0.001511 
I0428 19:30:13.480581  7412 solver.cpp:224] Iteration 25300 (4.05243 iter/s, 24.6765s/100 iters), loss = 0.316022
I0428 19:30:13.481582  7412 solver.cpp:243]     Train net output #0: loss = 0.316022 (* 1 = 0.316022 loss)
I0428 19:30:13.481582  7412 sgd_solver.cpp:137] Iteration 25300, lr = 0.01
I0428 19:30:13.489583  7412 sgd_solver.cpp:169] scale layer:0.445997 0.475000 0.526185 0.551293 0.514505 0.484380 0.493225 0.487243 0.487596 0.491540 0.498376 0.482330 0.463178 0.443502 0.431515 0.401066 0.373835 0.351002 0.360028 1.016143 
I0428 19:30:13.491582  7412 sgd_solver.cpp:200] weight diff/data:0.012491 0.037510 0.012956 0.013058 0.016721 0.019901 0.127990 0.020931 0.011310 0.011321 0.010871 0.015264 0.025549 0.016883 0.016398 0.014725 0.014723 0.009892 0.014514 0.025286 0.001191 
I0428 19:30:38.070988  7412 solver.cpp:224] Iteration 25400 (4.06664 iter/s, 24.5903s/100 iters), loss = 0.393747
I0428 19:30:38.070988  7412 solver.cpp:243]     Train net output #0: loss = 0.393746 (* 1 = 0.393746 loss)
I0428 19:30:38.070988  7412 sgd_solver.cpp:137] Iteration 25400, lr = 0.01
I0428 19:30:38.079988  7412 sgd_solver.cpp:169] scale layer:0.443317 0.475122 0.526262 0.551667 0.515109 0.486113 0.494109 0.488187 0.487740 0.491413 0.497819 0.482238 0.463743 0.443456 0.431201 0.401687 0.374207 0.350842 0.359515 1.015591 
I0428 19:30:38.081990  7412 sgd_solver.cpp:200] weight diff/data:0.008068 0.041486 0.018688 0.014074 0.010725 0.016531 0.015674 0.020599 0.013927 0.015041 0.018308 0.015473 0.012566 0.017466 0.013321 0.026410 0.013131 0.019981 0.159175 0.011137 0.001108 
I0428 19:31:02.536387  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_25500.caffemodel
I0428 19:31:02.544389  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_25500.solverstate
I0428 19:31:02.547389  7412 solver.cpp:336] Iteration 25500, Testing net (#0)
I0428 19:31:11.337891 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:11.702913  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8003
I0428 19:31:11.702913  7412 solver.cpp:403]     Test net output #1: loss = 0.655128 (* 1 = 0.655128 loss)
I0428 19:31:11.939925  7412 solver.cpp:224] Iteration 25500 (2.95256 iter/s, 33.8689s/100 iters), loss = 0.207804
I0428 19:31:11.939925  7412 solver.cpp:243]     Train net output #0: loss = 0.207804 (* 1 = 0.207804 loss)
I0428 19:31:11.939925  7412 sgd_solver.cpp:137] Iteration 25500, lr = 0.01
I0428 19:31:11.948926  7412 sgd_solver.cpp:169] scale layer:0.443100 0.472308 0.526651 0.553625 0.519127 0.486445 0.494621 0.488116 0.487030 0.491050 0.497293 0.482842 0.462972 0.443286 0.430789 0.401172 0.373910 0.350457 0.358940 1.015421 
I0428 19:31:11.949926  7412 sgd_solver.cpp:200] weight diff/data:0.016401 0.011985 0.015253 0.018881 0.011416 0.016753 0.012006 0.015410 0.013321 0.012934 0.019152 0.010680 0.017240 0.013062 0.010738 0.025826 0.018319 0.019139 0.013912 0.013811 0.002115 
I0428 19:31:35.433269 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:36.655339  7412 solver.cpp:224] Iteration 25600 (4.04601 iter/s, 24.7157s/100 iters), loss = 0.340204
I0428 19:31:36.655339  7412 solver.cpp:243]     Train net output #0: loss = 0.340203 (* 1 = 0.340203 loss)
I0428 19:31:36.655339  7412 sgd_solver.cpp:137] Iteration 25600, lr = 0.01
I0428 19:31:36.664340  7412 sgd_solver.cpp:169] scale layer:0.446714 0.471229 0.526396 0.550128 0.521348 0.486031 0.493369 0.487516 0.486690 0.490786 0.497904 0.483366 0.462410 0.442166 0.430345 0.400884 0.373757 0.349648 0.358069 1.014844 
I0428 19:31:36.666339  7412 sgd_solver.cpp:200] weight diff/data:0.012380 0.020875 0.012390 0.014789 0.022008 0.016538 0.018368 0.015576 0.030704 0.015120 0.015295 0.013014 0.017401 0.013829 0.014322 0.014300 0.016326 0.023100 0.018676 0.008615 0.001257 
I0428 19:32:01.352751  7412 solver.cpp:224] Iteration 25700 (4.04889 iter/s, 24.6981s/100 iters), loss = 0.241144
I0428 19:32:01.353751  7412 solver.cpp:243]     Train net output #0: loss = 0.241144 (* 1 = 0.241144 loss)
I0428 19:32:01.353751  7412 sgd_solver.cpp:137] Iteration 25700, lr = 0.01
I0428 19:32:01.362752  7412 sgd_solver.cpp:169] scale layer:0.443847 0.473860 0.525435 0.552225 0.518917 0.485917 0.492832 0.487572 0.487213 0.491381 0.497797 0.484542 0.461667 0.442049 0.429930 0.400404 0.373193 0.349030 0.357437 1.015167 
I0428 19:32:01.364753  7412 sgd_solver.cpp:200] weight diff/data:0.011439 0.015321 0.011817 0.033110 0.013499 0.011670 0.017266 0.012449 0.020122 0.013265 0.083294 0.012239 0.025194 0.036862 0.051428 0.019618 0.018848 0.018054 0.020954 0.008659 0.001108 
I0428 19:32:26.061166  7412 solver.cpp:224] Iteration 25800 (4.04726 iter/s, 24.7081s/100 iters), loss = 0.345045
I0428 19:32:26.061166  7412 solver.cpp:243]     Train net output #0: loss = 0.345045 (* 1 = 0.345045 loss)
I0428 19:32:26.061166  7412 sgd_solver.cpp:137] Iteration 25800, lr = 0.01
I0428 19:32:26.070165  7412 sgd_solver.cpp:169] scale layer:0.446825 0.474107 0.525775 0.552406 0.520176 0.485302 0.492945 0.486975 0.487757 0.491108 0.497963 0.484680 0.463119 0.442243 0.430921 0.400803 0.373022 0.349230 0.356829 1.014485 
I0428 19:32:26.073165  7412 sgd_solver.cpp:200] weight diff/data:0.022580 0.019078 0.016467 0.012606 0.010768 0.012160 0.012608 0.013314 0.025318 0.013852 0.013116 0.025052 0.016396 0.046905 0.017014 0.033706 0.024527 0.012056 0.015843 0.011168 0.000899 
I0428 19:32:50.786579  7412 solver.cpp:224] Iteration 25900 (4.04445 iter/s, 24.7253s/100 iters), loss = 0.244809
I0428 19:32:50.786579  7412 solver.cpp:243]     Train net output #0: loss = 0.244808 (* 1 = 0.244808 loss)
I0428 19:32:50.786579  7412 sgd_solver.cpp:137] Iteration 25900, lr = 0.01
I0428 19:32:50.795579  7412 sgd_solver.cpp:169] scale layer:0.450725 0.472152 0.527185 0.552209 0.520907 0.485540 0.492972 0.486182 0.488063 0.491307 0.496778 0.484404 0.463034 0.441607 0.430207 0.400362 0.371837 0.348147 0.355776 1.015609 
I0428 19:32:50.797580  7412 sgd_solver.cpp:200] weight diff/data:0.010255 0.013865 0.008909 0.011010 0.011336 0.033822 0.011585 0.011073 0.014178 0.012515 0.012199 0.010529 0.013173 0.018504 0.018202 0.018627 0.019678 0.010771 0.012783 0.015177 0.001596 
I0428 19:33:14.290923 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:33:15.281980  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_26000.caffemodel
I0428 19:33:15.289980  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_26000.solverstate
I0428 19:33:15.292981  7412 solver.cpp:336] Iteration 26000, Testing net (#0)
I0428 19:33:24.039481 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:33:24.404502  7412 solver.cpp:403]     Test net output #0: accuracy = 0.799
I0428 19:33:24.404502  7412 solver.cpp:403]     Test net output #1: loss = 0.670978 (* 1 = 0.670978 loss)
I0428 19:33:24.636515  7412 solver.cpp:224] Iteration 26000 (2.95411 iter/s, 33.8511s/100 iters), loss = 0.47499
I0428 19:33:24.637516  7412 solver.cpp:243]     Train net output #0: loss = 0.474989 (* 1 = 0.474989 loss)
I0428 19:33:24.637516  7412 sgd_solver.cpp:137] Iteration 26000, lr = 0.01
I0428 19:33:24.645515  7412 sgd_solver.cpp:169] scale layer:0.448875 0.473696 0.525047 0.552768 0.518236 0.485953 0.492390 0.485817 0.487922 0.491135 0.496805 0.484405 0.462367 0.441213 0.429756 0.399998 0.371774 0.347594 0.355229 1.014362 
I0428 19:33:24.647516  7412 sgd_solver.cpp:200] weight diff/data:0.014862 0.012620 0.013739 0.022208 0.010222 0.017735 0.021269 0.027195 0.018400 0.012601 0.014179 0.017232 0.018235 0.018546 0.016658 0.013936 0.014444 0.012363 0.015606 0.024169 0.002175 
I0428 19:33:49.330927  7412 solver.cpp:224] Iteration 26100 (4.04961 iter/s, 24.6937s/100 iters), loss = 0.215781
I0428 19:33:49.330927  7412 solver.cpp:243]     Train net output #0: loss = 0.215781 (* 1 = 0.215781 loss)
I0428 19:33:49.330927  7412 sgd_solver.cpp:137] Iteration 26100, lr = 0.01
I0428 19:33:49.339928  7412 sgd_solver.cpp:169] scale layer:0.446054 0.475824 0.526618 0.553383 0.519960 0.485669 0.493242 0.486302 0.486099 0.491667 0.498375 0.483476 0.462038 0.440753 0.429485 0.399887 0.370229 0.347235 0.354695 1.013629 
I0428 19:33:49.341928  7412 sgd_solver.cpp:200] weight diff/data:0.007624 0.010752 0.062377 0.013009 0.056480 0.013292 0.097479 0.020222 0.015587 0.013476 0.014617 0.012360 0.012862 0.014484 0.034769 0.011515 0.010233 0.051527 0.012696 0.010667 0.001013 
I0428 19:34:14.072343  7412 solver.cpp:224] Iteration 26200 (4.0417 iter/s, 24.7421s/100 iters), loss = 0.310298
I0428 19:34:14.072343  7412 solver.cpp:243]     Train net output #0: loss = 0.310298 (* 1 = 0.310298 loss)
I0428 19:34:14.072343  7412 sgd_solver.cpp:137] Iteration 26200, lr = 0.01
I0428 19:34:14.081343  7412 sgd_solver.cpp:169] scale layer:0.448504 0.472563 0.525742 0.552738 0.522926 0.485431 0.493243 0.487057 0.487478 0.490556 0.497661 0.483384 0.462553 0.440546 0.427984 0.399319 0.370113 0.346875 0.354512 1.013966 
I0428 19:34:14.083343  7412 sgd_solver.cpp:200] weight diff/data:0.004359 0.010857 0.015391 0.021809 0.011133 0.031466 0.018399 0.024509 0.015458 0.038298 0.092491 0.014863 0.018879 0.043401 0.015023 0.017356 0.018723 0.013322 0.008284 0.011283 0.001413 
I0428 19:34:38.790756  7412 solver.cpp:224] Iteration 26300 (4.04555 iter/s, 24.7185s/100 iters), loss = 0.22968
I0428 19:34:38.790756  7412 solver.cpp:243]     Train net output #0: loss = 0.22968 (* 1 = 0.22968 loss)
I0428 19:34:38.790756  7412 sgd_solver.cpp:137] Iteration 26300, lr = 0.01
I0428 19:34:38.798758  7412 sgd_solver.cpp:169] scale layer:0.446640 0.468436 0.527082 0.554075 0.520752 0.484187 0.495516 0.488868 0.488756 0.490739 0.497712 0.482798 0.463005 0.441244 0.428232 0.398541 0.369700 0.345623 0.353852 1.015494 
I0428 19:34:38.800757  7412 sgd_solver.cpp:200] weight diff/data:0.014837 0.010448 0.012636 0.041138 0.011473 0.010189 0.023937 0.017015 0.009848 0.021441 0.027798 0.013541 0.020364 0.017420 0.014611 0.016993 0.019365 0.013148 0.012889 0.010846 0.002556 
I0428 19:35:02.287101 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:35:03.509171  7412 solver.cpp:224] Iteration 26400 (4.0455 iter/s, 24.7188s/100 iters), loss = 0.288541
I0428 19:35:03.509171  7412 solver.cpp:243]     Train net output #0: loss = 0.288541 (* 1 = 0.288541 loss)
I0428 19:35:03.509171  7412 sgd_solver.cpp:137] Iteration 26400, lr = 0.01
I0428 19:35:03.519171  7412 sgd_solver.cpp:169] scale layer:0.439841 0.471618 0.527420 0.552353 0.521721 0.483843 0.494481 0.488507 0.486635 0.491030 0.495926 0.483305 0.462816 0.441199 0.428315 0.397390 0.370284 0.345445 0.353460 1.015291 
I0428 19:35:03.521172  7412 sgd_solver.cpp:200] weight diff/data:0.009454 0.060527 0.026601 0.024381 0.050158 0.145884 0.012188 0.018086 0.015462 0.015943 0.015012 0.013780 0.021791 0.016555 0.011628 0.015257 0.012454 0.017383 0.023126 0.022418 0.002593 
I0428 19:35:27.956568  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_26500.caffemodel
I0428 19:35:27.964570  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_26500.solverstate
I0428 19:35:27.967569  7412 solver.cpp:336] Iteration 26500, Testing net (#0)
I0428 19:35:36.744071 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:35:37.106092  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8015
I0428 19:35:37.106092  7412 solver.cpp:403]     Test net output #1: loss = 0.663751 (* 1 = 0.663751 loss)
I0428 19:35:37.342105  7412 solver.cpp:224] Iteration 26500 (2.9557 iter/s, 33.8329s/100 iters), loss = 0.351139
I0428 19:35:37.342105  7412 solver.cpp:243]     Train net output #0: loss = 0.351139 (* 1 = 0.351139 loss)
I0428 19:35:37.342105  7412 sgd_solver.cpp:137] Iteration 26500, lr = 0.01
I0428 19:35:37.350106  7412 sgd_solver.cpp:169] scale layer:0.440845 0.471721 0.528525 0.551777 0.521814 0.484033 0.493851 0.487559 0.486733 0.491312 0.496783 0.483925 0.462206 0.441365 0.427456 0.397478 0.369765 0.344754 0.352712 1.016247 
I0428 19:35:37.352107  7412 sgd_solver.cpp:200] weight diff/data:0.008121 0.016149 0.013931 0.024327 0.012182 0.017095 0.021682 0.018216 0.014103 0.013665 0.031190 0.024703 0.015498 0.014620 0.025393 0.020680 0.019712 0.013856 0.017817 0.010849 0.001290 
I0428 19:36:01.997515  7412 solver.cpp:224] Iteration 26600 (4.05589 iter/s, 24.6555s/100 iters), loss = 0.305343
I0428 19:36:01.997515  7412 solver.cpp:243]     Train net output #0: loss = 0.305342 (* 1 = 0.305342 loss)
I0428 19:36:01.997515  7412 sgd_solver.cpp:137] Iteration 26600, lr = 0.01
I0428 19:36:02.006516  7412 sgd_solver.cpp:169] scale layer:0.447128 0.473252 0.525900 0.553532 0.520692 0.485098 0.494116 0.489172 0.487339 0.490633 0.496678 0.483585 0.462994 0.441132 0.427267 0.397902 0.369405 0.344441 0.352036 1.013521 
I0428 19:36:02.008517  7412 sgd_solver.cpp:200] weight diff/data:0.195139 0.008807 0.008954 0.013451 0.012456 0.017839 0.267336 0.017635 0.016039 0.011602 0.016795 0.014174 0.012375 0.012381 0.015461 0.011311 0.020878 0.008436 0.008870 0.008840 0.001420 
I0428 19:36:26.629925  7412 solver.cpp:224] Iteration 26700 (4.05964 iter/s, 24.6327s/100 iters), loss = 0.248884
I0428 19:36:26.629925  7412 solver.cpp:243]     Train net output #0: loss = 0.248884 (* 1 = 0.248884 loss)
I0428 19:36:26.629925  7412 sgd_solver.cpp:137] Iteration 26700, lr = 0.01
I0428 19:36:26.638926  7412 sgd_solver.cpp:169] scale layer:0.447655 0.471451 0.526575 0.553077 0.522337 0.484325 0.494432 0.487834 0.488080 0.489587 0.496695 0.481883 0.462766 0.441208 0.427303 0.397736 0.367918 0.344324 0.351513 1.012134 
I0428 19:36:26.640925  7412 sgd_solver.cpp:200] weight diff/data:0.011493 0.013031 0.009511 0.015577 0.013559 0.013515 0.015544 0.013477 0.015294 0.035624 0.013325 0.013232 0.017924 0.012035 0.012994 0.013787 0.012897 0.013861 0.009513 0.022087 0.001696 
I0428 19:36:49.994261 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:36:51.208330  7412 solver.cpp:224] Iteration 26800 (4.06857 iter/s, 24.5786s/100 iters), loss = 0.245802
I0428 19:36:51.208330  7412 solver.cpp:243]     Train net output #0: loss = 0.245802 (* 1 = 0.245802 loss)
I0428 19:36:51.208330  7412 sgd_solver.cpp:137] Iteration 26800, lr = 0.01
I0428 19:36:51.218331  7412 sgd_solver.cpp:169] scale layer:0.442768 0.471768 0.526660 0.553284 0.521534 0.485149 0.494441 0.486746 0.487406 0.490442 0.497528 0.482445 0.462869 0.440885 0.426934 0.397608 0.367603 0.343085 0.351049 1.010564 
I0428 19:36:51.220331  7412 sgd_solver.cpp:200] weight diff/data:0.008000 0.011515 0.010649 0.059835 0.015810 0.034419 0.015244 0.012419 0.013145 0.018430 0.020017 0.019338 0.030451 0.013855 0.013660 0.014607 0.020075 0.015340 0.014605 0.021131 0.002476 
I0428 19:37:15.826738  7412 solver.cpp:224] Iteration 26900 (4.06196 iter/s, 24.6187s/100 iters), loss = 0.249121
I0428 19:37:15.827739  7412 solver.cpp:243]     Train net output #0: loss = 0.249121 (* 1 = 0.249121 loss)
I0428 19:37:15.827739  7412 sgd_solver.cpp:137] Iteration 26900, lr = 0.01
I0428 19:37:15.835739  7412 sgd_solver.cpp:169] scale layer:0.440801 0.470782 0.528365 0.553947 0.519124 0.485214 0.494938 0.487488 0.487400 0.490906 0.497256 0.483081 0.461930 0.441124 0.426916 0.397586 0.367610 0.342039 0.350613 1.012078 
I0428 19:37:15.837739  7412 sgd_solver.cpp:200] weight diff/data:0.020663 0.011685 0.016975 0.010159 0.014086 0.017287 0.015227 0.022180 0.022236 0.024338 0.016552 0.014585 0.022008 0.017953 0.013788 0.015460 0.011711 0.032267 0.010377 0.010705 0.000989 
I0428 19:37:40.202132  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_27000.caffemodel
I0428 19:37:40.210134  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_27000.solverstate
I0428 19:37:40.213133  7412 solver.cpp:336] Iteration 27000, Testing net (#0)
I0428 19:37:48.954633 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:37:49.319654  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8055
I0428 19:37:49.319654  7412 solver.cpp:403]     Test net output #1: loss = 0.662971 (* 1 = 0.662971 loss)
I0428 19:37:49.553668  7412 solver.cpp:224] Iteration 27000 (2.96501 iter/s, 33.7267s/100 iters), loss = 0.279408
I0428 19:37:49.553668  7412 solver.cpp:243]     Train net output #0: loss = 0.279408 (* 1 = 0.279408 loss)
I0428 19:37:49.553668  7412 sgd_solver.cpp:137] Iteration 27000, lr = 0.01
I0428 19:37:49.561668  7412 sgd_solver.cpp:169] scale layer:0.446880 0.466178 0.529194 0.551963 0.516232 0.485408 0.494095 0.487885 0.487516 0.491541 0.496880 0.483530 0.462598 0.441130 0.427459 0.397987 0.368260 0.341605 0.349734 1.012139 
I0428 19:37:49.563668  7412 sgd_solver.cpp:200] weight diff/data:0.007324 0.022108 0.007698 0.011302 0.054578 0.019974 0.011519 0.015131 0.030326 0.019526 0.014492 0.029011 0.052955 0.014115 0.017412 0.010974 0.015831 0.012062 0.016093 0.011856 0.002429 
I0428 19:38:14.201077  7412 solver.cpp:224] Iteration 27100 (4.05718 iter/s, 24.6477s/100 iters), loss = 0.198463
I0428 19:38:14.201077  7412 solver.cpp:243]     Train net output #0: loss = 0.198462 (* 1 = 0.198462 loss)
I0428 19:38:14.201077  7412 sgd_solver.cpp:137] Iteration 27100, lr = 0.01
I0428 19:38:14.209079  7412 sgd_solver.cpp:169] scale layer:0.444832 0.468880 0.527911 0.553791 0.514823 0.484466 0.494416 0.486742 0.487385 0.491426 0.496764 0.483128 0.462082 0.440507 0.427393 0.397581 0.368541 0.340494 0.349188 1.014347 
I0428 19:38:14.211078  7412 sgd_solver.cpp:200] weight diff/data:0.009997 0.021392 0.012106 0.016987 0.012832 0.038278 0.030145 0.014038 0.029382 0.035972 0.015577 0.016365 0.027266 0.015398 0.014153 0.015789 0.150300 0.020757 0.013758 0.015966 0.003925 
I0428 19:38:37.592416 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:38:38.812485  7412 solver.cpp:224] Iteration 27200 (4.063 iter/s, 24.6123s/100 iters), loss = 0.350213
I0428 19:38:38.813485  7412 solver.cpp:243]     Train net output #0: loss = 0.350212 (* 1 = 0.350212 loss)
I0428 19:38:38.813485  7412 sgd_solver.cpp:137] Iteration 27200, lr = 0.01
I0428 19:38:38.821485  7412 sgd_solver.cpp:169] scale layer:0.446165 0.469264 0.526633 0.553211 0.513593 0.484814 0.494812 0.487651 0.485144 0.490441 0.497791 0.482959 0.461313 0.439762 0.427422 0.397680 0.368079 0.340599 0.348250 1.013770 
I0428 19:38:38.823485  7412 sgd_solver.cpp:200] weight diff/data:0.012341 0.011460 0.052810 0.021474 0.011373 0.011370 0.014061 0.015388 0.021851 0.013193 0.016294 0.016806 0.014782 0.019573 0.014904 0.014012 0.014737 0.011737 0.015426 0.030185 0.001779 
I0428 19:39:03.441895  7412 solver.cpp:224] Iteration 27300 (4.06024 iter/s, 24.6291s/100 iters), loss = 0.183132
I0428 19:39:03.441895  7412 solver.cpp:243]     Train net output #0: loss = 0.183132 (* 1 = 0.183132 loss)
I0428 19:39:03.441895  7412 sgd_solver.cpp:137] Iteration 27300, lr = 0.01
I0428 19:39:03.450894  7412 sgd_solver.cpp:169] scale layer:0.449710 0.470002 0.529202 0.555174 0.513262 0.484870 0.494897 0.487766 0.486436 0.490473 0.498299 0.483443 0.461168 0.439352 0.427429 0.396436 0.367145 0.340392 0.347942 1.013025 
I0428 19:39:03.451894  7412 sgd_solver.cpp:200] weight diff/data:0.005571 0.010533 0.010867 0.011779 0.011371 0.012662 0.020600 0.016342 0.016319 0.013164 0.019409 0.015375 0.017523 0.010198 0.020154 0.013869 0.017475 0.010974 0.010931 0.015783 0.001177 
I0428 19:39:28.064302  7412 solver.cpp:224] Iteration 27400 (4.06128 iter/s, 24.6228s/100 iters), loss = 0.288112
I0428 19:39:28.064302  7412 solver.cpp:243]     Train net output #0: loss = 0.288111 (* 1 = 0.288111 loss)
I0428 19:39:28.064302  7412 sgd_solver.cpp:137] Iteration 27400, lr = 0.01
I0428 19:39:28.073302  7412 sgd_solver.cpp:169] scale layer:0.450602 0.472140 0.527836 0.554047 0.511751 0.484152 0.495076 0.490301 0.486942 0.489860 0.497695 0.483398 0.461815 0.439842 0.427464 0.396014 0.367319 0.340355 0.347018 1.012005 
I0428 19:39:28.076303  7412 sgd_solver.cpp:200] weight diff/data:0.006261 0.010513 0.010622 0.010810 0.012213 0.012362 0.028585 0.012082 0.017322 0.022593 0.010938 0.013530 0.018416 0.013494 0.014067 0.076169 0.025212 0.015337 0.011143 0.011606 0.001161 
I0428 19:39:52.413695  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_27500.caffemodel
I0428 19:39:52.421695  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_27500.solverstate
I0428 19:39:52.424695  7412 solver.cpp:336] Iteration 27500, Testing net (#0)
I0428 19:40:01.184196 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:01.547217  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8025
I0428 19:40:01.547217  7412 solver.cpp:403]     Test net output #1: loss = 0.646861 (* 1 = 0.646861 loss)
I0428 19:40:01.782230  7412 solver.cpp:224] Iteration 27500 (2.96573 iter/s, 33.7185s/100 iters), loss = 0.19185
I0428 19:40:01.782230  7412 solver.cpp:243]     Train net output #0: loss = 0.191849 (* 1 = 0.191849 loss)
I0428 19:40:01.782230  7412 sgd_solver.cpp:137] Iteration 27500, lr = 0.01
I0428 19:40:01.791231  7412 sgd_solver.cpp:169] scale layer:0.448374 0.470287 0.529873 0.555446 0.506014 0.483745 0.494677 0.490273 0.487868 0.490031 0.497209 0.483101 0.461772 0.439573 0.426810 0.396098 0.366138 0.339190 0.346498 1.013131 
I0428 19:40:01.793231  7412 sgd_solver.cpp:200] weight diff/data:0.012069 0.014663 0.010893 0.008826 0.009904 0.019824 0.019733 0.014415 0.017708 0.013123 0.014326 0.018164 0.020868 0.012490 0.040879 0.012947 0.011547 0.025878 0.009330 0.058805 0.003050 
I0428 19:40:25.135566 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:26.360636  7412 solver.cpp:224] Iteration 27600 (4.06856 iter/s, 24.5787s/100 iters), loss = 0.362067
I0428 19:40:26.360636  7412 solver.cpp:243]     Train net output #0: loss = 0.362067 (* 1 = 0.362067 loss)
I0428 19:40:26.360636  7412 sgd_solver.cpp:137] Iteration 27600, lr = 0.01
I0428 19:40:26.369637  7412 sgd_solver.cpp:169] scale layer:0.449841 0.468545 0.528194 0.554727 0.509751 0.484322 0.494104 0.490584 0.487213 0.489852 0.497719 0.483188 0.461328 0.438583 0.425405 0.396417 0.366129 0.339765 0.345958 1.011746 
I0428 19:40:26.371637  7412 sgd_solver.cpp:200] weight diff/data:0.011436 0.023104 0.011016 0.015850 0.010375 0.026025 0.031552 0.014257 0.028507 0.039374 0.011133 0.014855 0.011278 0.021594 0.026511 0.016445 0.012226 0.023474 0.011235 0.012219 0.001326 
I0428 19:40:51.164055  7412 solver.cpp:224] Iteration 27700 (4.03168 iter/s, 24.8035s/100 iters), loss = 0.212924
I0428 19:40:51.164055  7412 solver.cpp:243]     Train net output #0: loss = 0.212924 (* 1 = 0.212924 loss)
I0428 19:40:51.164055  7412 sgd_solver.cpp:137] Iteration 27700, lr = 0.01
I0428 19:40:51.172055  7412 sgd_solver.cpp:169] scale layer:0.451163 0.471224 0.530240 0.553550 0.511298 0.484460 0.494103 0.490441 0.488075 0.488859 0.496715 0.482892 0.461606 0.438059 0.425220 0.396416 0.365938 0.339107 0.345399 1.012564 
I0428 19:40:51.174055  7412 sgd_solver.cpp:200] weight diff/data:0.012366 0.038991 0.013588 0.015839 0.010492 0.014167 0.021046 0.015360 0.056349 0.017144 0.014624 0.020016 0.018458 0.012337 0.015551 0.015364 0.015374 0.073068 0.010185 0.016499 0.001854 
I0428 19:41:15.835466  7412 solver.cpp:224] Iteration 27800 (4.05315 iter/s, 24.6721s/100 iters), loss = 0.293905
I0428 19:41:15.835466  7412 solver.cpp:243]     Train net output #0: loss = 0.293904 (* 1 = 0.293904 loss)
I0428 19:41:15.835466  7412 sgd_solver.cpp:137] Iteration 27800, lr = 0.01
I0428 19:41:15.843466  7412 sgd_solver.cpp:169] scale layer:0.448712 0.471793 0.528930 0.554589 0.512116 0.484516 0.494108 0.491788 0.487806 0.488780 0.496985 0.482216 0.461589 0.437955 0.425128 0.396259 0.365898 0.338579 0.344771 1.013608 
I0428 19:41:15.845468  7412 sgd_solver.cpp:200] weight diff/data:0.007538 0.011165 0.014235 0.011451 0.012813 0.018138 0.014187 0.023171 0.013617 0.012496 0.044229 0.014066 0.016705 0.027020 0.021262 0.157935 0.018606 0.012230 0.009955 0.023026 0.001004 
I0428 19:41:40.525878  7412 solver.cpp:224] Iteration 27900 (4.05011 iter/s, 24.6907s/100 iters), loss = 0.284804
I0428 19:41:40.525878  7412 solver.cpp:243]     Train net output #0: loss = 0.284804 (* 1 = 0.284804 loss)
I0428 19:41:40.525878  7412 sgd_solver.cpp:137] Iteration 27900, lr = 0.01
I0428 19:41:40.534879  7412 sgd_solver.cpp:169] scale layer:0.445171 0.466953 0.528230 0.553289 0.507983 0.483909 0.493961 0.491317 0.486574 0.488887 0.498337 0.482421 0.461315 0.437767 0.425098 0.395404 0.366104 0.337967 0.343790 1.015763 
I0428 19:41:40.536880  7412 sgd_solver.cpp:200] weight diff/data:0.006405 0.015968 0.053009 0.011113 0.028844 0.016767 0.029741 0.018922 0.012514 0.015168 0.016111 0.014288 0.012457 0.038485 0.020991 0.029825 0.012129 0.013146 0.014364 0.130976 0.002938 
I0428 19:42:03.985220 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:42:04.960276  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_28000.caffemodel
I0428 19:42:04.969276  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_28000.solverstate
I0428 19:42:04.972276  7412 solver.cpp:336] Iteration 28000, Testing net (#0)
I0428 19:42:13.754779 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:42:14.116801  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8045
I0428 19:42:14.116801  7412 solver.cpp:403]     Test net output #1: loss = 0.668381 (* 1 = 0.668381 loss)
I0428 19:42:14.351814  7412 solver.cpp:224] Iteration 28000 (2.95621 iter/s, 33.8271s/100 iters), loss = 0.333898
I0428 19:42:14.351814  7412 solver.cpp:243]     Train net output #0: loss = 0.333898 (* 1 = 0.333898 loss)
I0428 19:42:14.351814  7412 sgd_solver.cpp:137] Iteration 28000, lr = 0.01
I0428 19:42:14.361814  7412 sgd_solver.cpp:169] scale layer:0.443695 0.469897 0.528058 0.554744 0.510595 0.484352 0.492531 0.491381 0.487549 0.489585 0.498211 0.482568 0.461247 0.436965 0.425432 0.394392 0.366664 0.337950 0.343662 1.015684 
I0428 19:42:14.362814  7412 sgd_solver.cpp:200] weight diff/data:0.022074 0.014691 0.045756 0.012222 0.011416 0.335986 0.018634 0.040242 0.016621 0.013469 0.024549 0.013962 0.026562 0.013093 0.040284 0.064833 0.018511 0.014852 0.012291 0.012198 0.001616 
I0428 19:42:39.064226  7412 solver.cpp:224] Iteration 28100 (4.04656 iter/s, 24.7124s/100 iters), loss = 0.222639
I0428 19:42:39.064226  7412 solver.cpp:243]     Train net output #0: loss = 0.222638 (* 1 = 0.222638 loss)
I0428 19:42:39.064226  7412 sgd_solver.cpp:137] Iteration 28100, lr = 0.01
I0428 19:42:39.072227  7412 sgd_solver.cpp:169] scale layer:0.442939 0.472731 0.526539 0.551790 0.512915 0.483507 0.492125 0.490414 0.487359 0.489054 0.498473 0.482387 0.460061 0.437336 0.425556 0.394265 0.366646 0.338049 0.343188 1.013797 
I0428 19:42:39.074228  7412 sgd_solver.cpp:200] weight diff/data:0.051051 0.010360 0.017409 0.011263 0.018941 0.011221 0.015377 0.016066 0.041733 0.013087 0.011067 0.020765 0.036519 0.022441 0.014045 0.017164 0.012755 0.028292 0.011203 0.011744 0.001142 
I0428 19:43:03.740638  7412 solver.cpp:224] Iteration 28200 (4.05227 iter/s, 24.6775s/100 iters), loss = 0.295934
I0428 19:43:03.740638  7412 solver.cpp:243]     Train net output #0: loss = 0.295934 (* 1 = 0.295934 loss)
I0428 19:43:03.740638  7412 sgd_solver.cpp:137] Iteration 28200, lr = 0.01
I0428 19:43:03.750638  7412 sgd_solver.cpp:169] scale layer:0.439203 0.469302 0.525793 0.553184 0.517701 0.482702 0.492100 0.490758 0.487741 0.489359 0.498589 0.482949 0.459753 0.436622 0.425084 0.394826 0.366933 0.337854 0.343096 1.010910 
I0428 19:43:03.752640  7412 sgd_solver.cpp:200] weight diff/data:0.009078 0.012733 0.015115 0.010241 0.019879 0.009654 0.014772 0.015408 0.027846 0.011425 0.012329 0.015904 0.016262 0.012740 0.022025 0.014133 0.014581 0.017795 0.009717 0.017846 0.000870 
I0428 19:43:28.391048  7412 solver.cpp:224] Iteration 28300 (4.05671 iter/s, 24.6505s/100 iters), loss = 0.20118
I0428 19:43:28.391048  7412 solver.cpp:243]     Train net output #0: loss = 0.20118 (* 1 = 0.20118 loss)
I0428 19:43:28.391048  7412 sgd_solver.cpp:137] Iteration 28300, lr = 0.01
I0428 19:43:28.399049  7412 sgd_solver.cpp:169] scale layer:0.443378 0.470155 0.527279 0.553946 0.514522 0.483487 0.493591 0.490971 0.487979 0.489054 0.498434 0.483066 0.460535 0.436486 0.425093 0.395554 0.366940 0.337773 0.342764 1.011493 
I0428 19:43:28.401048  7412 sgd_solver.cpp:200] weight diff/data:0.012000 0.013149 0.008313 0.011239 0.011958 0.026917 0.019644 0.018075 0.014679 0.012262 0.009501 0.010435 0.013026 0.008854 0.016120 0.009227 0.012468 0.009392 0.017381 0.013973 0.001698 
I0428 19:43:51.798388 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:53.019456  7412 solver.cpp:224] Iteration 28400 (4.06017 iter/s, 24.6295s/100 iters), loss = 0.312138
I0428 19:43:53.020457  7412 solver.cpp:243]     Train net output #0: loss = 0.312138 (* 1 = 0.312138 loss)
I0428 19:43:53.020457  7412 sgd_solver.cpp:137] Iteration 28400, lr = 0.01
I0428 19:43:53.029458  7412 sgd_solver.cpp:169] scale layer:0.443260 0.468792 0.526110 0.553797 0.512283 0.483834 0.493508 0.490657 0.487609 0.488838 0.499039 0.482654 0.460218 0.436807 0.425326 0.394554 0.366171 0.337343 0.342148 1.012851 
I0428 19:43:53.031457  7412 sgd_solver.cpp:200] weight diff/data:0.012207 0.009344 0.010724 0.013637 0.014632 0.045505 0.018083 0.013556 0.020468 0.020204 0.012595 0.017163 0.022616 0.018380 0.017625 0.020719 0.013735 0.016731 0.012331 0.016403 0.001678 
I0428 19:44:17.356848  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_28500.caffemodel
I0428 19:44:17.364850  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_28500.solverstate
I0428 19:44:17.367849  7412 solver.cpp:336] Iteration 28500, Testing net (#0)
I0428 19:44:26.121351 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:26.483371  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8025
I0428 19:44:26.483371  7412 solver.cpp:403]     Test net output #1: loss = 0.659167 (* 1 = 0.659167 loss)
I0428 19:44:26.717384  7412 solver.cpp:224] Iteration 28500 (2.96753 iter/s, 33.698s/100 iters), loss = 0.200227
I0428 19:44:26.717384  7412 solver.cpp:243]     Train net output #0: loss = 0.200227 (* 1 = 0.200227 loss)
I0428 19:44:26.717384  7412 sgd_solver.cpp:137] Iteration 28500, lr = 0.01
I0428 19:44:26.727385  7412 sgd_solver.cpp:169] scale layer:0.444859 0.466109 0.527943 0.553058 0.513170 0.485369 0.492687 0.490230 0.487630 0.489702 0.498729 0.483322 0.460559 0.438062 0.424782 0.395137 0.365890 0.336743 0.341706 1.011163 
I0428 19:44:26.728384  7412 sgd_solver.cpp:200] weight diff/data:0.007934 0.011444 0.011238 0.024241 0.020965 0.019054 0.018056 0.012984 0.013962 0.015207 0.013020 0.021385 0.016703 0.016453 0.258527 0.130750 0.018289 0.166316 0.012773 0.014533 0.001257 
I0428 19:44:51.389796  7412 solver.cpp:224] Iteration 28600 (4.05315 iter/s, 24.6721s/100 iters), loss = 0.216217
I0428 19:44:51.389796  7412 solver.cpp:243]     Train net output #0: loss = 0.216217 (* 1 = 0.216217 loss)
I0428 19:44:51.389796  7412 sgd_solver.cpp:137] Iteration 28600, lr = 0.01
I0428 19:44:51.398797  7412 sgd_solver.cpp:169] scale layer:0.447469 0.466316 0.529162 0.554191 0.514518 0.485547 0.493631 0.491266 0.487316 0.489616 0.499223 0.482625 0.460308 0.437615 0.425406 0.395479 0.365161 0.336788 0.341165 1.010866 
I0428 19:44:51.400796  7412 sgd_solver.cpp:200] weight diff/data:0.043221 0.023534 0.009830 0.010137 0.010865 0.021184 0.013668 0.024472 0.017618 0.011843 0.010146 0.016365 0.015516 0.015131 0.013067 0.013728 0.016169 0.011603 0.025661 0.012871 0.001189 
I0428 19:45:16.064206  7412 solver.cpp:224] Iteration 28700 (4.05275 iter/s, 24.6746s/100 iters), loss = 0.239866
I0428 19:45:16.064206  7412 solver.cpp:243]     Train net output #0: loss = 0.239866 (* 1 = 0.239866 loss)
I0428 19:45:16.064206  7412 sgd_solver.cpp:137] Iteration 28700, lr = 0.01
I0428 19:45:16.073207  7412 sgd_solver.cpp:169] scale layer:0.447359 0.467324 0.527865 0.554429 0.517051 0.484847 0.495502 0.490558 0.486831 0.488939 0.499255 0.483791 0.460971 0.437011 0.425229 0.394899 0.365197 0.335923 0.340559 1.012201 
I0428 19:45:16.075207  7412 sgd_solver.cpp:200] weight diff/data:0.009968 0.012201 0.012975 0.011767 0.010640 0.012291 0.015095 0.016239 0.012131 0.055115 0.011659 0.012854 0.016437 0.012601 0.023872 0.016466 0.017787 0.077956 0.012138 0.016018 0.001434 
I0428 19:45:39.517549 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:45:40.741618  7412 solver.cpp:224] Iteration 28800 (4.05222 iter/s, 24.6778s/100 iters), loss = 0.304988
I0428 19:45:40.741618  7412 solver.cpp:243]     Train net output #0: loss = 0.304988 (* 1 = 0.304988 loss)
I0428 19:45:40.742619  7412 sgd_solver.cpp:137] Iteration 28800, lr = 0.01
I0428 19:45:40.750618  7412 sgd_solver.cpp:169] scale layer:0.452316 0.465032 0.525489 0.554072 0.517689 0.484359 0.494245 0.490574 0.487382 0.488118 0.498843 0.483370 0.460583 0.438062 0.424969 0.394469 0.365382 0.335100 0.340139 1.011821 
I0428 19:45:40.752619  7412 sgd_solver.cpp:200] weight diff/data:0.013281 0.023456 0.010979 0.014026 0.022888 0.012410 0.012152 0.013041 0.014077 0.011583 0.015109 0.017421 0.070312 0.028856 0.015234 0.011338 0.021823 0.018627 0.015133 0.017136 0.001681 
I0428 19:46:05.424031  7412 solver.cpp:224] Iteration 28900 (4.05152 iter/s, 24.6821s/100 iters), loss = 0.201302
I0428 19:46:05.424031  7412 solver.cpp:243]     Train net output #0: loss = 0.201302 (* 1 = 0.201302 loss)
I0428 19:46:05.424031  7412 sgd_solver.cpp:137] Iteration 28900, lr = 0.01
I0428 19:46:05.433030  7412 sgd_solver.cpp:169] scale layer:0.452446 0.468393 0.525838 0.551208 0.520123 0.484267 0.495320 0.489982 0.487618 0.489628 0.499163 0.483310 0.460288 0.438482 0.424493 0.394639 0.365054 0.335937 0.339663 1.010228 
I0428 19:46:05.435030  7412 sgd_solver.cpp:200] weight diff/data:0.004236 0.011189 0.017287 0.015744 0.012758 0.013742 0.010643 0.026902 0.017639 0.013191 0.019417 0.056369 0.019207 0.014029 0.023703 0.010328 0.015074 0.015623 0.009874 0.009430 0.001375 
I0428 19:46:29.875428  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_29000.caffemodel
I0428 19:46:29.884429  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_29000.solverstate
I0428 19:46:29.887429  7412 solver.cpp:336] Iteration 29000, Testing net (#0)
I0428 19:46:38.664932 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:46:39.028952  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8041
I0428 19:46:39.029953  7412 solver.cpp:403]     Test net output #1: loss = 0.664116 (* 1 = 0.664116 loss)
I0428 19:46:39.262965  7412 solver.cpp:224] Iteration 29000 (2.95511 iter/s, 33.8396s/100 iters), loss = 0.346267
I0428 19:46:39.262965  7412 solver.cpp:243]     Train net output #0: loss = 0.346267 (* 1 = 0.346267 loss)
I0428 19:46:39.262965  7412 sgd_solver.cpp:137] Iteration 29000, lr = 0.01
I0428 19:46:39.271966  7412 sgd_solver.cpp:169] scale layer:0.447553 0.464850 0.525271 0.552724 0.519406 0.485403 0.495872 0.490834 0.487855 0.489885 0.499229 0.483217 0.460469 0.438628 0.425276 0.393997 0.364784 0.335371 0.339247 1.010601 
I0428 19:46:39.273967  7412 sgd_solver.cpp:200] weight diff/data:0.009918 0.016368 0.008730 0.022502 0.012213 0.030455 0.011603 0.023550 0.014899 0.016790 0.012703 0.012903 0.012791 0.015843 0.014888 0.015431 0.035216 0.011680 0.013059 0.013611 0.004020 
I0428 19:47:03.878373  7412 solver.cpp:224] Iteration 29100 (4.0624 iter/s, 24.616s/100 iters), loss = 0.15921
I0428 19:47:03.878373  7412 solver.cpp:243]     Train net output #0: loss = 0.15921 (* 1 = 0.15921 loss)
I0428 19:47:03.879374  7412 sgd_solver.cpp:137] Iteration 29100, lr = 0.01
I0428 19:47:03.887373  7412 sgd_solver.cpp:169] scale layer:0.447931 0.462659 0.525955 0.554025 0.520024 0.483847 0.495976 0.490949 0.487397 0.490179 0.498821 0.482904 0.461337 0.437851 0.425224 0.393090 0.364449 0.335011 0.338828 1.012172 
I0428 19:47:03.889374  7412 sgd_solver.cpp:200] weight diff/data:0.008808 0.012592 0.013453 0.015789 0.014323 0.009062 0.026695 0.015089 0.014970 0.009497 0.012819 0.144432 0.017587 0.022373 0.018331 0.010446 0.010697 0.013398 0.012901 0.163250 0.001957 
I0428 19:47:27.272711 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:47:28.495781  7412 solver.cpp:224] Iteration 29200 (4.06213 iter/s, 24.6176s/100 iters), loss = 0.39666
I0428 19:47:28.495781  7412 solver.cpp:243]     Train net output #0: loss = 0.39666 (* 1 = 0.39666 loss)
I0428 19:47:28.496781  7412 sgd_solver.cpp:137] Iteration 29200, lr = 0.01
I0428 19:47:28.504781  7412 sgd_solver.cpp:169] scale layer:0.446909 0.466758 0.525951 0.551248 0.521288 0.484260 0.494443 0.491231 0.486655 0.490225 0.500012 0.482674 0.460305 0.438343 0.425033 0.393602 0.363981 0.334214 0.338329 1.009514 
I0428 19:47:28.506783  7412 sgd_solver.cpp:200] weight diff/data:0.006236 0.011575 0.011223 0.018904 0.012841 0.011506 0.017340 0.025039 0.011784 0.017157 0.011985 0.012265 0.014388 0.010964 0.019889 0.009816 0.013979 0.012030 0.012893 0.011598 0.001225 
I0428 19:47:53.096189  7412 solver.cpp:224] Iteration 29300 (4.06492 iter/s, 24.6007s/100 iters), loss = 0.297699
I0428 19:47:53.097188  7412 solver.cpp:243]     Train net output #0: loss = 0.297698 (* 1 = 0.297698 loss)
I0428 19:47:53.097188  7412 sgd_solver.cpp:137] Iteration 29300, lr = 0.01
I0428 19:47:53.106189  7412 sgd_solver.cpp:169] scale layer:0.451028 0.466759 0.526117 0.555665 0.519895 0.484660 0.495929 0.490936 0.487199 0.491002 0.499711 0.483616 0.460409 0.437120 0.425629 0.392272 0.363655 0.334146 0.337925 1.009339 
I0428 19:47:53.108189  7412 sgd_solver.cpp:200] weight diff/data:0.008053 0.012852 0.021721 0.011074 0.011688 0.010377 0.014799 0.011963 0.015278 0.014395 0.012064 0.013259 0.038092 0.015429 0.011213 0.016408 0.015866 0.025464 0.049314 0.029792 0.000724 
I0428 19:48:17.698596  7412 solver.cpp:224] Iteration 29400 (4.06477 iter/s, 24.6017s/100 iters), loss = 0.235465
I0428 19:48:17.698596  7412 solver.cpp:243]     Train net output #0: loss = 0.235464 (* 1 = 0.235464 loss)
I0428 19:48:17.698596  7412 sgd_solver.cpp:137] Iteration 29400, lr = 0.01
I0428 19:48:17.707597  7412 sgd_solver.cpp:169] scale layer:0.445998 0.464914 0.527777 0.554617 0.522223 0.484162 0.495891 0.491536 0.486571 0.490594 0.500313 0.483951 0.460550 0.437071 0.426498 0.391726 0.363683 0.333292 0.337608 1.010444 
I0428 19:48:17.709596  7412 sgd_solver.cpp:200] weight diff/data:0.014057 0.013697 0.007962 0.011198 0.009942 0.020310 0.010954 0.011706 0.015296 0.012551 0.036209 0.069993 0.016022 0.013554 0.013060 0.012469 0.021061 0.012581 0.009268 0.013407 0.000962 
I0428 19:48:42.027987  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_29500.caffemodel
I0428 19:48:42.036988  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_29500.solverstate
I0428 19:48:42.039988  7412 solver.cpp:336] Iteration 29500, Testing net (#0)
I0428 19:48:50.808490 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:51.176511  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7971
I0428 19:48:51.176511  7412 solver.cpp:403]     Test net output #1: loss = 0.667835 (* 1 = 0.667835 loss)
I0428 19:48:51.409523  7412 solver.cpp:224] Iteration 29500 (2.96633 iter/s, 33.7117s/100 iters), loss = 0.240379
I0428 19:48:51.409523  7412 solver.cpp:243]     Train net output #0: loss = 0.240379 (* 1 = 0.240379 loss)
I0428 19:48:51.410523  7412 sgd_solver.cpp:137] Iteration 29500, lr = 0.01
I0428 19:48:51.418524  7412 sgd_solver.cpp:169] scale layer:0.441304 0.464655 0.525648 0.555353 0.517582 0.483689 0.495755 0.491068 0.486122 0.490629 0.500449 0.483435 0.460826 0.437690 0.426437 0.392452 0.363300 0.332574 0.337495 1.010329 
I0428 19:48:51.420524  7412 sgd_solver.cpp:200] weight diff/data:0.004786 0.015388 0.020378 0.014194 0.013455 0.013468 0.014587 0.012005 0.013157 0.015196 0.012137 0.012944 0.014922 0.011924 0.014365 0.014450 0.013996 0.012226 0.020446 0.013073 0.003085 
I0428 19:49:14.820863 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:49:16.061933  7412 solver.cpp:224] Iteration 29600 (4.05649 iter/s, 24.6518s/100 iters), loss = 0.282333
I0428 19:49:16.061933  7412 solver.cpp:243]     Train net output #0: loss = 0.282332 (* 1 = 0.282332 loss)
I0428 19:49:16.061933  7412 sgd_solver.cpp:137] Iteration 29600, lr = 0.01
I0428 19:49:16.070935  7412 sgd_solver.cpp:169] scale layer:0.442722 0.460635 0.524171 0.555364 0.518831 0.483245 0.494662 0.490586 0.487191 0.490036 0.499744 0.483159 0.459938 0.437939 0.425699 0.392502 0.363245 0.332364 0.337431 1.010836 
I0428 19:49:16.072934  7412 sgd_solver.cpp:200] weight diff/data:0.032460 0.019384 0.010632 0.025342 0.013809 0.011089 0.068455 0.015598 0.015627 0.033581 0.014296 0.018122 0.014953 0.012721 0.013565 0.013206 0.016274 0.015467 0.011733 0.049488 0.001316 
I0428 19:49:40.757346  7412 solver.cpp:224] Iteration 29700 (4.04926 iter/s, 24.6959s/100 iters), loss = 0.204484
I0428 19:49:40.757346  7412 solver.cpp:243]     Train net output #0: loss = 0.204484 (* 1 = 0.204484 loss)
I0428 19:49:40.757346  7412 sgd_solver.cpp:137] Iteration 29700, lr = 0.01
I0428 19:49:40.765347  7412 sgd_solver.cpp:169] scale layer:0.444991 0.460094 0.525252 0.556944 0.520115 0.483248 0.494787 0.489802 0.486842 0.490091 0.499776 0.482652 0.459705 0.437393 0.425623 0.392626 0.362624 0.332046 0.337207 1.013190 
I0428 19:49:40.767346  7412 sgd_solver.cpp:200] weight diff/data:0.006179 0.012923 0.009790 0.011869 0.018727 0.017222 0.016972 0.014251 0.025528 0.011639 0.011435 0.012499 0.010910 0.015630 0.015113 0.015013 0.012520 0.053431 0.010775 0.013351 0.001129 
I0428 19:50:05.460759  7412 solver.cpp:224] Iteration 29800 (4.04795 iter/s, 24.7039s/100 iters), loss = 0.368124
I0428 19:50:05.460759  7412 solver.cpp:243]     Train net output #0: loss = 0.368124 (* 1 = 0.368124 loss)
I0428 19:50:05.460759  7412 sgd_solver.cpp:137] Iteration 29800, lr = 0.01
I0428 19:50:05.469759  7412 sgd_solver.cpp:169] scale layer:0.443135 0.464332 0.526139 0.558246 0.521164 0.483836 0.496032 0.490780 0.486896 0.490548 0.499859 0.483906 0.461210 0.437792 0.426179 0.392308 0.363249 0.332061 0.336648 1.011611 
I0428 19:50:05.471760  7412 sgd_solver.cpp:200] weight diff/data:0.011346 0.010982 0.021002 0.014471 0.019860 0.016617 0.018130 0.011753 0.014669 0.013088 0.015987 0.016038 0.023681 0.012961 0.034397 0.019371 0.013182 0.012663 0.011906 0.010096 0.000910 
I0428 19:50:30.087167  7412 solver.cpp:224] Iteration 29900 (4.06063 iter/s, 24.6267s/100 iters), loss = 0.200615
I0428 19:50:30.087167  7412 solver.cpp:243]     Train net output #0: loss = 0.200615 (* 1 = 0.200615 loss)
I0428 19:50:30.087167  7412 sgd_solver.cpp:137] Iteration 29900, lr = 0.01
I0428 19:50:30.096168  7412 sgd_solver.cpp:169] scale layer:0.440561 0.466517 0.525589 0.558558 0.517880 0.483432 0.497270 0.490696 0.487071 0.490499 0.499739 0.482978 0.460940 0.437432 0.425457 0.392141 0.363507 0.331173 0.335897 1.011666 
I0428 19:50:30.099169  7412 sgd_solver.cpp:200] weight diff/data:0.014251 0.019155 0.036208 0.016193 0.011150 0.011394 0.012727 0.015200 0.013431 0.014881 0.017991 0.138966 0.015222 0.011408 0.010534 0.017414 0.014862 0.014952 0.011161 0.050061 0.001066 
I0428 19:50:53.489506 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:54.469563  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_30000.caffemodel
I0428 19:50:54.477562  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_30000.solverstate
I0428 19:50:54.480563  7412 solver.cpp:336] Iteration 30000, Testing net (#0)
I0428 19:51:03.257066 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:03.621085  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8049
I0428 19:51:03.621085  7412 solver.cpp:403]     Test net output #1: loss = 0.650456 (* 1 = 0.650456 loss)
I0428 19:51:03.855099  7412 solver.cpp:224] Iteration 30000 (2.96135 iter/s, 33.7684s/100 iters), loss = 0.319245
I0428 19:51:03.855099  7412 solver.cpp:243]     Train net output #0: loss = 0.319244 (* 1 = 0.319244 loss)
I0428 19:51:03.855099  7412 sgd_solver.cpp:137] Iteration 30000, lr = 0.01
I0428 19:51:03.864099  7412 sgd_solver.cpp:169] scale layer:0.441457 0.464889 0.524247 0.555729 0.515640 0.484185 0.497017 0.490994 0.486916 0.489717 0.499526 0.484022 0.460148 0.437533 0.425210 0.392162 0.362993 0.330171 0.335257 1.009395 
I0428 19:51:03.866099  7412 sgd_solver.cpp:200] weight diff/data:0.135541 0.022235 0.012415 0.015293 0.026756 0.021686 0.013253 0.023414 0.016865 0.014379 0.013474 0.020508 0.018824 0.029269 0.175825 0.029254 0.020171 0.012941 0.013775 0.032659 0.000938 
I0428 19:51:28.467507  7412 solver.cpp:224] Iteration 30100 (4.063 iter/s, 24.6124s/100 iters), loss = 0.249761
I0428 19:51:28.467507  7412 solver.cpp:243]     Train net output #0: loss = 0.249761 (* 1 = 0.249761 loss)
I0428 19:51:28.467507  7412 sgd_solver.cpp:137] Iteration 30100, lr = 0.01
I0428 19:51:28.476507  7412 sgd_solver.cpp:169] scale layer:0.443368 0.466779 0.524884 0.555230 0.518892 0.483767 0.496269 0.489991 0.487304 0.488875 0.499681 0.483278 0.461104 0.437578 0.423898 0.392065 0.362810 0.329302 0.334881 1.010983 
I0428 19:51:28.478507  7412 sgd_solver.cpp:200] weight diff/data:0.015640 0.018689 0.017916 0.013696 0.029596 0.012653 0.016874 0.015545 0.017630 0.014641 0.012008 0.016527 0.013949 0.012529 0.014852 0.016316 0.030638 0.014847 0.016050 0.016936 0.000956 
I0428 19:51:53.132917  7412 solver.cpp:224] Iteration 30200 (4.05412 iter/s, 24.6663s/100 iters), loss = 0.304732
I0428 19:51:53.132917  7412 solver.cpp:243]     Train net output #0: loss = 0.304731 (* 1 = 0.304731 loss)
I0428 19:51:53.132917  7412 sgd_solver.cpp:137] Iteration 30200, lr = 0.01
I0428 19:51:53.141918  7412 sgd_solver.cpp:169] scale layer:0.444758 0.465384 0.523578 0.556653 0.518754 0.484392 0.496233 0.490700 0.488371 0.489078 0.499017 0.483006 0.460838 0.438114 0.424258 0.391233 0.362382 0.330352 0.335065 1.010577 
I0428 19:51:53.143918  7412 sgd_solver.cpp:200] weight diff/data:0.007011 0.013725 0.013443 0.022963 0.016409 0.014856 0.016099 0.011842 0.011881 0.011473 0.012299 0.018374 0.023748 0.010907 0.017377 0.019421 0.011971 0.014504 0.010231 0.009552 0.000961 
I0428 19:52:17.854331  7412 solver.cpp:224] Iteration 30300 (4.04501 iter/s, 24.7218s/100 iters), loss = 0.168122
I0428 19:52:17.854331  7412 solver.cpp:243]     Train net output #0: loss = 0.168122 (* 1 = 0.168122 loss)
I0428 19:52:17.854331  7412 sgd_solver.cpp:137] Iteration 30300, lr = 0.01
I0428 19:52:17.863332  7412 sgd_solver.cpp:169] scale layer:0.446100 0.464746 0.522891 0.557738 0.520092 0.483364 0.496169 0.490361 0.487658 0.489631 0.500635 0.482670 0.461331 0.438109 0.424809 0.390812 0.361811 0.330245 0.334303 1.010371 
I0428 19:52:17.865332  7412 sgd_solver.cpp:200] weight diff/data:0.007804 0.012830 0.013025 0.009262 0.011357 0.011635 0.012008 0.011700 0.016869 0.011642 0.030395 0.015001 0.012854 0.243293 0.008412 0.011077 0.025803 0.015420 0.011216 0.072338 0.001312 
I0428 19:52:41.367676 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:42.586746  7412 solver.cpp:224] Iteration 30400 (4.04317 iter/s, 24.733s/100 iters), loss = 0.246214
I0428 19:52:42.586746  7412 solver.cpp:243]     Train net output #0: loss = 0.246213 (* 1 = 0.246213 loss)
I0428 19:52:42.586746  7412 sgd_solver.cpp:137] Iteration 30400, lr = 0.01
I0428 19:52:42.595746  7412 sgd_solver.cpp:169] scale layer:0.447075 0.468816 0.521573 0.555196 0.518666 0.483632 0.495041 0.489468 0.487376 0.489482 0.499392 0.482363 0.461667 0.437705 0.424687 0.390867 0.361637 0.329653 0.333873 1.011682 
I0428 19:52:42.597748  7412 sgd_solver.cpp:200] weight diff/data:0.008116 0.012571 0.013444 0.014126 0.013628 0.014259 0.037435 0.013680 0.030453 0.016338 0.023316 0.011443 0.020390 0.014440 0.012456 0.015088 0.053451 0.013155 0.010619 0.009473 0.001782 
I0428 19:53:07.126150  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_30500.caffemodel
I0428 19:53:07.134150  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_30500.solverstate
I0428 19:53:07.137151  7412 solver.cpp:336] Iteration 30500, Testing net (#0)
I0428 19:53:15.937654 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:53:16.301674  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8051
I0428 19:53:16.301674  7412 solver.cpp:403]     Test net output #1: loss = 0.670687 (* 1 = 0.670687 loss)
I0428 19:53:16.537688  7412 solver.cpp:224] Iteration 30500 (2.94533 iter/s, 33.952s/100 iters), loss = 0.253691
I0428 19:53:16.537688  7412 solver.cpp:243]     Train net output #0: loss = 0.25369 (* 1 = 0.25369 loss)
I0428 19:53:16.537688  7412 sgd_solver.cpp:137] Iteration 30500, lr = 0.01
I0428 19:53:16.546689  7412 sgd_solver.cpp:169] scale layer:0.451114 0.469564 0.523008 0.553741 0.518296 0.484415 0.494736 0.489665 0.487259 0.489325 0.500660 0.483855 0.461853 0.437691 0.424774 0.391417 0.361639 0.330543 0.333763 1.014382 
I0428 19:53:16.547688  7412 sgd_solver.cpp:200] weight diff/data:0.010914 0.014135 0.016680 0.017010 0.015378 0.017294 0.014838 0.016036 0.016381 0.023115 0.017760 0.018961 0.015235 0.015228 0.012739 0.012264 0.021722 0.017394 0.049370 0.011599 0.001024 
I0428 19:53:41.224100  7412 solver.cpp:224] Iteration 30600 (4.05072 iter/s, 24.6869s/100 iters), loss = 0.245546
I0428 19:53:41.225100  7412 solver.cpp:243]     Train net output #0: loss = 0.245546 (* 1 = 0.245546 loss)
I0428 19:53:41.225100  7412 sgd_solver.cpp:137] Iteration 30600, lr = 0.01
I0428 19:53:41.234100  7412 sgd_solver.cpp:169] scale layer:0.447429 0.465460 0.523052 0.555089 0.515424 0.485175 0.493968 0.491223 0.487182 0.489218 0.500851 0.482708 0.461691 0.438307 0.425035 0.391675 0.362096 0.329735 0.333521 1.012812 
I0428 19:53:41.236100  7412 sgd_solver.cpp:200] weight diff/data:0.027444 0.012119 0.012505 0.013126 0.014395 0.048989 0.014776 0.012320 0.013886 0.018046 0.015399 0.011894 0.033594 0.012049 0.011613 0.015465 0.014273 0.011576 0.027138 0.042445 0.001692 
I0428 19:54:05.867509  7412 solver.cpp:224] Iteration 30700 (4.0579 iter/s, 24.6433s/100 iters), loss = 0.250962
I0428 19:54:05.867509  7412 solver.cpp:243]     Train net output #0: loss = 0.250961 (* 1 = 0.250961 loss)
I0428 19:54:05.867509  7412 sgd_solver.cpp:137] Iteration 30700, lr = 0.01
I0428 19:54:05.875510  7412 sgd_solver.cpp:169] scale layer:0.443948 0.466003 0.521955 0.556422 0.511112 0.484205 0.495153 0.490938 0.487221 0.488579 0.500889 0.482131 0.462159 0.438107 0.424305 0.391882 0.361819 0.329676 0.333063 1.014527 
I0428 19:54:05.878510  7412 sgd_solver.cpp:200] weight diff/data:0.052062 0.021517 0.012561 0.014563 0.492269 0.009985 0.015474 0.013834 0.015814 0.011552 0.017223 0.016129 0.010615 0.011576 0.009778 0.020902 0.012120 0.019895 0.013407 0.019550 0.002481 
I0428 19:54:29.284849 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:54:30.504918  7412 solver.cpp:224] Iteration 30800 (4.05876 iter/s, 24.6381s/100 iters), loss = 0.319845
I0428 19:54:30.504918  7412 solver.cpp:243]     Train net output #0: loss = 0.319845 (* 1 = 0.319845 loss)
I0428 19:54:30.505919  7412 sgd_solver.cpp:137] Iteration 30800, lr = 0.01
I0428 19:54:30.513919  7412 sgd_solver.cpp:169] scale layer:0.449792 0.467572 0.521016 0.554270 0.509140 0.483975 0.495192 0.489844 0.487380 0.488896 0.500748 0.482111 0.462653 0.437625 0.424718 0.392343 0.362266 0.328771 0.332684 1.014232 
I0428 19:54:30.515919  7412 sgd_solver.cpp:200] weight diff/data:0.009157 0.108382 0.015643 0.010724 0.049678 0.012386 0.011667 0.015311 0.017711 0.021152 0.009967 0.021402 0.019637 0.013840 0.071594 0.012795 0.033572 0.017056 0.010823 0.011691 0.001883 
I0428 19:54:55.143328  7412 solver.cpp:224] Iteration 30900 (4.05866 iter/s, 24.6387s/100 iters), loss = 0.193637
I0428 19:54:55.144328  7412 solver.cpp:243]     Train net output #0: loss = 0.193637 (* 1 = 0.193637 loss)
I0428 19:54:55.144328  7412 sgd_solver.cpp:137] Iteration 30900, lr = 0.01
I0428 19:54:55.152328  7412 sgd_solver.cpp:169] scale layer:0.448598 0.467979 0.524433 0.556247 0.514315 0.483951 0.495603 0.489241 0.486739 0.489381 0.500166 0.482863 0.462210 0.437860 0.424943 0.392458 0.362622 0.328595 0.332827 1.013934 
I0428 19:54:55.154328  7412 sgd_solver.cpp:200] weight diff/data:0.007132 0.010081 0.008224 0.053330 0.011087 0.016456 0.011298 0.015220 0.014447 0.011530 0.014110 0.015342 0.011697 0.017038 0.018135 0.012744 0.014945 0.013232 0.013458 0.016559 0.001945 
I0428 19:55:19.531723  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_31000.caffemodel
I0428 19:55:19.539723  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_31000.solverstate
I0428 19:55:19.542723  7412 solver.cpp:336] Iteration 31000, Testing net (#0)
I0428 19:55:28.293225 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:55:28.655246  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8116
I0428 19:55:28.655246  7412 solver.cpp:403]     Test net output #1: loss = 0.64403 (* 1 = 0.64403 loss)
I0428 19:55:28.889258  7412 solver.cpp:224] Iteration 31000 (2.96329 iter/s, 33.7463s/100 iters), loss = 0.1905
I0428 19:55:28.889258  7412 solver.cpp:243]     Train net output #0: loss = 0.1905 (* 1 = 0.1905 loss)
I0428 19:55:28.890259  7412 sgd_solver.cpp:137] Iteration 31000, lr = 0.01
I0428 19:55:28.898258  7412 sgd_solver.cpp:169] scale layer:0.446014 0.468170 0.521966 0.556022 0.514584 0.484186 0.496196 0.488833 0.488168 0.489312 0.499190 0.483283 0.462766 0.438323 0.425031 0.391688 0.361489 0.327964 0.332614 1.013626 
I0428 19:55:28.899258  7412 sgd_solver.cpp:200] weight diff/data:0.006899 0.107756 0.015390 0.009486 0.024906 0.099396 0.015261 0.029692 0.018593 0.022054 0.015619 0.014050 0.021437 0.012351 0.013671 0.016642 0.018141 0.036091 0.011789 0.016157 0.001016 
I0428 19:55:53.501667  7412 solver.cpp:224] Iteration 31100 (4.06306 iter/s, 24.612s/100 iters), loss = 0.244986
I0428 19:55:53.501667  7412 solver.cpp:243]     Train net output #0: loss = 0.244986 (* 1 = 0.244986 loss)
I0428 19:55:53.501667  7412 sgd_solver.cpp:137] Iteration 31100, lr = 0.01
I0428 19:55:53.510666  7412 sgd_solver.cpp:169] scale layer:0.440872 0.467874 0.521218 0.556522 0.514984 0.484187 0.495714 0.489109 0.489204 0.490309 0.499308 0.482257 0.462394 0.437638 0.425138 0.391957 0.361847 0.328006 0.332205 1.013692 
I0428 19:55:53.512666  7412 sgd_solver.cpp:200] weight diff/data:0.011525 0.013837 0.010041 0.010157 0.016399 0.017414 0.013413 0.010887 0.018302 0.025901 0.012777 0.013553 0.125754 0.020048 0.020416 0.015137 0.013782 0.013522 0.016159 0.012726 0.001355 
I0428 19:56:16.881003 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:18.098073  7412 solver.cpp:224] Iteration 31200 (4.06556 iter/s, 24.5969s/100 iters), loss = 0.336203
I0428 19:56:18.098073  7412 solver.cpp:243]     Train net output #0: loss = 0.336202 (* 1 = 0.336202 loss)
I0428 19:56:18.098073  7412 sgd_solver.cpp:137] Iteration 31200, lr = 0.01
I0428 19:56:18.107074  7412 sgd_solver.cpp:169] scale layer:0.442858 0.468414 0.520278 0.556331 0.514468 0.484400 0.495630 0.490329 0.488413 0.490165 0.500156 0.482518 0.462112 0.437124 0.424694 0.392156 0.361465 0.328680 0.331232 1.012914 
I0428 19:56:18.109073  7412 sgd_solver.cpp:200] weight diff/data:0.022781 0.017216 0.010204 0.011821 0.020539 0.045913 0.013935 0.105947 0.016231 0.012655 0.013226 0.013287 0.587196 0.013708 0.094851 0.014103 0.017851 0.012337 0.015659 0.014278 0.002052 
I0428 19:56:42.672478  7412 solver.cpp:224] Iteration 31300 (4.06923 iter/s, 24.5747s/100 iters), loss = 0.241389
I0428 19:56:42.672478  7412 solver.cpp:243]     Train net output #0: loss = 0.241388 (* 1 = 0.241388 loss)
I0428 19:56:42.672478  7412 sgd_solver.cpp:137] Iteration 31300, lr = 0.01
I0428 19:56:42.681479  7412 sgd_solver.cpp:169] scale layer:0.444659 0.466437 0.519981 0.555832 0.516817 0.485608 0.494837 0.489754 0.488402 0.490465 0.500547 0.481854 0.462711 0.437768 0.424893 0.392137 0.360090 0.327759 0.331158 1.010987 
I0428 19:56:42.683480  7412 sgd_solver.cpp:200] weight diff/data:0.016220 0.010763 0.019702 0.011625 0.012867 0.016958 0.013758 0.015204 0.023464 0.018750 0.012457 0.048499 0.021591 0.023962 0.030226 0.018367 0.016885 0.015114 0.011731 0.019322 0.001841 
I0428 19:57:07.237884  7412 solver.cpp:224] Iteration 31400 (4.07081 iter/s, 24.5651s/100 iters), loss = 0.308699
I0428 19:57:07.237884  7412 solver.cpp:243]     Train net output #0: loss = 0.308699 (* 1 = 0.308699 loss)
I0428 19:57:07.237884  7412 sgd_solver.cpp:137] Iteration 31400, lr = 0.01
I0428 19:57:07.246884  7412 sgd_solver.cpp:169] scale layer:0.440157 0.466904 0.522418 0.557906 0.516529 0.485424 0.494322 0.490501 0.488406 0.490053 0.500301 0.483008 0.462560 0.437419 0.425390 0.391032 0.359945 0.327277 0.330828 1.012478 
I0428 19:57:07.248884  7412 sgd_solver.cpp:200] weight diff/data:0.009094 0.016161 0.468766 0.043133 0.013060 0.011026 0.011504 0.014155 0.100630 0.013554 0.014558 0.012758 0.016269 0.012230 0.035186 0.018905 0.015367 0.011416 0.011703 0.013919 0.000831 
I0428 19:57:31.596276  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_31500.caffemodel
I0428 19:57:31.605278  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_31500.solverstate
I0428 19:57:31.608278  7412 solver.cpp:336] Iteration 31500, Testing net (#0)
I0428 19:57:40.363778 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:40.727799  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7959
I0428 19:57:40.727799  7412 solver.cpp:403]     Test net output #1: loss = 0.690701 (* 1 = 0.690701 loss)
I0428 19:57:40.961812  7412 solver.cpp:224] Iteration 31500 (2.9652 iter/s, 33.7245s/100 iters), loss = 0.247034
I0428 19:57:40.961812  7412 solver.cpp:243]     Train net output #0: loss = 0.247034 (* 1 = 0.247034 loss)
I0428 19:57:40.961812  7412 sgd_solver.cpp:137] Iteration 31500, lr = 0.01
I0428 19:57:40.970813  7412 sgd_solver.cpp:169] scale layer:0.442905 0.467845 0.522482 0.559030 0.513082 0.486539 0.495909 0.490754 0.488021 0.490615 0.500139 0.483652 0.463529 0.437210 0.425018 0.390396 0.360331 0.327632 0.330554 1.011456 
I0428 19:57:40.972813  7412 sgd_solver.cpp:200] weight diff/data:0.013300 0.007586 0.051386 0.015253 0.016471 0.015726 0.012757 0.011933 0.017968 0.012110 0.012401 0.113652 0.021650 0.013857 0.013114 0.012289 0.016444 0.015054 0.012202 0.015897 0.001462 
I0428 19:58:04.344149 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:58:05.555219  7412 solver.cpp:224] Iteration 31600 (4.06609 iter/s, 24.5936s/100 iters), loss = 0.263107
I0428 19:58:05.555219  7412 solver.cpp:243]     Train net output #0: loss = 0.263107 (* 1 = 0.263107 loss)
I0428 19:58:05.555219  7412 sgd_solver.cpp:137] Iteration 31600, lr = 0.01
I0428 19:58:05.564219  7412 sgd_solver.cpp:169] scale layer:0.445240 0.469216 0.521942 0.560009 0.513934 0.485004 0.495909 0.491059 0.486907 0.490763 0.499286 0.483128 0.462971 0.436928 0.425656 0.390786 0.359889 0.327540 0.330037 1.010954 
I0428 19:58:05.566220  7412 sgd_solver.cpp:200] weight diff/data:0.009067 0.034897 0.013396 0.014710 0.012850 0.014290 0.012324 0.014274 0.023762 0.011920 0.193777 0.024131 0.020851 0.012158 0.012921 0.012979 0.020274 0.015815 0.016232 0.010788 0.017520 
I0428 19:58:30.158627  7412 solver.cpp:224] Iteration 31700 (4.0644 iter/s, 24.6039s/100 iters), loss = 0.239138
I0428 19:58:30.158627  7412 solver.cpp:243]     Train net output #0: loss = 0.239137 (* 1 = 0.239137 loss)
I0428 19:58:30.158627  7412 sgd_solver.cpp:137] Iteration 31700, lr = 0.01
I0428 19:58:30.168627  7412 sgd_solver.cpp:169] scale layer:0.447656 0.467994 0.522682 0.557859 0.515120 0.485850 0.495980 0.490024 0.487325 0.489998 0.500327 0.483728 0.462714 0.437801 0.424953 0.391033 0.358913 0.327642 0.329573 1.010932 
I0428 19:58:30.170627  7412 sgd_solver.cpp:200] weight diff/data:0.009024 0.028208 0.010794 0.009071 0.018165 0.017917 0.018257 0.023155 0.012448 0.079433 0.011490 0.017752 0.012790 0.024149 0.013303 0.020430 0.019378 0.011509 0.017008 0.017513 0.002057 
I0428 19:58:54.820036  7412 solver.cpp:224] Iteration 31800 (4.05492 iter/s, 24.6614s/100 iters), loss = 0.270742
I0428 19:58:54.820036  7412 solver.cpp:243]     Train net output #0: loss = 0.270741 (* 1 = 0.270741 loss)
I0428 19:58:54.820036  7412 sgd_solver.cpp:137] Iteration 31800, lr = 0.01
I0428 19:58:54.829037  7412 sgd_solver.cpp:169] scale layer:0.441572 0.469092 0.522570 0.556883 0.519130 0.485656 0.494895 0.491487 0.487359 0.490423 0.500462 0.483950 0.462493 0.437333 0.424829 0.391833 0.360141 0.327386 0.329672 1.010691 
I0428 19:58:54.831037  7412 sgd_solver.cpp:200] weight diff/data:0.007723 0.010942 0.021283 0.011363 0.010948 0.011321 0.017234 0.014334 0.013277 0.012113 0.014938 0.015635 0.062117 0.011230 0.012605 0.010248 0.012634 0.073094 0.012093 0.010277 0.001244 
I0428 19:59:19.451445  7412 solver.cpp:224] Iteration 31900 (4.05987 iter/s, 24.6313s/100 iters), loss = 0.280593
I0428 19:59:19.451445  7412 solver.cpp:243]     Train net output #0: loss = 0.280593 (* 1 = 0.280593 loss)
I0428 19:59:19.451445  7412 sgd_solver.cpp:137] Iteration 31900, lr = 0.01
I0428 19:59:19.460446  7412 sgd_solver.cpp:169] scale layer:0.436380 0.467464 0.522707 0.558244 0.519557 0.486175 0.494436 0.491247 0.487429 0.490128 0.501344 0.483477 0.461891 0.438010 0.423627 0.391223 0.359690 0.326651 0.328980 1.010187 
I0428 19:59:19.462446  7412 sgd_solver.cpp:200] weight diff/data:0.006907 0.011349 0.011425 0.010109 0.018691 0.027606 0.084175 0.011972 0.010460 0.011632 0.010285 0.019382 0.011470 0.028794 0.012483 0.013352 0.024322 0.016323 0.294596 0.016370 0.001300 
I0428 19:59:42.871785 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:43.858841  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_32000.caffemodel
I0428 19:59:43.867842  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_32000.solverstate
I0428 19:59:43.871842  7412 solver.cpp:336] Iteration 32000, Testing net (#0)
I0428 19:59:52.643344 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:53.005364  7412 solver.cpp:403]     Test net output #0: accuracy = 0.796
I0428 19:59:53.005364  7412 solver.cpp:403]     Test net output #1: loss = 0.682697 (* 1 = 0.682697 loss)
I0428 19:59:53.240378  7412 solver.cpp:224] Iteration 32000 (2.95949 iter/s, 33.7896s/100 iters), loss = 0.324072
I0428 19:59:53.241379  7412 solver.cpp:243]     Train net output #0: loss = 0.324072 (* 1 = 0.324072 loss)
I0428 19:59:53.241379  7412 sgd_solver.cpp:137] Iteration 32000, lr = 0.01
I0428 19:59:53.249378  7412 sgd_solver.cpp:169] scale layer:0.438869 0.465160 0.524076 0.558964 0.516998 0.484976 0.495058 0.491341 0.487468 0.489245 0.501002 0.483993 0.460330 0.437978 0.423845 0.391082 0.359119 0.326672 0.328707 1.009271 
I0428 19:59:53.251379  7412 sgd_solver.cpp:200] weight diff/data:0.011749 0.015892 0.013775 0.014625 0.022433 0.022476 0.022208 0.041269 0.019303 0.020355 0.013011 0.013374 0.014552 0.014763 0.015115 0.027039 0.013841 0.013567 0.024636 0.028201 0.001636 
I0428 20:00:17.878787  7412 solver.cpp:224] Iteration 32100 (4.05882 iter/s, 24.6377s/100 iters), loss = 0.359121
I0428 20:00:17.878787  7412 solver.cpp:243]     Train net output #0: loss = 0.35912 (* 1 = 0.35912 loss)
I0428 20:00:17.878787  7412 sgd_solver.cpp:137] Iteration 32100, lr = 0.01
I0428 20:00:17.887789  7412 sgd_solver.cpp:169] scale layer:0.443251 0.467659 0.524998 0.560229 0.521014 0.486241 0.495713 0.490974 0.488460 0.489439 0.501670 0.484551 0.461294 0.437368 0.423870 0.391398 0.359090 0.327238 0.328622 1.010076 
I0428 20:00:17.890789  7412 sgd_solver.cpp:200] weight diff/data:0.010057 0.010956 0.018250 0.093732 0.099826 0.011743 0.010846 0.011967 0.026893 0.015667 0.016312 0.011344 0.016596 0.012584 0.015027 0.013712 0.014625 0.023491 0.014398 0.012563 0.002001 
I0428 20:00:42.583200  7412 solver.cpp:224] Iteration 32200 (4.04775 iter/s, 24.7051s/100 iters), loss = 0.256211
I0428 20:00:42.583200  7412 solver.cpp:243]     Train net output #0: loss = 0.256211 (* 1 = 0.256211 loss)
I0428 20:00:42.583200  7412 sgd_solver.cpp:137] Iteration 32200, lr = 0.01
I0428 20:00:42.592201  7412 sgd_solver.cpp:169] scale layer:0.449815 0.466509 0.524319 0.560045 0.519663 0.487026 0.495695 0.491323 0.488237 0.489238 0.501910 0.485548 0.462447 0.436853 0.424232 0.391460 0.358814 0.326273 0.328572 1.010794 
I0428 20:00:42.594202  7412 sgd_solver.cpp:200] weight diff/data:0.012710 0.010934 0.011686 0.010656 0.056162 0.047183 0.011415 0.013279 0.015628 0.014382 0.017000 0.012161 0.012943 0.010411 0.014484 0.013324 0.013269 0.013179 0.011656 0.010183 0.001800 
I0428 20:01:07.258612  7412 solver.cpp:224] Iteration 32300 (4.05262 iter/s, 24.6754s/100 iters), loss = 0.310498
I0428 20:01:07.258612  7412 solver.cpp:243]     Train net output #0: loss = 0.310497 (* 1 = 0.310497 loss)
I0428 20:01:07.258612  7412 sgd_solver.cpp:137] Iteration 32300, lr = 0.01
I0428 20:01:07.267612  7412 sgd_solver.cpp:169] scale layer:0.441911 0.464999 0.523173 0.562357 0.518023 0.486809 0.495569 0.490421 0.488237 0.489290 0.502587 0.485582 0.461447 0.436414 0.423913 0.391217 0.358789 0.325955 0.328063 1.011088 
I0428 20:01:07.269613  7412 sgd_solver.cpp:200] weight diff/data:0.029200 0.054205 0.011423 0.013327 0.018246 0.014990 0.012670 0.017818 0.014028 0.014285 0.015349 0.015668 0.049633 0.014729 0.014841 0.011772 0.013972 0.016941 0.010874 0.016045 0.001910 
I0428 20:01:30.698952 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:01:31.908021  7412 solver.cpp:224] Iteration 32400 (4.05675 iter/s, 24.6503s/100 iters), loss = 0.389376
I0428 20:01:31.908021  7412 solver.cpp:243]     Train net output #0: loss = 0.389375 (* 1 = 0.389375 loss)
I0428 20:01:31.909021  7412 sgd_solver.cpp:137] Iteration 32400, lr = 0.01
I0428 20:01:31.917022  7412 sgd_solver.cpp:169] scale layer:0.443959 0.468634 0.522980 0.562232 0.520311 0.487386 0.494705 0.490120 0.487191 0.489855 0.502515 0.485001 0.461862 0.436442 0.423895 0.390467 0.358989 0.326427 0.327447 1.011010 
I0428 20:01:31.919023  7412 sgd_solver.cpp:200] weight diff/data:0.020167 0.018235 0.038282 0.012877 0.032098 0.016193 0.022255 0.014282 0.016037 0.017956 0.018722 0.011539 0.016215 0.017754 0.012546 0.018404 0.030946 0.013299 0.015834 0.015807 0.001648 
I0428 20:01:56.322418  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_32500.caffemodel
I0428 20:01:56.330418  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_32500.solverstate
I0428 20:01:56.334419  7412 solver.cpp:336] Iteration 32500, Testing net (#0)
I0428 20:02:05.109920 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:02:05.475941  7412 solver.cpp:403]     Test net output #0: accuracy = 0.799
I0428 20:02:05.475941  7412 solver.cpp:403]     Test net output #1: loss = 0.700176 (* 1 = 0.700176 loss)
I0428 20:02:05.708956  7412 solver.cpp:224] Iteration 32500 (2.95849 iter/s, 33.801s/100 iters), loss = 0.307221
I0428 20:02:05.708956  7412 solver.cpp:243]     Train net output #0: loss = 0.30722 (* 1 = 0.30722 loss)
I0428 20:02:05.708956  7412 sgd_solver.cpp:137] Iteration 32500, lr = 0.01
I0428 20:02:05.716955  7412 sgd_solver.cpp:169] scale layer:0.441157 0.466914 0.524468 0.559736 0.522078 0.487202 0.494434 0.489833 0.488867 0.489877 0.501955 0.486291 0.462828 0.437517 0.424144 0.390257 0.360277 0.325920 0.326835 1.009639 
I0428 20:02:05.718955  7412 sgd_solver.cpp:200] weight diff/data:0.012391 0.010355 0.020294 0.012094 0.010979 0.012274 0.026019 0.018038 0.016096 0.028156 0.012736 0.016380 0.020587 0.011435 0.049306 0.030716 0.033726 0.016656 0.019824 0.010332 0.010728 
I0428 20:02:30.328363  7412 solver.cpp:224] Iteration 32600 (4.0617 iter/s, 24.6203s/100 iters), loss = 0.248545
I0428 20:02:30.329363  7412 solver.cpp:243]     Train net output #0: loss = 0.248545 (* 1 = 0.248545 loss)
I0428 20:02:30.329363  7412 sgd_solver.cpp:137] Iteration 32600, lr = 0.01
I0428 20:02:30.337363  7412 sgd_solver.cpp:169] scale layer:0.441430 0.464008 0.526567 0.559814 0.519486 0.487299 0.496135 0.491495 0.490312 0.490320 0.502516 0.485245 0.463298 0.437937 0.424970 0.390743 0.359334 0.325591 0.327302 1.006739 
I0428 20:02:30.339363  7412 sgd_solver.cpp:200] weight diff/data:0.030940 0.011868 0.020776 0.020704 0.014910 0.012980 0.026371 0.016807 0.014032 0.016812 0.038648 0.011973 0.105244 0.029682 0.013416 0.018157 0.016163 0.015303 0.034402 0.016019 0.002180 
I0428 20:02:54.980773  7412 solver.cpp:224] Iteration 32700 (4.05639 iter/s, 24.6525s/100 iters), loss = 0.156897
I0428 20:02:54.980773  7412 solver.cpp:243]     Train net output #0: loss = 0.156897 (* 1 = 0.156897 loss)
I0428 20:02:54.980773  7412 sgd_solver.cpp:137] Iteration 32700, lr = 0.01
I0428 20:02:54.989773  7412 sgd_solver.cpp:169] scale layer:0.446410 0.462956 0.525243 0.560675 0.519966 0.488255 0.496629 0.491340 0.490997 0.491625 0.502896 0.485491 0.464589 0.437714 0.425110 0.391686 0.359275 0.325228 0.327623 1.006015 
I0428 20:02:54.991773  7412 sgd_solver.cpp:200] weight diff/data:0.005120 0.007277 0.010067 0.013306 0.011201 0.013926 0.018617 0.092272 0.105313 0.015441 0.015253 0.024288 0.013676 0.011747 0.016831 0.013620 0.013268 0.274686 0.057295 0.049981 0.003835 
I0428 20:03:18.426115 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:03:19.652184  7412 solver.cpp:224] Iteration 32800 (4.05325 iter/s, 24.6715s/100 iters), loss = 0.241599
I0428 20:03:19.652184  7412 solver.cpp:243]     Train net output #0: loss = 0.241598 (* 1 = 0.241598 loss)
I0428 20:03:19.652184  7412 sgd_solver.cpp:137] Iteration 32800, lr = 0.01
I0428 20:03:19.661185  7412 sgd_solver.cpp:169] scale layer:0.446601 0.466026 0.525017 0.560206 0.517537 0.488548 0.496822 0.492459 0.489402 0.490905 0.502486 0.485393 0.463456 0.437717 0.424612 0.391766 0.358928 0.324595 0.326822 1.007497 
I0428 20:03:19.663185  7412 sgd_solver.cpp:200] weight diff/data:0.008149 0.009722 0.009963 0.009989 0.012670 0.011829 0.015901 0.012679 0.015447 0.022022 0.015565 0.017500 0.036195 0.015242 0.013764 0.022073 0.036089 0.012208 0.012806 0.023785 0.001510 
I0428 20:03:44.327595  7412 solver.cpp:224] Iteration 32900 (4.05255 iter/s, 24.6758s/100 iters), loss = 0.130951
I0428 20:03:44.328595  7412 solver.cpp:243]     Train net output #0: loss = 0.130951 (* 1 = 0.130951 loss)
I0428 20:03:44.328595  7412 sgd_solver.cpp:137] Iteration 32900, lr = 0.01
I0428 20:03:44.337596  7412 sgd_solver.cpp:169] scale layer:0.451165 0.468336 0.523602 0.560841 0.515645 0.487937 0.496621 0.490872 0.489238 0.491400 0.502947 0.485099 0.463628 0.437733 0.424803 0.391338 0.359116 0.324798 0.327061 1.008914 
I0428 20:03:44.339596  7412 sgd_solver.cpp:200] weight diff/data:0.007619 0.016260 0.009569 0.016579 0.021249 0.012568 0.012907 0.011194 0.010815 0.009246 0.015941 0.013540 0.016175 0.041217 0.035581 0.028568 0.019163 0.016703 0.013955 0.016560 0.004873 
I0428 20:04:08.762993  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_33000.caffemodel
I0428 20:04:08.770993  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_33000.solverstate
I0428 20:04:08.773994  7412 solver.cpp:336] Iteration 33000, Testing net (#0)
I0428 20:04:17.552496 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:17.913516  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8105
I0428 20:04:17.913516  7412 solver.cpp:403]     Test net output #1: loss = 0.655159 (* 1 = 0.655159 loss)
I0428 20:04:18.144531  7412 solver.cpp:224] Iteration 33000 (2.95712 iter/s, 33.8166s/100 iters), loss = 0.229286
I0428 20:04:18.144531  7412 solver.cpp:243]     Train net output #0: loss = 0.229286 (* 1 = 0.229286 loss)
I0428 20:04:18.144531  7412 sgd_solver.cpp:137] Iteration 33000, lr = 0.01
I0428 20:04:18.152530  7412 sgd_solver.cpp:169] scale layer:0.446059 0.466318 0.522603 0.560128 0.514182 0.487813 0.496153 0.491922 0.489782 0.491443 0.502762 0.485295 0.462952 0.437603 0.424662 0.391974 0.358803 0.324789 0.326589 1.008684 
I0428 20:04:18.154531  7412 sgd_solver.cpp:200] weight diff/data:0.012787 0.018357 0.027557 0.010748 0.013240 0.013609 0.017547 0.015065 0.031412 0.009131 0.012354 0.019563 0.015188 0.015840 0.019335 0.011952 0.014353 0.023004 0.042356 0.013793 0.001025 
I0428 20:04:42.822942  7412 solver.cpp:224] Iteration 33100 (4.05211 iter/s, 24.6785s/100 iters), loss = 0.233578
I0428 20:04:42.822942  7412 solver.cpp:243]     Train net output #0: loss = 0.233578 (* 1 = 0.233578 loss)
I0428 20:04:42.822942  7412 sgd_solver.cpp:137] Iteration 33100, lr = 0.01
I0428 20:04:42.830942  7412 sgd_solver.cpp:169] scale layer:0.440232 0.467388 0.523628 0.560935 0.515946 0.486890 0.496232 0.491817 0.489188 0.491716 0.502291 0.485008 0.463682 0.437106 0.424814 0.391983 0.357845 0.324191 0.325990 1.013078 
I0428 20:04:42.832942  7412 sgd_solver.cpp:200] weight diff/data:0.005891 0.257615 0.016210 0.016346 0.016025 0.013635 0.016522 0.011469 0.044258 0.010686 0.025215 0.020194 0.012272 0.022193 0.015348 0.012860 0.013013 0.013000 0.013080 0.030627 0.003204 
I0428 20:05:06.270282 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:05:07.493352  7412 solver.cpp:224] Iteration 33200 (4.05329 iter/s, 24.6713s/100 iters), loss = 0.32103
I0428 20:05:07.494352  7412 solver.cpp:243]     Train net output #0: loss = 0.321029 (* 1 = 0.321029 loss)
I0428 20:05:07.494352  7412 sgd_solver.cpp:137] Iteration 33200, lr = 0.01
I0428 20:05:07.502353  7412 sgd_solver.cpp:169] scale layer:0.442867 0.466768 0.523322 0.561187 0.511570 0.485737 0.495888 0.491131 0.487413 0.491823 0.501184 0.485623 0.462592 0.437717 0.424605 0.391912 0.358791 0.323950 0.325568 1.011362 
I0428 20:05:07.504354  7412 sgd_solver.cpp:200] weight diff/data:0.012373 0.010839 0.011876 0.014434 0.015600 0.014073 0.016417 0.014176 0.016842 0.034545 0.013026 0.025902 0.031819 0.029655 0.011234 0.028755 0.012920 0.013710 0.012075 0.012826 0.001503 
I0428 20:05:32.182765  7412 solver.cpp:224] Iteration 33300 (4.05037 iter/s, 24.6891s/100 iters), loss = 0.251911
I0428 20:05:32.182765  7412 solver.cpp:243]     Train net output #0: loss = 0.251911 (* 1 = 0.251911 loss)
I0428 20:05:32.182765  7412 sgd_solver.cpp:137] Iteration 33300, lr = 0.01
I0428 20:05:32.191766  7412 sgd_solver.cpp:169] scale layer:0.446169 0.473377 0.522644 0.561882 0.511923 0.486712 0.495728 0.490982 0.487867 0.491751 0.501643 0.484709 0.464814 0.437948 0.424553 0.391947 0.358865 0.324206 0.325383 1.009430 
I0428 20:05:32.193765  7412 sgd_solver.cpp:200] weight diff/data:0.011826 0.008420 0.018906 0.013131 0.020717 0.011944 0.024298 0.019572 0.022172 0.017545 0.019647 0.026804 0.012908 0.012800 0.022429 0.017783 0.012797 0.016475 0.014983 0.011973 0.001168 
I0428 20:05:56.859176  7412 solver.cpp:224] Iteration 33400 (4.05247 iter/s, 24.6763s/100 iters), loss = 0.191659
I0428 20:05:56.859176  7412 solver.cpp:243]     Train net output #0: loss = 0.191659 (* 1 = 0.191659 loss)
I0428 20:05:56.859176  7412 sgd_solver.cpp:137] Iteration 33400, lr = 0.01
I0428 20:05:56.867177  7412 sgd_solver.cpp:169] scale layer:0.439869 0.468352 0.523754 0.560783 0.512202 0.486424 0.495011 0.491352 0.487663 0.491492 0.501875 0.484836 0.464455 0.438044 0.424781 0.391469 0.359122 0.324026 0.325051 1.007443 
I0428 20:05:56.869176  7412 sgd_solver.cpp:200] weight diff/data:0.011123 0.020604 0.019173 0.013045 0.012479 0.010945 0.021269 0.012771 0.016006 0.013847 0.260472 0.012509 0.025191 0.018379 0.029520 0.011708 0.029032 0.012203 0.011385 0.017819 0.001788 
I0428 20:06:21.303575  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_33500.caffemodel
I0428 20:06:21.312574  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_33500.solverstate
I0428 20:06:21.315575  7412 solver.cpp:336] Iteration 33500, Testing net (#0)
I0428 20:06:30.108078 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:06:30.470098  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8011
I0428 20:06:30.471098  7412 solver.cpp:403]     Test net output #1: loss = 0.685262 (* 1 = 0.685262 loss)
I0428 20:06:30.709112  7412 solver.cpp:224] Iteration 33500 (2.95411 iter/s, 33.8511s/100 iters), loss = 0.228419
I0428 20:06:30.710113  7412 solver.cpp:243]     Train net output #0: loss = 0.228418 (* 1 = 0.228418 loss)
I0428 20:06:30.710113  7412 sgd_solver.cpp:137] Iteration 33500, lr = 0.01
I0428 20:06:30.718112  7412 sgd_solver.cpp:169] scale layer:0.441297 0.467598 0.523072 0.561051 0.515358 0.486082 0.496094 0.491202 0.489382 0.491928 0.501716 0.483802 0.464826 0.437600 0.424769 0.391216 0.358152 0.323532 0.324989 1.010572 
I0428 20:06:30.719112  7412 sgd_solver.cpp:200] weight diff/data:0.011282 0.016727 0.012107 0.015457 0.016046 0.019448 0.028605 0.017830 0.019036 0.015868 0.017042 0.015323 0.024422 0.018498 0.033973 0.017441 0.016055 0.023536 0.023505 0.013895 0.003092 
I0428 20:06:54.122452 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:06:55.334520  7412 solver.cpp:224] Iteration 33600 (4.06086 iter/s, 24.6253s/100 iters), loss = 0.273812
I0428 20:06:55.334520  7412 solver.cpp:243]     Train net output #0: loss = 0.273812 (* 1 = 0.273812 loss)
I0428 20:06:55.334520  7412 sgd_solver.cpp:137] Iteration 33600, lr = 0.01
I0428 20:06:55.343521  7412 sgd_solver.cpp:169] scale layer:0.445420 0.466903 0.521475 0.560182 0.512997 0.486412 0.496346 0.491137 0.488818 0.491059 0.501129 0.484325 0.464517 0.436867 0.424518 0.391558 0.358387 0.324067 0.324534 1.008989 
I0428 20:06:55.345521  7412 sgd_solver.cpp:200] weight diff/data:0.017728 0.019394 0.008748 0.010964 0.010353 0.015333 0.050478 0.043153 0.013737 0.017325 0.015275 0.018217 0.022579 0.017809 0.019822 0.012067 0.017304 0.047108 0.023961 0.009120 0.004651 
I0428 20:07:19.983930  7412 solver.cpp:224] Iteration 33700 (4.05695 iter/s, 24.6491s/100 iters), loss = 0.169133
I0428 20:07:19.983930  7412 solver.cpp:243]     Train net output #0: loss = 0.169133 (* 1 = 0.169133 loss)
I0428 20:07:19.983930  7412 sgd_solver.cpp:137] Iteration 33700, lr = 0.01
I0428 20:07:19.992931  7412 sgd_solver.cpp:169] scale layer:0.445562 0.470718 0.522784 0.557877 0.515849 0.486303 0.496444 0.490817 0.489704 0.490939 0.500641 0.484973 0.464349 0.437027 0.423815 0.391691 0.358946 0.324471 0.324539 1.008095 
I0428 20:07:19.994931  7412 sgd_solver.cpp:200] weight diff/data:0.007964 0.012390 0.012583 0.023495 0.020522 0.013423 0.019379 0.013947 0.009187 0.029421 0.017487 0.012839 0.013830 0.010073 0.077885 0.014590 0.017975 0.019131 0.010008 0.009060 0.001330 
I0428 20:07:44.665343  7412 solver.cpp:224] Iteration 33800 (4.0516 iter/s, 24.6816s/100 iters), loss = 0.249152
I0428 20:07:44.665343  7412 solver.cpp:243]     Train net output #0: loss = 0.249152 (* 1 = 0.249152 loss)
I0428 20:07:44.665343  7412 sgd_solver.cpp:137] Iteration 33800, lr = 0.01
I0428 20:07:44.674342  7412 sgd_solver.cpp:169] scale layer:0.441692 0.469882 0.524431 0.559009 0.512206 0.485906 0.495388 0.490885 0.488139 0.489895 0.500832 0.485103 0.464706 0.436223 0.423810 0.390407 0.358820 0.323629 0.324227 1.011921 
I0428 20:07:44.676343  7412 sgd_solver.cpp:200] weight diff/data:0.010129 0.010584 0.010720 0.011122 0.031229 0.016660 0.016725 0.017051 0.018355 0.012487 0.028632 0.015979 0.014747 0.026600 0.015294 0.030471 0.042059 0.011502 0.015885 0.010892 0.001667 
I0428 20:08:09.313752  7412 solver.cpp:224] Iteration 33900 (4.05707 iter/s, 24.6484s/100 iters), loss = 0.264757
I0428 20:08:09.313752  7412 solver.cpp:243]     Train net output #0: loss = 0.264757 (* 1 = 0.264757 loss)
I0428 20:08:09.313752  7412 sgd_solver.cpp:137] Iteration 33900, lr = 0.01
I0428 20:08:09.321753  7412 sgd_solver.cpp:169] scale layer:0.434240 0.471857 0.523924 0.560590 0.514952 0.486444 0.496176 0.490221 0.489051 0.490933 0.500628 0.485662 0.464986 0.436437 0.424368 0.391217 0.358191 0.323255 0.323784 1.012332 
I0428 20:08:09.323752  7412 sgd_solver.cpp:200] weight diff/data:0.012220 0.010178 0.014522 0.023793 0.021441 0.012687 0.015699 0.012259 0.019089 0.015216 0.016755 0.021409 0.012719 0.011607 0.022910 0.013659 0.011008 0.018033 0.021265 0.012151 0.001171 
I0428 20:08:32.724092 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:33.707147  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_34000.caffemodel
I0428 20:08:33.715147  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_34000.solverstate
I0428 20:08:33.718148  7412 solver.cpp:336] Iteration 34000, Testing net (#0)
I0428 20:08:42.508651 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:42.870671  7412 solver.cpp:403]     Test net output #0: accuracy = 0.802
I0428 20:08:42.870671  7412 solver.cpp:403]     Test net output #1: loss = 0.687239 (* 1 = 0.687239 loss)
I0428 20:08:43.107686  7412 solver.cpp:224] Iteration 34000 (2.95899 iter/s, 33.7953s/100 iters), loss = 0.2782
I0428 20:08:43.107686  7412 solver.cpp:243]     Train net output #0: loss = 0.278199 (* 1 = 0.278199 loss)
I0428 20:08:43.108685  7412 sgd_solver.cpp:137] Iteration 34000, lr = 0.01
I0428 20:08:43.117686  7412 sgd_solver.cpp:169] scale layer:0.443292 0.468117 0.522519 0.559716 0.516353 0.485946 0.495674 0.490097 0.489207 0.490665 0.500912 0.486548 0.466012 0.436117 0.424600 0.390678 0.358832 0.322306 0.323614 1.011396 
I0428 20:08:43.118685  7412 sgd_solver.cpp:200] weight diff/data:0.007996 0.012771 0.012938 0.011863 0.011234 0.012372 0.013199 0.018369 0.016245 0.016127 0.018031 0.015440 0.025034 0.015692 0.016616 0.013964 0.011567 0.011495 0.015637 0.019772 0.001979 
I0428 20:09:07.790096  7412 solver.cpp:224] Iteration 34100 (4.05147 iter/s, 24.6824s/100 iters), loss = 0.261083
I0428 20:09:07.790096  7412 solver.cpp:243]     Train net output #0: loss = 0.261082 (* 1 = 0.261082 loss)
I0428 20:09:07.790096  7412 sgd_solver.cpp:137] Iteration 34100, lr = 0.01
I0428 20:09:07.800097  7412 sgd_solver.cpp:169] scale layer:0.445122 0.470828 0.522464 0.559134 0.516665 0.487642 0.496902 0.490104 0.489667 0.491150 0.501320 0.486788 0.466462 0.436193 0.424369 0.391889 0.359333 0.323698 0.323707 1.011863 
I0428 20:09:07.802098  7412 sgd_solver.cpp:200] weight diff/data:0.008134 0.011312 0.011224 0.016692 0.035403 0.029859 0.015808 0.011092 0.026274 0.012877 0.028119 0.018730 0.029008 0.012885 0.018713 0.014616 0.013173 0.017951 0.019686 0.010384 0.001870 
I0428 20:09:32.426506  7412 solver.cpp:224] Iteration 34200 (4.05893 iter/s, 24.637s/100 iters), loss = 0.227454
I0428 20:09:32.427506  7412 solver.cpp:243]     Train net output #0: loss = 0.227454 (* 1 = 0.227454 loss)
I0428 20:09:32.427506  7412 sgd_solver.cpp:137] Iteration 34200, lr = 0.01
I0428 20:09:32.435506  7412 sgd_solver.cpp:169] scale layer:0.443376 0.469836 0.525044 0.561464 0.515274 0.487042 0.495864 0.490386 0.489962 0.491740 0.501253 0.487010 0.466715 0.436852 0.424751 0.392091 0.359205 0.323377 0.323368 1.011908 
I0428 20:09:32.437506  7412 sgd_solver.cpp:200] weight diff/data:0.027623 0.015889 0.016494 0.012604 0.012094 0.017196 0.017996 0.014330 0.014631 0.011165 0.011713 0.015899 0.017894 0.012523 0.013766 0.014095 0.011195 0.012003 0.015841 0.013074 0.001339 
I0428 20:09:57.071915  7412 solver.cpp:224] Iteration 34300 (4.05748 iter/s, 24.6458s/100 iters), loss = 0.148783
I0428 20:09:57.072916  7412 solver.cpp:243]     Train net output #0: loss = 0.148783 (* 1 = 0.148783 loss)
I0428 20:09:57.072916  7412 sgd_solver.cpp:137] Iteration 34300, lr = 0.01
I0428 20:09:57.081917  7412 sgd_solver.cpp:169] scale layer:0.443166 0.471802 0.525053 0.561909 0.511603 0.486779 0.496918 0.491627 0.490286 0.491644 0.502631 0.487402 0.466746 0.436828 0.424157 0.391208 0.359274 0.322733 0.323199 1.010506 
I0428 20:09:57.083916  7412 sgd_solver.cpp:200] weight diff/data:0.013956 0.017177 0.021809 0.019611 0.024112 0.038160 0.012325 0.052141 0.011919 0.011190 0.078931 0.015488 0.010898 0.011370 0.020619 0.018320 0.015598 0.028832 0.026796 0.031939 0.004919 
I0428 20:10:20.483254 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:21.703325  7412 solver.cpp:224] Iteration 34400 (4.05978 iter/s, 24.6319s/100 iters), loss = 0.2279
I0428 20:10:21.704324  7412 solver.cpp:243]     Train net output #0: loss = 0.2279 (* 1 = 0.2279 loss)
I0428 20:10:21.704324  7412 sgd_solver.cpp:137] Iteration 34400, lr = 0.01
I0428 20:10:21.712324  7412 sgd_solver.cpp:169] scale layer:0.442654 0.470540 0.522946 0.558708 0.512312 0.486301 0.497247 0.491140 0.491731 0.491222 0.502355 0.486624 0.466376 0.436754 0.424357 0.391147 0.359100 0.322594 0.322867 1.010339 
I0428 20:10:21.714325  7412 sgd_solver.cpp:200] weight diff/data:0.015489 0.013262 0.012766 0.019156 0.011496 0.036489 0.013428 0.019826 0.012927 0.014171 0.011083 0.012508 0.025621 0.014991 0.013063 0.014134 0.021895 0.011252 0.017809 0.013514 0.002348 
I0428 20:10:46.126721  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_34500.caffemodel
I0428 20:10:46.135721  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_34500.solverstate
I0428 20:10:46.138722  7412 solver.cpp:336] Iteration 34500, Testing net (#0)
I0428 20:10:54.918225 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:55.280244  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7977
I0428 20:10:55.280244  7412 solver.cpp:403]     Test net output #1: loss = 0.712618 (* 1 = 0.712618 loss)
I0428 20:10:55.512259  7412 solver.cpp:224] Iteration 34500 (2.95782 iter/s, 33.8087s/100 iters), loss = 0.19106
I0428 20:10:55.512259  7412 solver.cpp:243]     Train net output #0: loss = 0.19106 (* 1 = 0.19106 loss)
I0428 20:10:55.512259  7412 sgd_solver.cpp:137] Iteration 34500, lr = 0.01
I0428 20:10:55.520258  7412 sgd_solver.cpp:169] scale layer:0.444179 0.473784 0.522582 0.556759 0.507250 0.487825 0.496834 0.492200 0.491440 0.491633 0.502224 0.485715 0.464906 0.437365 0.424887 0.390625 0.359369 0.323411 0.322227 1.009861 
I0428 20:10:55.522258  7412 sgd_solver.cpp:200] weight diff/data:0.006294 0.008944 0.012906 0.010128 0.010173 0.011177 0.021969 0.016775 0.014516 0.015764 0.015256 0.025951 0.027800 0.011405 0.024225 0.014488 0.016707 0.013849 0.016888 0.019549 0.001701 
I0428 20:11:20.169668  7412 solver.cpp:224] Iteration 34600 (4.05539 iter/s, 24.6585s/100 iters), loss = 0.25312
I0428 20:11:20.169668  7412 solver.cpp:243]     Train net output #0: loss = 0.25312 (* 1 = 0.25312 loss)
I0428 20:11:20.169668  7412 sgd_solver.cpp:137] Iteration 34600, lr = 0.01
I0428 20:11:20.178669  7412 sgd_solver.cpp:169] scale layer:0.447439 0.471925 0.523624 0.559919 0.512455 0.486936 0.496637 0.491654 0.491119 0.491117 0.502046 0.485454 0.466103 0.436565 0.425161 0.390526 0.359604 0.322297 0.322119 1.007527 
I0428 20:11:20.180670  7412 sgd_solver.cpp:200] weight diff/data:0.015363 0.011554 0.014698 0.090498 0.009728 0.025120 0.038551 0.013566 0.011152 0.030417 0.017262 0.014487 0.012688 0.018658 0.013160 0.015396 0.012378 0.013459 0.018721 0.044988 0.001783 
I0428 20:11:44.849081  7412 solver.cpp:224] Iteration 34700 (4.05194 iter/s, 24.6796s/100 iters), loss = 0.239271
I0428 20:11:44.849081  7412 solver.cpp:243]     Train net output #0: loss = 0.239271 (* 1 = 0.239271 loss)
I0428 20:11:44.849081  7412 sgd_solver.cpp:137] Iteration 34700, lr = 0.01
I0428 20:11:44.858080  7412 sgd_solver.cpp:169] scale layer:0.448594 0.473924 0.525417 0.558895 0.513096 0.486090 0.498166 0.490966 0.491232 0.491159 0.502587 0.484597 0.466165 0.436346 0.424950 0.390791 0.359433 0.321916 0.321967 1.007599 
I0428 20:11:44.860080  7412 sgd_solver.cpp:200] weight diff/data:0.014538 0.014147 0.015427 0.008831 0.017696 0.010840 0.019606 0.018075 0.012938 0.020557 0.011651 0.021269 0.013323 0.012041 0.012972 0.021064 0.014119 0.023325 0.010459 0.009542 0.001866 
I0428 20:12:08.298421 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:12:09.524492  7412 solver.cpp:224] Iteration 34800 (4.05248 iter/s, 24.6763s/100 iters), loss = 0.306401
I0428 20:12:09.524492  7412 solver.cpp:243]     Train net output #0: loss = 0.306401 (* 1 = 0.306401 loss)
I0428 20:12:09.524492  7412 sgd_solver.cpp:137] Iteration 34800, lr = 0.01
I0428 20:12:09.534492  7412 sgd_solver.cpp:169] scale layer:0.449086 0.471564 0.525202 0.559402 0.511045 0.487516 0.498016 0.492346 0.491476 0.490662 0.501375 0.484684 0.465486 0.436613 0.424685 0.390246 0.359845 0.322565 0.321844 1.009030 
I0428 20:12:09.536492  7412 sgd_solver.cpp:200] weight diff/data:0.015031 0.009440 0.018240 0.014703 0.011146 0.014191 0.015685 0.011162 0.031334 0.012397 0.015884 0.012824 0.011912 0.024410 0.016790 0.015833 0.036526 0.016646 0.011774 0.015095 0.001036 
I0428 20:12:34.219903  7412 solver.cpp:224] Iteration 34900 (4.04928 iter/s, 24.6958s/100 iters), loss = 0.158032
I0428 20:12:34.219903  7412 solver.cpp:243]     Train net output #0: loss = 0.158032 (* 1 = 0.158032 loss)
I0428 20:12:34.219903  7412 sgd_solver.cpp:137] Iteration 34900, lr = 0.01
I0428 20:12:34.228904  7412 sgd_solver.cpp:169] scale layer:0.451202 0.471964 0.525500 0.560612 0.506258 0.487480 0.498795 0.491046 0.491717 0.490942 0.501716 0.484550 0.465311 0.436232 0.424734 0.390571 0.359809 0.322917 0.321445 1.010399 
I0428 20:12:34.230904  7412 sgd_solver.cpp:200] weight diff/data:0.007085 0.011307 0.015668 0.009694 0.010561 0.021347 0.012997 0.016271 0.017937 0.018660 0.010419 0.014619 0.011992 0.018389 0.012791 0.016287 0.015517 0.011876 0.024989 0.010077 0.001318 
I0428 20:12:58.626299  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_35000.caffemodel
I0428 20:12:58.635300  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_35000.solverstate
I0428 20:12:58.638300  7412 solver.cpp:336] Iteration 35000, Testing net (#0)
I0428 20:13:07.429803 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:13:07.794824  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8064
I0428 20:13:07.794824  7412 solver.cpp:403]     Test net output #1: loss = 0.675715 (* 1 = 0.675715 loss)
I0428 20:13:08.028837  7412 solver.cpp:224] Iteration 35000 (2.95771 iter/s, 33.81s/100 iters), loss = 0.310822
I0428 20:13:08.028837  7412 solver.cpp:243]     Train net output #0: loss = 0.310821 (* 1 = 0.310821 loss)
I0428 20:13:08.028837  7412 sgd_solver.cpp:137] Iteration 35000, lr = 0.01
I0428 20:13:08.037838  7412 sgd_solver.cpp:169] scale layer:0.443755 0.465980 0.523198 0.560948 0.508507 0.486407 0.499069 0.491055 0.490986 0.490551 0.502350 0.485907 0.466493 0.437515 0.425048 0.390468 0.359440 0.321797 0.321322 1.009989 
I0428 20:13:08.038838  7412 sgd_solver.cpp:200] weight diff/data:0.009083 0.013322 0.010489 0.025449 0.024162 0.011897 0.045254 0.018160 0.011604 0.013399 0.012331 0.012479 0.014288 0.329488 0.017697 0.011223 0.013932 0.020564 0.021457 0.013571 0.000971 
I0428 20:13:32.682247  7412 solver.cpp:224] Iteration 35100 (4.05624 iter/s, 24.6534s/100 iters), loss = 0.225667
I0428 20:13:32.682247  7412 solver.cpp:243]     Train net output #0: loss = 0.225667 (* 1 = 0.225667 loss)
I0428 20:13:32.682247  7412 sgd_solver.cpp:137] Iteration 35100, lr = 0.01
I0428 20:13:32.691248  7412 sgd_solver.cpp:169] scale layer:0.444928 0.469010 0.522843 0.558831 0.510427 0.485461 0.498910 0.491154 0.490705 0.491253 0.502349 0.486005 0.466693 0.438231 0.425416 0.390279 0.358610 0.321342 0.320706 1.010485 
I0428 20:13:32.693248  7412 sgd_solver.cpp:200] weight diff/data:0.008121 0.009817 0.011878 0.090088 0.100815 0.028863 0.016190 0.014535 0.015792 0.021500 0.047645 0.035106 0.019116 0.011805 0.021705 0.014609 0.016236 0.027631 0.016520 0.010673 0.001186 
I0428 20:13:56.166591 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:13:57.388660  7412 solver.cpp:224] Iteration 35200 (4.04734 iter/s, 24.7076s/100 iters), loss = 0.247859
I0428 20:13:57.389660  7412 solver.cpp:243]     Train net output #0: loss = 0.247859 (* 1 = 0.247859 loss)
I0428 20:13:57.389660  7412 sgd_solver.cpp:137] Iteration 35200, lr = 0.01
I0428 20:13:57.397661  7412 sgd_solver.cpp:169] scale layer:0.440539 0.471319 0.525070 0.559891 0.508536 0.485806 0.498279 0.492170 0.489823 0.491021 0.501653 0.485888 0.466074 0.437696 0.425499 0.391591 0.358336 0.320697 0.320825 1.010050 
I0428 20:13:57.399662  7412 sgd_solver.cpp:200] weight diff/data:0.014253 0.026103 0.011743 0.034640 0.013369 0.015020 0.024882 0.015795 0.012313 0.016498 0.029255 0.011776 0.052901 0.012009 0.012846 0.014802 0.010995 0.014999 0.010318 0.012987 0.001225 
I0428 20:14:22.022069  7412 solver.cpp:224] Iteration 35300 (4.05955 iter/s, 24.6333s/100 iters), loss = 0.229147
I0428 20:14:22.022069  7412 solver.cpp:243]     Train net output #0: loss = 0.229147 (* 1 = 0.229147 loss)
I0428 20:14:22.022069  7412 sgd_solver.cpp:137] Iteration 35300, lr = 0.01
I0428 20:14:22.031070  7412 sgd_solver.cpp:169] scale layer:0.444258 0.471414 0.525000 0.560572 0.508915 0.486842 0.497987 0.491969 0.491272 0.490484 0.502447 0.485766 0.466019 0.437940 0.425163 0.391504 0.359180 0.320876 0.321019 1.006941 
I0428 20:14:22.032070  7412 sgd_solver.cpp:200] weight diff/data:0.008029 0.207695 0.040987 0.011251 0.013204 0.014503 0.022463 0.015846 0.012216 0.036155 0.020218 0.012914 0.013579 0.014935 0.015191 0.019985 0.016197 0.014375 0.024480 0.025266 0.001594 
I0428 20:14:46.643478  7412 solver.cpp:224] Iteration 35400 (4.06138 iter/s, 24.6222s/100 iters), loss = 0.282958
I0428 20:14:46.644479  7412 solver.cpp:243]     Train net output #0: loss = 0.282957 (* 1 = 0.282957 loss)
I0428 20:14:46.644479  7412 sgd_solver.cpp:137] Iteration 35400, lr = 0.01
I0428 20:14:46.652478  7412 sgd_solver.cpp:169] scale layer:0.444020 0.467184 0.521791 0.561311 0.512217 0.486518 0.497592 0.492482 0.491641 0.491498 0.502519 0.486928 0.466245 0.438249 0.425075 0.390329 0.358366 0.320749 0.320963 1.005908 
I0428 20:14:46.654479  7412 sgd_solver.cpp:200] weight diff/data:0.010543 0.011136 0.008909 0.017133 0.047180 0.011925 0.042442 0.019480 0.013001 0.016197 0.013466 0.012861 0.013971 0.012163 0.010785 0.015111 0.015235 0.027746 0.012614 0.008480 0.001076 
I0428 20:15:11.023872  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_35500.caffemodel
I0428 20:15:11.032873  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_35500.solverstate
I0428 20:15:11.035873  7412 solver.cpp:336] Iteration 35500, Testing net (#0)
I0428 20:15:19.803375 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:15:20.167395  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8073
I0428 20:15:20.167395  7412 solver.cpp:403]     Test net output #1: loss = 0.663094 (* 1 = 0.663094 loss)
I0428 20:15:20.402410  7412 solver.cpp:224] Iteration 35500 (2.96217 iter/s, 33.7591s/100 iters), loss = 0.253379
I0428 20:15:20.402410  7412 solver.cpp:243]     Train net output #0: loss = 0.253379 (* 1 = 0.253379 loss)
I0428 20:15:20.402410  7412 sgd_solver.cpp:137] Iteration 35500, lr = 0.01
I0428 20:15:20.410409  7412 sgd_solver.cpp:169] scale layer:0.446859 0.470456 0.523590 0.560948 0.511181 0.485942 0.498532 0.492485 0.490958 0.490439 0.503010 0.487393 0.465772 0.437407 0.425531 0.390154 0.358665 0.320357 0.320481 1.007497 
I0428 20:15:20.412410  7412 sgd_solver.cpp:200] weight diff/data:0.008890 0.012444 0.020493 0.016912 0.009253 0.017666 0.012484 0.012025 0.024297 0.013797 0.015871 0.011778 0.014534 0.020467 0.017073 0.014991 0.012866 0.020611 0.015938 0.037494 0.001542 
I0428 20:15:43.842749 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:15:45.059819  7412 solver.cpp:224] Iteration 35600 (4.05546 iter/s, 24.6581s/100 iters), loss = 0.280712
I0428 20:15:45.059819  7412 solver.cpp:243]     Train net output #0: loss = 0.280712 (* 1 = 0.280712 loss)
I0428 20:15:45.059819  7412 sgd_solver.cpp:137] Iteration 35600, lr = 0.01
I0428 20:15:45.068819  7412 sgd_solver.cpp:169] scale layer:0.445896 0.467529 0.523330 0.561143 0.514068 0.486197 0.497893 0.492228 0.490155 0.490590 0.502448 0.487595 0.465425 0.436965 0.425092 0.389847 0.358269 0.320996 0.320754 1.007684 
I0428 20:15:45.070819  7412 sgd_solver.cpp:200] weight diff/data:0.012549 0.020453 0.011584 0.012312 0.019131 0.023827 0.016074 0.023606 0.018389 0.018551 0.012353 0.012085 0.013090 0.013043 0.020425 0.013546 0.029812 0.011091 0.016346 0.013616 0.001734 
I0428 20:16:09.709229  7412 solver.cpp:224] Iteration 35700 (4.05679 iter/s, 24.65s/100 iters), loss = 0.24825
I0428 20:16:09.709229  7412 solver.cpp:243]     Train net output #0: loss = 0.248249 (* 1 = 0.248249 loss)
I0428 20:16:09.709229  7412 sgd_solver.cpp:137] Iteration 35700, lr = 0.01
I0428 20:16:09.718230  7412 sgd_solver.cpp:169] scale layer:0.450952 0.463772 0.525252 0.563148 0.512884 0.487253 0.498207 0.491536 0.490465 0.490934 0.502655 0.487300 0.465620 0.438040 0.425956 0.389972 0.359239 0.321454 0.320375 1.006007 
I0428 20:16:09.720229  7412 sgd_solver.cpp:200] weight diff/data:0.012934 0.012538 0.022159 0.013876 0.016957 0.033284 0.015444 0.030747 0.013087 0.011482 0.012562 0.018481 0.012243 0.023388 0.013634 0.014246 0.017152 0.014744 0.013880 0.014314 0.001018 
I0428 20:16:34.363639  7412 solver.cpp:224] Iteration 35800 (4.05598 iter/s, 24.655s/100 iters), loss = 0.266418
I0428 20:16:34.363639  7412 solver.cpp:243]     Train net output #0: loss = 0.266417 (* 1 = 0.266417 loss)
I0428 20:16:34.363639  7412 sgd_solver.cpp:137] Iteration 35800, lr = 0.01
I0428 20:16:34.372639  7412 sgd_solver.cpp:169] scale layer:0.446090 0.460736 0.524192 0.562553 0.514260 0.485571 0.499012 0.492200 0.490858 0.490340 0.503142 0.487237 0.465871 0.438313 0.425572 0.390085 0.358233 0.320551 0.319907 1.005914 
I0428 20:16:34.375640  7412 sgd_solver.cpp:200] weight diff/data:0.004639 0.012615 0.009146 0.011916 0.011884 0.019637 0.018443 0.020134 0.014933 0.014428 0.107094 0.012892 0.011144 0.009953 0.014223 0.014460 0.014274 0.013724 0.011685 0.012534 0.000905 
I0428 20:16:59.017050  7412 solver.cpp:224] Iteration 35900 (4.05621 iter/s, 24.6536s/100 iters), loss = 0.25252
I0428 20:16:59.017050  7412 solver.cpp:243]     Train net output #0: loss = 0.25252 (* 1 = 0.25252 loss)
I0428 20:16:59.017050  7412 sgd_solver.cpp:137] Iteration 35900, lr = 0.01
I0428 20:16:59.026049  7412 sgd_solver.cpp:169] scale layer:0.442146 0.465201 0.525903 0.562455 0.512591 0.485405 0.497905 0.493636 0.490401 0.490662 0.502710 0.487866 0.465821 0.438358 0.424926 0.390194 0.357910 0.320363 0.319609 1.007077 
I0428 20:16:59.028050  7412 sgd_solver.cpp:200] weight diff/data:0.008823 0.008696 0.009074 0.010786 0.025583 0.016761 0.013341 0.014154 0.014396 0.047254 0.022327 0.016149 0.013921 0.015983 0.080399 0.027963 0.014053 0.020582 0.138323 0.013840 0.001847 
I0428 20:17:22.476392 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:17:23.463448  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_36000.caffemodel
I0428 20:17:23.471448  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_36000.solverstate
I0428 20:17:23.474448  7412 solver.cpp:336] Iteration 36000, Testing net (#0)
I0428 20:17:32.260951 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:17:32.622972  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7983
I0428 20:17:32.622972  7412 solver.cpp:403]     Test net output #1: loss = 0.707521 (* 1 = 0.707521 loss)
I0428 20:17:32.856986  7412 solver.cpp:224] Iteration 36000 (2.95504 iter/s, 33.8404s/100 iters), loss = 0.164396
I0428 20:17:32.856986  7412 solver.cpp:243]     Train net output #0: loss = 0.164396 (* 1 = 0.164396 loss)
I0428 20:17:32.856986  7412 sgd_solver.cpp:137] Iteration 36000, lr = 0.01
I0428 20:17:32.864985  7412 sgd_solver.cpp:169] scale layer:0.441301 0.462498 0.523828 0.562597 0.511276 0.487175 0.497148 0.494465 0.490556 0.492012 0.502322 0.487248 0.465486 0.437518 0.426124 0.391273 0.357908 0.319939 0.319262 1.007928 
I0428 20:17:32.866986  7412 sgd_solver.cpp:200] weight diff/data:0.006877 0.015788 0.013766 0.008962 0.015427 0.012172 0.030736 0.017208 0.010389 0.016544 0.014677 0.013701 0.069054 0.014919 0.025989 0.013654 0.054286 0.014139 0.012499 0.012596 0.001196 
I0428 20:17:57.543396  7412 solver.cpp:224] Iteration 36100 (4.05062 iter/s, 24.6876s/100 iters), loss = 0.313205
I0428 20:17:57.544397  7412 solver.cpp:243]     Train net output #0: loss = 0.313205 (* 1 = 0.313205 loss)
I0428 20:17:57.544397  7412 sgd_solver.cpp:137] Iteration 36100, lr = 0.01
I0428 20:17:57.552397  7412 sgd_solver.cpp:169] scale layer:0.447292 0.462048 0.525039 0.562489 0.513702 0.486913 0.498180 0.493497 0.491156 0.492238 0.502686 0.486419 0.464935 0.437855 0.425364 0.391353 0.357657 0.318925 0.318630 1.007790 
I0428 20:17:57.554397  7412 sgd_solver.cpp:200] weight diff/data:0.008992 0.010134 0.022222 0.012436 0.016664 0.018962 0.017617 0.013859 0.012735 0.018212 0.013111 0.014676 0.013222 0.017692 0.031683 0.016241 0.023398 0.018666 0.013818 0.010662 0.000878 
I0428 20:18:22.179806  7412 solver.cpp:224] Iteration 36200 (4.059 iter/s, 24.6366s/100 iters), loss = 0.299044
I0428 20:18:22.179806  7412 solver.cpp:243]     Train net output #0: loss = 0.299044 (* 1 = 0.299044 loss)
I0428 20:18:22.179806  7412 sgd_solver.cpp:137] Iteration 36200, lr = 0.01
I0428 20:18:22.188807  7412 sgd_solver.cpp:169] scale layer:0.447106 0.464565 0.523640 0.560806 0.514879 0.486644 0.497502 0.492771 0.490417 0.490318 0.502978 0.486765 0.465046 0.438184 0.424223 0.391340 0.357488 0.319124 0.318252 1.007786 
I0428 20:18:22.190806  7412 sgd_solver.cpp:200] weight diff/data:0.005920 0.018285 0.012759 0.051372 0.012845 0.047751 0.309213 0.077290 0.013153 0.013384 0.034339 0.016423 0.019978 0.010999 0.014422 0.013377 0.011260 0.040977 0.012158 0.009861 0.001040 
I0428 20:18:46.813215  7412 solver.cpp:224] Iteration 36300 (4.0594 iter/s, 24.6342s/100 iters), loss = 0.2466
I0428 20:18:46.814215  7412 solver.cpp:243]     Train net output #0: loss = 0.246599 (* 1 = 0.246599 loss)
I0428 20:18:46.814215  7412 sgd_solver.cpp:137] Iteration 36300, lr = 0.01
I0428 20:18:46.822216  7412 sgd_solver.cpp:169] scale layer:0.442079 0.465630 0.521651 0.560530 0.515807 0.486217 0.497621 0.493620 0.490338 0.490701 0.503111 0.486524 0.464799 0.437917 0.424598 0.390819 0.356825 0.318356 0.317582 1.008461 
I0428 20:18:46.824215  7412 sgd_solver.cpp:200] weight diff/data:0.011387 0.020263 0.013956 0.016678 0.009333 0.015182 0.013515 0.017089 0.016747 0.020511 0.010776 0.025248 0.133515 0.013744 0.012131 0.015762 0.013722 0.015545 0.020053 0.015986 0.001787 
I0428 20:19:10.241555 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:11.467625  7412 solver.cpp:224] Iteration 36400 (4.05609 iter/s, 24.6543s/100 iters), loss = 0.2589
I0428 20:19:11.467625  7412 solver.cpp:243]     Train net output #0: loss = 0.2589 (* 1 = 0.2589 loss)
I0428 20:19:11.467625  7412 sgd_solver.cpp:137] Iteration 36400, lr = 0.01
I0428 20:19:11.476625  7412 sgd_solver.cpp:169] scale layer:0.444025 0.463942 0.525106 0.564160 0.518084 0.485696 0.496525 0.492534 0.491772 0.491461 0.502741 0.485719 0.464602 0.437177 0.424985 0.391115 0.356644 0.317939 0.317007 1.008788 
I0428 20:19:11.478626  7412 sgd_solver.cpp:200] weight diff/data:0.007078 0.016109 0.018079 0.028692 0.012000 0.015457 0.014778 0.015244 0.013841 0.016417 0.011568 0.021231 0.015307 0.018570 0.014756 0.020697 0.012130 0.014554 0.013738 0.030613 0.001863 
I0428 20:19:35.911023  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_36500.caffemodel
I0428 20:19:35.920023  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_36500.solverstate
I0428 20:19:35.923023  7412 solver.cpp:336] Iteration 36500, Testing net (#0)
I0428 20:19:44.699525 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:45.063546  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8033
I0428 20:19:45.063546  7412 solver.cpp:403]     Test net output #1: loss = 0.679323 (* 1 = 0.679323 loss)
I0428 20:19:45.297560  7412 solver.cpp:224] Iteration 36500 (2.95588 iter/s, 33.8309s/100 iters), loss = 0.185672
I0428 20:19:45.297560  7412 solver.cpp:243]     Train net output #0: loss = 0.185672 (* 1 = 0.185672 loss)
I0428 20:19:45.297560  7412 sgd_solver.cpp:137] Iteration 36500, lr = 0.01
I0428 20:19:45.306560  7412 sgd_solver.cpp:169] scale layer:0.447667 0.467651 0.524375 0.563083 0.518466 0.486163 0.497393 0.492213 0.491546 0.490990 0.503190 0.486418 0.464273 0.437483 0.424962 0.390900 0.357241 0.318590 0.316571 1.008361 
I0428 20:19:45.307560  7412 sgd_solver.cpp:200] weight diff/data:0.004186 0.015583 0.011332 0.031144 0.011157 0.017928 0.012655 0.030853 0.013883 0.021121 0.010757 0.024056 0.012785 0.021233 0.019039 0.014167 0.011149 0.016882 0.019504 0.008913 0.000902 
I0428 20:20:09.974972  7412 solver.cpp:224] Iteration 36600 (4.05225 iter/s, 24.6776s/100 iters), loss = 0.288594
I0428 20:20:09.974972  7412 solver.cpp:243]     Train net output #0: loss = 0.288593 (* 1 = 0.288593 loss)
I0428 20:20:09.974972  7412 sgd_solver.cpp:137] Iteration 36600, lr = 0.01
I0428 20:20:09.983973  7412 sgd_solver.cpp:169] scale layer:0.442444 0.466833 0.522459 0.563454 0.517452 0.487725 0.496887 0.491679 0.492336 0.490985 0.503323 0.486813 0.464361 0.438149 0.424753 0.390464 0.357745 0.318339 0.316650 1.006753 
I0428 20:20:09.984972  7412 sgd_solver.cpp:200] weight diff/data:0.016110 0.009222 0.012670 0.034620 0.015431 0.012263 0.013626 0.011035 0.014171 0.011830 0.016625 0.017646 0.046484 0.014441 0.024846 0.012443 0.013563 0.014012 0.011319 0.099633 0.001174 
I0428 20:20:34.623381  7412 solver.cpp:224] Iteration 36700 (4.05702 iter/s, 24.6486s/100 iters), loss = 0.146615
I0428 20:20:34.623381  7412 solver.cpp:243]     Train net output #0: loss = 0.146615 (* 1 = 0.146615 loss)
I0428 20:20:34.623381  7412 sgd_solver.cpp:137] Iteration 36700, lr = 0.01
I0428 20:20:34.631381  7412 sgd_solver.cpp:169] scale layer:0.443003 0.464761 0.523862 0.561073 0.517135 0.487878 0.498117 0.491957 0.492787 0.491891 0.503729 0.486473 0.464814 0.438451 0.424979 0.390196 0.359105 0.319118 0.316385 1.008497 
I0428 20:20:34.633383  7412 sgd_solver.cpp:200] weight diff/data:0.038908 0.010527 0.009304 0.009801 0.008573 0.010004 0.009489 0.012391 0.012564 0.050542 0.011344 0.012782 0.010497 0.024153 0.011069 0.008535 0.010339 0.008026 0.010831 0.039404 0.001349 
I0428 20:20:58.083724 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:20:59.312793  7412 solver.cpp:224] Iteration 36800 (4.05015 iter/s, 24.6905s/100 iters), loss = 0.34658
I0428 20:20:59.312793  7412 solver.cpp:243]     Train net output #0: loss = 0.34658 (* 1 = 0.34658 loss)
I0428 20:20:59.312793  7412 sgd_solver.cpp:137] Iteration 36800, lr = 0.01
I0428 20:20:59.321794  7412 sgd_solver.cpp:169] scale layer:0.448036 0.466050 0.524973 0.560713 0.520614 0.486255 0.496905 0.491745 0.490796 0.492147 0.503870 0.486992 0.464874 0.437452 0.425171 0.391176 0.358599 0.319674 0.316384 1.008510 
I0428 20:20:59.323794  7412 sgd_solver.cpp:200] weight diff/data:0.006633 0.012181 0.039725 0.011039 0.011079 0.010567 0.022460 0.018307 0.010014 0.012879 0.010415 0.033345 0.016261 0.020363 0.011257 0.012551 0.015658 0.012762 0.023233 0.011876 0.001299 
I0428 20:21:23.991205  7412 solver.cpp:224] Iteration 36900 (4.05204 iter/s, 24.6789s/100 iters), loss = 0.243575
I0428 20:21:23.991205  7412 solver.cpp:243]     Train net output #0: loss = 0.243575 (* 1 = 0.243575 loss)
I0428 20:21:23.991205  7412 sgd_solver.cpp:137] Iteration 36900, lr = 0.01
I0428 20:21:24.000205  7412 sgd_solver.cpp:169] scale layer:0.450085 0.468250 0.523669 0.563677 0.520110 0.487178 0.497386 0.490768 0.492029 0.492189 0.503305 0.487475 0.464281 0.438096 0.424050 0.390334 0.357889 0.319357 0.315677 1.010760 
I0428 20:21:24.002205  7412 sgd_solver.cpp:200] weight diff/data:0.006267 0.014232 0.024940 0.009419 0.012253 0.084682 0.012726 0.021258 0.020850 0.013429 0.016889 0.027261 0.021489 0.012670 0.015591 0.018435 0.017706 0.009751 0.011389 0.009752 0.000690 
I0428 20:21:48.395601  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_37000.caffemodel
I0428 20:21:48.403601  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_37000.solverstate
I0428 20:21:48.406601  7412 solver.cpp:336] Iteration 37000, Testing net (#0)
I0428 20:21:57.174103 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:21:57.537123  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8026
I0428 20:21:57.537123  7412 solver.cpp:403]     Test net output #1: loss = 0.688712 (* 1 = 0.688712 loss)
I0428 20:21:57.771137  7412 solver.cpp:224] Iteration 37000 (2.96027 iter/s, 33.7808s/100 iters), loss = 0.261926
I0428 20:21:57.771137  7412 solver.cpp:243]     Train net output #0: loss = 0.261926 (* 1 = 0.261926 loss)
I0428 20:21:57.771137  7412 sgd_solver.cpp:137] Iteration 37000, lr = 0.01
I0428 20:21:57.779137  7412 sgd_solver.cpp:169] scale layer:0.446378 0.466326 0.522204 0.562253 0.519565 0.487370 0.498142 0.492034 0.492933 0.493070 0.503366 0.486716 0.464632 0.438243 0.425006 0.391116 0.358272 0.319363 0.315790 1.011257 
I0428 20:21:57.781137  7412 sgd_solver.cpp:200] weight diff/data:0.008451 0.013140 0.065572 0.009444 0.010595 0.160188 0.012074 0.012758 0.022187 0.017831 0.011566 0.027678 0.011103 0.010205 0.021463 0.014020 0.011596 0.015607 0.019153 0.010545 0.001302 
I0428 20:22:22.423547  7412 solver.cpp:224] Iteration 37100 (4.05637 iter/s, 24.6526s/100 iters), loss = 0.222903
I0428 20:22:22.423547  7412 solver.cpp:243]     Train net output #0: loss = 0.222903 (* 1 = 0.222903 loss)
I0428 20:22:22.423547  7412 sgd_solver.cpp:137] Iteration 37100, lr = 0.01
I0428 20:22:22.432548  7412 sgd_solver.cpp:169] scale layer:0.452737 0.470802 0.521243 0.562751 0.517994 0.487066 0.499805 0.492000 0.492921 0.493140 0.503517 0.486670 0.463587 0.437803 0.424966 0.391054 0.358404 0.318427 0.315783 1.010289 
I0428 20:22:22.434547  7412 sgd_solver.cpp:200] weight diff/data:0.009108 0.010297 0.017468 0.008802 0.013473 0.012262 0.011858 0.012013 0.017249 0.022518 0.024323 0.081778 0.022820 0.469393 0.021042 0.033782 0.017510 0.015205 0.014434 0.016428 0.001689 
I0428 20:22:45.851887 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:22:47.084957  7412 solver.cpp:224] Iteration 37200 (4.05479 iter/s, 24.6622s/100 iters), loss = 0.27724
I0428 20:22:47.084957  7412 solver.cpp:243]     Train net output #0: loss = 0.27724 (* 1 = 0.27724 loss)
I0428 20:22:47.084957  7412 sgd_solver.cpp:137] Iteration 37200, lr = 0.01
I0428 20:22:47.092958  7412 sgd_solver.cpp:169] scale layer:0.447691 0.466891 0.520313 0.561308 0.517653 0.486683 0.498622 0.492164 0.492599 0.494200 0.503720 0.487718 0.464016 0.437362 0.424716 0.391059 0.357451 0.317349 0.315303 1.010785 
I0428 20:22:47.094959  7412 sgd_solver.cpp:200] weight diff/data:0.018716 0.013766 0.012825 0.016185 0.012788 0.011931 0.013184 0.013351 0.016716 0.013790 0.013400 0.012410 0.016991 0.014027 0.014644 0.010821 0.012033 0.012752 0.017914 0.024258 0.001370 
I0428 20:23:11.805371  7412 solver.cpp:224] Iteration 37300 (4.04508 iter/s, 24.7214s/100 iters), loss = 0.221442
I0428 20:23:11.805371  7412 solver.cpp:243]     Train net output #0: loss = 0.221442 (* 1 = 0.221442 loss)
I0428 20:23:11.805371  7412 sgd_solver.cpp:137] Iteration 37300, lr = 0.01
I0428 20:23:11.814373  7412 sgd_solver.cpp:169] scale layer:0.453229 0.469256 0.523653 0.559164 0.516479 0.485635 0.497615 0.491829 0.491613 0.493442 0.504144 0.487265 0.463981 0.436133 0.424964 0.390487 0.357741 0.317635 0.315006 1.010285 
I0428 20:23:11.816372  7412 sgd_solver.cpp:200] weight diff/data:0.009441 0.020799 0.013375 0.014120 0.015606 0.014265 0.037177 0.012945 0.037761 0.025430 0.015543 0.016107 0.013027 0.013959 0.010326 0.019088 0.019932 0.014632 0.016269 0.013981 0.000899 
I0428 20:23:36.463783  7412 solver.cpp:224] Iteration 37400 (4.05539 iter/s, 24.6585s/100 iters), loss = 0.246864
I0428 20:23:36.463783  7412 solver.cpp:243]     Train net output #0: loss = 0.246864 (* 1 = 0.246864 loss)
I0428 20:23:36.463783  7412 sgd_solver.cpp:137] Iteration 37400, lr = 0.01
I0428 20:23:36.472782  7412 sgd_solver.cpp:169] scale layer:0.448964 0.470229 0.522916 0.559990 0.520393 0.486331 0.499151 0.492783 0.492532 0.493311 0.503968 0.486787 0.463438 0.436453 0.424805 0.390141 0.358625 0.318044 0.315171 1.008379 
I0428 20:23:36.474782  7412 sgd_solver.cpp:200] weight diff/data:0.011599 0.010136 0.010807 0.014016 0.027945 0.017702 0.020928 0.023108 0.013536 0.020613 0.012786 0.014002 0.011336 0.019434 0.014599 0.024892 0.017835 0.048138 0.013088 0.017343 0.001054 
I0428 20:24:00.886179  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_37500.caffemodel
I0428 20:24:00.894179  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_37500.solverstate
I0428 20:24:00.897179  7412 solver.cpp:336] Iteration 37500, Testing net (#0)
I0428 20:24:09.678681 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:24:10.043702  7412 solver.cpp:403]     Test net output #0: accuracy = 0.805
I0428 20:24:10.043702  7412 solver.cpp:403]     Test net output #1: loss = 0.68054 (* 1 = 0.68054 loss)
I0428 20:24:10.275717  7412 solver.cpp:224] Iteration 37500 (2.95746 iter/s, 33.8128s/100 iters), loss = 0.178784
I0428 20:24:10.275717  7412 solver.cpp:243]     Train net output #0: loss = 0.178784 (* 1 = 0.178784 loss)
I0428 20:24:10.275717  7412 sgd_solver.cpp:137] Iteration 37500, lr = 0.01
I0428 20:24:10.284716  7412 sgd_solver.cpp:169] scale layer:0.448817 0.471153 0.522407 0.564143 0.519371 0.486762 0.499876 0.493460 0.493610 0.493149 0.503901 0.486180 0.463841 0.436635 0.424993 0.389996 0.358398 0.319272 0.315230 1.010120 
I0428 20:24:10.286716  7412 sgd_solver.cpp:200] weight diff/data:0.006275 0.009212 0.032190 0.077680 0.010803 0.115555 0.011518 0.018954 0.056371 0.011602 0.029294 0.013560 0.015304 0.011032 0.014630 0.015968 0.012972 0.011363 0.016564 0.016067 0.001560 
I0428 20:24:33.729058 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:24:34.948127  7412 solver.cpp:224] Iteration 37600 (4.05307 iter/s, 24.6726s/100 iters), loss = 0.314733
I0428 20:24:34.948127  7412 solver.cpp:243]     Train net output #0: loss = 0.314733 (* 1 = 0.314733 loss)
I0428 20:24:34.948127  7412 sgd_solver.cpp:137] Iteration 37600, lr = 0.01
I0428 20:24:34.956127  7412 sgd_solver.cpp:169] scale layer:0.451103 0.468063 0.520987 0.564384 0.523742 0.486962 0.500018 0.493425 0.493448 0.493282 0.503659 0.486032 0.463391 0.436553 0.424216 0.391036 0.358868 0.319153 0.315277 1.008933 
I0428 20:24:34.959127  7412 sgd_solver.cpp:200] weight diff/data:0.007142 0.016923 0.014415 0.014026 0.013237 0.048376 0.018628 0.060751 0.014676 0.017010 0.022800 0.023094 0.015698 0.021508 0.062073 0.039876 0.013648 0.024495 0.022047 0.019357 0.001486 
I0428 20:24:59.596537  7412 solver.cpp:224] Iteration 37700 (4.05688 iter/s, 24.6495s/100 iters), loss = 0.142362
I0428 20:24:59.597537  7412 solver.cpp:243]     Train net output #0: loss = 0.142362 (* 1 = 0.142362 loss)
I0428 20:24:59.597537  7412 sgd_solver.cpp:137] Iteration 37700, lr = 0.01
I0428 20:24:59.605537  7412 sgd_solver.cpp:169] scale layer:0.451399 0.471954 0.520044 0.562714 0.523224 0.488294 0.500398 0.492501 0.491176 0.493736 0.504996 0.487293 0.462944 0.436676 0.425191 0.390803 0.359559 0.319158 0.315382 1.008614 
I0428 20:24:59.607537  7412 sgd_solver.cpp:200] weight diff/data:0.007975 0.011386 0.010453 0.011093 0.013344 0.019120 0.012806 0.011275 0.017664 0.072930 0.013414 0.022240 0.017982 0.017916 0.081988 0.012733 0.016123 0.011858 0.020966 0.024900 0.000973 
I0428 20:25:24.254947  7412 solver.cpp:224] Iteration 37800 (4.05549 iter/s, 24.6579s/100 iters), loss = 0.29797
I0428 20:25:24.254947  7412 solver.cpp:243]     Train net output #0: loss = 0.29797 (* 1 = 0.29797 loss)
I0428 20:25:24.254947  7412 sgd_solver.cpp:137] Iteration 37800, lr = 0.01
I0428 20:25:24.263947  7412 sgd_solver.cpp:169] scale layer:0.450532 0.470559 0.520878 0.564627 0.520733 0.489412 0.500653 0.491920 0.493077 0.493871 0.504948 0.487565 0.462558 0.437597 0.425177 0.391048 0.358685 0.319044 0.315090 1.008966 
I0428 20:25:24.265949  7412 sgd_solver.cpp:200] weight diff/data:0.005491 0.012593 0.016235 0.012157 0.024834 0.014574 0.204039 0.015133 0.021944 0.015291 0.019491 0.019169 0.036251 0.015970 0.013706 0.012625 0.014702 0.013522 0.013703 0.010334 0.001094 
I0428 20:25:48.941359  7412 solver.cpp:224] Iteration 37900 (4.05074 iter/s, 24.6868s/100 iters), loss = 0.238233
I0428 20:25:48.941359  7412 solver.cpp:243]     Train net output #0: loss = 0.238233 (* 1 = 0.238233 loss)
I0428 20:25:48.941359  7412 sgd_solver.cpp:137] Iteration 37900, lr = 0.01
I0428 20:25:48.949359  7412 sgd_solver.cpp:169] scale layer:0.444515 0.467206 0.521576 0.564851 0.520304 0.488918 0.501672 0.493201 0.494430 0.493865 0.505391 0.487849 0.462601 0.437633 0.425735 0.390443 0.358409 0.318824 0.315308 1.007412 
I0428 20:25:48.951359  7412 sgd_solver.cpp:200] weight diff/data:0.010313 0.027260 0.009308 0.013437 0.010735 0.010764 0.017608 0.115970 0.034378 0.017626 0.015744 0.011400 0.013640 0.030744 0.014949 0.015363 0.023015 0.015548 0.011513 0.012380 0.001688 
I0428 20:26:12.372699 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:13.354755  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_38000.caffemodel
I0428 20:26:13.363756  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_38000.solverstate
I0428 20:26:13.366756  7412 solver.cpp:336] Iteration 38000, Testing net (#0)
I0428 20:26:22.170259 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:22.534281  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8001
I0428 20:26:22.534281  7412 solver.cpp:403]     Test net output #1: loss = 0.672008 (* 1 = 0.672008 loss)
I0428 20:26:22.766294  7412 solver.cpp:224] Iteration 38000 (2.95634 iter/s, 33.8256s/100 iters), loss = 0.340989
I0428 20:26:22.766294  7412 solver.cpp:243]     Train net output #0: loss = 0.340989 (* 1 = 0.340989 loss)
I0428 20:26:22.766294  7412 sgd_solver.cpp:137] Iteration 38000, lr = 0.01
I0428 20:26:22.775295  7412 sgd_solver.cpp:169] scale layer:0.446259 0.466363 0.520330 0.565159 0.521158 0.489517 0.500360 0.492439 0.492415 0.494170 0.505905 0.487851 0.463657 0.437690 0.426402 0.390623 0.358388 0.318336 0.314916 1.007148 
I0428 20:26:22.776294  7412 sgd_solver.cpp:200] weight diff/data:0.008483 0.011357 0.009332 0.011533 0.029757 0.020448 0.010876 0.013862 0.013846 0.011187 0.011693 0.014100 0.014281 0.015935 0.012168 0.015328 0.018334 0.015118 0.013291 0.016224 0.000952 
I0428 20:26:47.425704  7412 solver.cpp:224] Iteration 38100 (4.05521 iter/s, 24.6597s/100 iters), loss = 0.188217
I0428 20:26:47.425704  7412 solver.cpp:243]     Train net output #0: loss = 0.188217 (* 1 = 0.188217 loss)
I0428 20:26:47.425704  7412 sgd_solver.cpp:137] Iteration 38100, lr = 0.01
I0428 20:26:47.434705  7412 sgd_solver.cpp:169] scale layer:0.444634 0.468158 0.520942 0.565485 0.518464 0.489037 0.499775 0.493679 0.491032 0.494210 0.506605 0.488214 0.463407 0.436924 0.425372 0.390471 0.358079 0.319037 0.314827 1.008829 
I0428 20:26:47.436705  7412 sgd_solver.cpp:200] weight diff/data:0.008683 0.024566 0.009545 0.015773 0.011272 0.009368 0.015651 0.011288 0.013857 0.011693 0.012824 0.014830 0.013083 0.014837 0.019321 0.013987 0.029914 0.012604 0.016405 0.017099 0.001098 
I0428 20:27:12.095115  7412 solver.cpp:224] Iteration 38200 (4.05364 iter/s, 24.6692s/100 iters), loss = 0.331338
I0428 20:27:12.095115  7412 solver.cpp:243]     Train net output #0: loss = 0.331337 (* 1 = 0.331337 loss)
I0428 20:27:12.095115  7412 sgd_solver.cpp:137] Iteration 38200, lr = 0.01
I0428 20:27:12.103116  7412 sgd_solver.cpp:169] scale layer:0.444552 0.470447 0.519463 0.564856 0.517461 0.489172 0.500810 0.493026 0.490334 0.494369 0.506201 0.488278 0.464058 0.437505 0.426052 0.390797 0.357807 0.318818 0.314836 1.008974 
I0428 20:27:12.105116  7412 sgd_solver.cpp:200] weight diff/data:0.017973 0.035974 0.015459 0.012318 0.034770 0.010351 0.019528 0.014588 0.010913 0.015811 0.012118 0.011319 0.010510 0.021993 0.015735 0.016132 0.011041 0.018671 0.013180 0.013175 0.001446 
I0428 20:27:36.759526  7412 solver.cpp:224] Iteration 38300 (4.05427 iter/s, 24.6653s/100 iters), loss = 0.218256
I0428 20:27:36.759526  7412 solver.cpp:243]     Train net output #0: loss = 0.218256 (* 1 = 0.218256 loss)
I0428 20:27:36.759526  7412 sgd_solver.cpp:137] Iteration 38300, lr = 0.01
I0428 20:27:36.769526  7412 sgd_solver.cpp:169] scale layer:0.444897 0.467007 0.521531 0.565740 0.517049 0.488840 0.500247 0.492417 0.490799 0.494281 0.506514 0.489556 0.464128 0.437094 0.425662 0.390505 0.358473 0.318345 0.314140 1.008865 
I0428 20:27:36.771526  7412 sgd_solver.cpp:200] weight diff/data:0.014383 0.010562 0.010738 0.024554 0.010035 0.009855 0.014829 0.012363 0.009401 0.011167 0.014145 0.060409 0.011976 0.015619 0.100749 0.017142 0.013696 0.027159 0.011847 0.019446 0.001746 
I0428 20:28:00.212867 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:28:01.444938  7412 solver.cpp:224] Iteration 38400 (4.05094 iter/s, 24.6856s/100 iters), loss = 0.270253
I0428 20:28:01.444938  7412 solver.cpp:243]     Train net output #0: loss = 0.270253 (* 1 = 0.270253 loss)
I0428 20:28:01.444938  7412 sgd_solver.cpp:137] Iteration 38400, lr = 0.01
I0428 20:28:01.453938  7412 sgd_solver.cpp:169] scale layer:0.447190 0.465448 0.522640 0.563307 0.520679 0.488611 0.499991 0.492019 0.490799 0.494441 0.505331 0.488915 0.463238 0.436664 0.425967 0.390356 0.358216 0.317092 0.314012 1.011059 
I0428 20:28:01.455938  7412 sgd_solver.cpp:200] weight diff/data:0.009025 0.035074 0.014018 0.009706 0.061586 0.013493 0.013870 0.016902 0.010082 0.011029 0.043835 0.014570 0.019076 0.015538 0.012961 0.028478 0.017506 0.018125 0.011460 0.028311 0.001569 
I0428 20:28:25.865334  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_38500.caffemodel
I0428 20:28:25.874336  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_38500.solverstate
I0428 20:28:25.877336  7412 solver.cpp:336] Iteration 38500, Testing net (#0)
I0428 20:28:34.653837 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:28:35.014858  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8015
I0428 20:28:35.014858  7412 solver.cpp:403]     Test net output #1: loss = 0.713408 (* 1 = 0.713408 loss)
I0428 20:28:35.248872  7412 solver.cpp:224] Iteration 38500 (2.95817 iter/s, 33.8047s/100 iters), loss = 0.291538
I0428 20:28:35.248872  7412 solver.cpp:243]     Train net output #0: loss = 0.291537 (* 1 = 0.291537 loss)
I0428 20:28:35.248872  7412 sgd_solver.cpp:137] Iteration 38500, lr = 0.01
I0428 20:28:35.257872  7412 sgd_solver.cpp:169] scale layer:0.446857 0.468499 0.523337 0.564563 0.517880 0.489099 0.499082 0.491319 0.492054 0.493426 0.505434 0.489016 0.464004 0.437528 0.425379 0.390764 0.358304 0.318173 0.313789 1.008754 
I0428 20:28:35.259872  7412 sgd_solver.cpp:200] weight diff/data:0.011288 0.011041 0.013311 0.011218 0.016720 0.011814 0.019228 0.016635 0.034689 0.014277 0.013695 0.014858 0.032351 0.013689 0.018049 0.013352 0.015582 0.016026 0.028431 0.019855 0.001409 
I0428 20:28:59.925283  7412 solver.cpp:224] Iteration 38600 (4.05235 iter/s, 24.677s/100 iters), loss = 0.153488
I0428 20:28:59.926283  7412 solver.cpp:243]     Train net output #0: loss = 0.153488 (* 1 = 0.153488 loss)
I0428 20:28:59.926283  7412 sgd_solver.cpp:137] Iteration 38600, lr = 0.01
I0428 20:28:59.934283  7412 sgd_solver.cpp:169] scale layer:0.442745 0.466558 0.522247 0.565584 0.518422 0.488810 0.498887 0.492349 0.491815 0.494396 0.505112 0.489026 0.464429 0.437286 0.426051 0.391216 0.358022 0.318222 0.314108 1.005106 
I0428 20:28:59.936283  7412 sgd_solver.cpp:200] weight diff/data:0.006933 0.011110 0.011966 0.018102 0.012689 0.010865 0.024944 0.013326 0.029259 0.021744 0.015756 0.014919 0.018598 0.203148 0.025541 0.019003 0.021590 0.015963 0.010523 0.011599 0.001763 
I0428 20:29:24.615695  7412 solver.cpp:224] Iteration 38700 (4.0501 iter/s, 24.6908s/100 iters), loss = 0.167656
I0428 20:29:24.616695  7412 solver.cpp:243]     Train net output #0: loss = 0.167656 (* 1 = 0.167656 loss)
I0428 20:29:24.616695  7412 sgd_solver.cpp:137] Iteration 38700, lr = 0.01
I0428 20:29:24.624696  7412 sgd_solver.cpp:169] scale layer:0.437536 0.464048 0.523140 0.565731 0.521951 0.489280 0.498464 0.493067 0.493194 0.494387 0.505833 0.488451 0.464042 0.436827 0.426405 0.390460 0.358484 0.318691 0.314039 1.006039 
I0428 20:29:24.626695  7412 sgd_solver.cpp:200] weight diff/data:0.006961 0.058547 0.010954 0.011424 0.012181 0.009868 0.038549 0.015995 0.013409 0.032152 0.012391 0.015292 0.012468 0.019912 0.011674 0.018116 0.018440 0.022541 0.011452 0.016941 0.002986 
I0428 20:29:48.038034 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:29:49.262104  7412 solver.cpp:224] Iteration 38800 (4.05735 iter/s, 24.6466s/100 iters), loss = 0.281759
I0428 20:29:49.262104  7412 solver.cpp:243]     Train net output #0: loss = 0.281758 (* 1 = 0.281758 loss)
I0428 20:29:49.262104  7412 sgd_solver.cpp:137] Iteration 38800, lr = 0.01
I0428 20:29:49.272105  7412 sgd_solver.cpp:169] scale layer:0.441378 0.459352 0.523609 0.562785 0.519918 0.488544 0.499024 0.493197 0.492977 0.494163 0.505974 0.488838 0.464560 0.437835 0.426067 0.391498 0.356894 0.318456 0.313687 1.005621 
I0428 20:29:49.273105  7412 sgd_solver.cpp:200] weight diff/data:0.006505 0.016457 0.010910 0.010675 0.018492 0.015347 0.012895 0.016262 0.014202 1.262374 0.013525 0.021330 0.015579 0.013154 0.014299 0.016310 0.073779 0.011609 0.019318 0.021273 0.031515 
I0428 20:30:13.939517  7412 solver.cpp:224] Iteration 38900 (4.05228 iter/s, 24.6775s/100 iters), loss = 0.263922
I0428 20:30:13.939517  7412 solver.cpp:243]     Train net output #0: loss = 0.263922 (* 1 = 0.263922 loss)
I0428 20:30:13.939517  7412 sgd_solver.cpp:137] Iteration 38900, lr = 0.01
I0428 20:30:13.947516  7412 sgd_solver.cpp:169] scale layer:0.438763 0.463786 0.524618 0.563858 0.516731 0.488795 0.498916 0.492077 0.492372 0.494195 0.506418 0.488848 0.464187 0.437763 0.425673 0.391897 0.357101 0.318340 0.313260 1.005476 
I0428 20:30:13.949517  7412 sgd_solver.cpp:200] weight diff/data:0.006888 0.028432 0.014016 0.012289 0.013081 0.017057 0.012511 0.018496 0.014587 0.021632 0.024647 0.012022 0.010998 0.015673 0.015003 0.016514 0.013716 0.016695 0.015592 0.013402 0.001775 
I0428 20:30:38.357913  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_39000.caffemodel
I0428 20:30:38.366914  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_39000.solverstate
I0428 20:30:38.369913  7412 solver.cpp:336] Iteration 39000, Testing net (#0)
I0428 20:30:47.159416 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:30:47.526437  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8118
I0428 20:30:47.526437  7412 solver.cpp:403]     Test net output #1: loss = 0.651748 (* 1 = 0.651748 loss)
I0428 20:30:47.757450  7412 solver.cpp:224] Iteration 39000 (2.95687 iter/s, 33.8195s/100 iters), loss = 0.224958
I0428 20:30:47.757450  7412 solver.cpp:243]     Train net output #0: loss = 0.224958 (* 1 = 0.224958 loss)
I0428 20:30:47.757450  7412 sgd_solver.cpp:137] Iteration 39000, lr = 0.01
I0428 20:30:47.766451  7412 sgd_solver.cpp:169] scale layer:0.447403 0.466868 0.523355 0.562481 0.518036 0.488931 0.498475 0.492411 0.492408 0.494066 0.506628 0.488127 0.463726 0.437912 0.425225 0.392198 0.356858 0.318559 0.312986 1.005182 
I0428 20:30:47.768451  7412 sgd_solver.cpp:200] weight diff/data:0.006231 0.008710 0.013637 0.012859 0.030927 0.018428 0.018641 0.013784 0.016536 0.012221 0.077684 0.013883 0.013106 0.018878 0.011877 0.018229 0.013763 0.012230 0.011119 0.017349 0.002306 
I0428 20:31:12.411860  7412 solver.cpp:224] Iteration 39100 (4.05598 iter/s, 24.655s/100 iters), loss = 0.206612
I0428 20:31:12.412860  7412 solver.cpp:243]     Train net output #0: loss = 0.206612 (* 1 = 0.206612 loss)
I0428 20:31:12.412860  7412 sgd_solver.cpp:137] Iteration 39100, lr = 0.01
I0428 20:31:12.421861  7412 sgd_solver.cpp:169] scale layer:0.444059 0.466567 0.522417 0.565872 0.520040 0.488444 0.499470 0.493699 0.492444 0.494010 0.506373 0.488514 0.464638 0.437217 0.425245 0.392099 0.356093 0.317836 0.312691 1.003946 
I0428 20:31:12.423861  7412 sgd_solver.cpp:200] weight diff/data:0.007407 0.015331 0.008901 0.014470 0.025640 0.012192 0.012461 0.009099 0.015318 0.019108 0.011673 0.019379 0.014432 0.024861 0.010732 0.012294 0.035028 0.011215 0.018133 0.015405 0.001722 
I0428 20:31:35.861202 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:31:37.087272  7412 solver.cpp:224] Iteration 39200 (4.05257 iter/s, 24.6757s/100 iters), loss = 0.34973
I0428 20:31:37.087272  7412 solver.cpp:243]     Train net output #0: loss = 0.34973 (* 1 = 0.34973 loss)
I0428 20:31:37.087272  7412 sgd_solver.cpp:137] Iteration 39200, lr = 0.01
I0428 20:31:37.096272  7412 sgd_solver.cpp:169] scale layer:0.444575 0.464546 0.523792 0.563888 0.520457 0.488286 0.500784 0.494671 0.493958 0.493687 0.506397 0.488709 0.464453 0.437462 0.425932 0.391677 0.356252 0.317696 0.312546 1.003977 
I0428 20:31:37.098273  7412 sgd_solver.cpp:200] weight diff/data:0.007868 0.009754 0.011018 0.040845 0.013156 0.013952 0.015942 0.089496 0.015122 0.015632 0.016569 0.020489 0.039554 0.015422 0.028965 0.012135 0.016343 0.020164 0.012783 0.012713 0.001219 
I0428 20:32:01.742682  7412 solver.cpp:224] Iteration 39300 (4.05581 iter/s, 24.656s/100 iters), loss = 0.223576
I0428 20:32:01.742682  7412 solver.cpp:243]     Train net output #0: loss = 0.223576 (* 1 = 0.223576 loss)
I0428 20:32:01.743682  7412 sgd_solver.cpp:137] Iteration 39300, lr = 0.01
I0428 20:32:01.751682  7412 sgd_solver.cpp:169] scale layer:0.446265 0.465248 0.523848 0.566783 0.516887 0.489617 0.501028 0.494043 0.493748 0.493990 0.507245 0.488590 0.464433 0.438004 0.426392 0.391304 0.356373 0.318207 0.312066 1.003358 
I0428 20:32:01.753684  7412 sgd_solver.cpp:200] weight diff/data:0.007284 0.009890 0.022586 0.020231 0.013356 0.014295 0.016476 0.015051 0.013186 0.011277 0.019025 0.012567 0.015101 0.013229 0.017184 0.013297 0.012604 0.014321 0.017908 0.012959 0.001096 
I0428 20:32:26.405093  7412 solver.cpp:224] Iteration 39400 (4.05467 iter/s, 24.6629s/100 iters), loss = 0.206857
I0428 20:32:26.405093  7412 solver.cpp:243]     Train net output #0: loss = 0.206857 (* 1 = 0.206857 loss)
I0428 20:32:26.406093  7412 sgd_solver.cpp:137] Iteration 39400, lr = 0.01
I0428 20:32:26.414093  7412 sgd_solver.cpp:169] scale layer:0.442708 0.467652 0.522253 0.564988 0.515239 0.488151 0.498661 0.493690 0.492940 0.494066 0.507704 0.487516 0.463486 0.437719 0.425943 0.391931 0.355519 0.317447 0.311606 1.004938 
I0428 20:32:26.416093  7412 sgd_solver.cpp:200] weight diff/data:0.006758 0.016216 0.009310 0.014266 0.013952 0.009997 0.016743 0.010078 0.013267 0.012346 0.012934 0.010435 0.011907 0.012242 0.012979 0.021518 0.013361 0.011146 0.011771 0.011863 0.001446 
I0428 20:32:50.816489  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_39500.caffemodel
I0428 20:32:50.825489  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_39500.solverstate
I0428 20:32:50.828490  7412 solver.cpp:336] Iteration 39500, Testing net (#0)
I0428 20:32:59.601991 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:32:59.966012  7412 solver.cpp:403]     Test net output #0: accuracy = 0.7997
I0428 20:32:59.967012  7412 solver.cpp:403]     Test net output #1: loss = 0.681987 (* 1 = 0.681987 loss)
I0428 20:33:00.200026  7412 solver.cpp:224] Iteration 39500 (2.95894 iter/s, 33.7959s/100 iters), loss = 0.191426
I0428 20:33:00.201026  7412 solver.cpp:243]     Train net output #0: loss = 0.191426 (* 1 = 0.191426 loss)
I0428 20:33:00.201026  7412 sgd_solver.cpp:137] Iteration 39500, lr = 0.01
I0428 20:33:00.209026  7412 sgd_solver.cpp:169] scale layer:0.444491 0.468213 0.524060 0.564735 0.518973 0.487529 0.499320 0.493782 0.492314 0.493551 0.507145 0.486266 0.464259 0.437587 0.424959 0.391581 0.355255 0.317347 0.311448 1.006440 
I0428 20:33:00.211026  7412 sgd_solver.cpp:200] weight diff/data:0.005607 0.007343 0.019318 0.011076 0.011616 0.012462 0.014871 0.020444 0.018257 0.014096 0.064836 0.028873 0.011857 0.012049 0.013641 0.012533 0.012219 0.014478 0.015916 0.014718 0.005378 
I0428 20:33:23.624366 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:33:24.844435  7412 solver.cpp:224] Iteration 39600 (4.05763 iter/s, 24.6449s/100 iters), loss = 0.286036
I0428 20:33:24.845435  7412 solver.cpp:243]     Train net output #0: loss = 0.286036 (* 1 = 0.286036 loss)
I0428 20:33:24.845435  7412 sgd_solver.cpp:137] Iteration 39600, lr = 0.01
I0428 20:33:24.853436  7412 sgd_solver.cpp:169] scale layer:0.444633 0.468358 0.522571 0.565155 0.517369 0.487639 0.499525 0.493703 0.491508 0.493261 0.506178 0.486329 0.463863 0.436857 0.425548 0.391600 0.355077 0.317012 0.311425 1.006143 
I0428 20:33:24.855437  7412 sgd_solver.cpp:200] weight diff/data:0.007128 0.014004 0.034345 0.013536 0.016444 0.014074 0.010482 0.012722 0.096358 0.011219 0.179907 0.012505 0.035866 0.012368 0.017841 0.013224 0.015730 0.012485 0.011788 0.027552 0.001147 
I0428 20:33:49.511847  7412 solver.cpp:224] Iteration 39700 (4.05396 iter/s, 24.6673s/100 iters), loss = 0.173722
I0428 20:33:49.511847  7412 solver.cpp:243]     Train net output #0: loss = 0.173722 (* 1 = 0.173722 loss)
I0428 20:33:49.511847  7412 sgd_solver.cpp:137] Iteration 39700, lr = 0.01
I0428 20:33:49.519846  7412 sgd_solver.cpp:169] scale layer:0.444760 0.465942 0.523158 0.563699 0.517595 0.487782 0.499825 0.492681 0.492155 0.492161 0.504720 0.486100 0.463941 0.436648 0.425368 0.391828 0.355689 0.317511 0.310866 1.008192 
I0428 20:33:49.522847  7412 sgd_solver.cpp:200] weight diff/data:0.006299 0.009233 0.020292 0.013022 0.016382 0.015815 0.016746 0.013910 0.012041 0.019411 0.012301 0.017019 0.015145 0.014207 0.049958 0.027645 0.023129 0.013511 0.010969 0.013067 0.001171 
I0428 20:34:14.188257  7412 solver.cpp:224] Iteration 39800 (4.05235 iter/s, 24.6771s/100 iters), loss = 0.24235
I0428 20:34:14.188257  7412 solver.cpp:243]     Train net output #0: loss = 0.24235 (* 1 = 0.24235 loss)
I0428 20:34:14.188257  7412 sgd_solver.cpp:137] Iteration 39800, lr = 0.01
I0428 20:34:14.197258  7412 sgd_solver.cpp:169] scale layer:0.446283 0.467826 0.522631 0.564179 0.516129 0.487979 0.499114 0.493106 0.493025 0.493579 0.505847 0.486712 0.464409 0.436348 0.424745 0.392256 0.356303 0.317240 0.311413 1.009017 
I0428 20:34:14.199259  7412 sgd_solver.cpp:200] weight diff/data:0.006423 0.010070 0.023013 0.041167 0.015169 0.012574 0.015899 0.013751 0.015562 0.031769 0.010207 0.012361 0.016054 0.023913 0.016742 0.017770 0.012701 0.015394 0.017356 0.026735 0.001850 
I0428 20:34:38.879670  7412 solver.cpp:224] Iteration 39900 (4.04994 iter/s, 24.6917s/100 iters), loss = 0.238595
I0428 20:34:38.879670  7412 solver.cpp:243]     Train net output #0: loss = 0.238595 (* 1 = 0.238595 loss)
I0428 20:34:38.879670  7412 sgd_solver.cpp:137] Iteration 39900, lr = 0.01
I0428 20:34:38.888670  7412 sgd_solver.cpp:169] scale layer:0.444452 0.465447 0.522701 0.562900 0.519336 0.488299 0.500577 0.493213 0.493181 0.494022 0.504694 0.486022 0.463816 0.436359 0.425148 0.390544 0.355864 0.317248 0.311280 1.008405 
I0428 20:34:38.891670  7412 sgd_solver.cpp:200] weight diff/data:0.008795 0.007912 0.011004 0.010776 0.009935 0.012734 0.010783 0.013144 0.016651 0.019882 0.013639 0.021769 0.015177 0.015080 0.012239 0.016101 0.014724 0.014407 0.013623 0.018196 0.002306 
I0428 20:35:02.339011 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:03.323068  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_40000.caffemodel
I0428 20:35:03.332068  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_40000.solverstate
I0428 20:35:03.334069  7412 solver.cpp:336] Iteration 40000, Testing net (#0)
I0428 20:35:12.115571 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:12.479591  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8093
I0428 20:35:12.480592  7412 solver.cpp:403]     Test net output #1: loss = 0.645871 (* 1 = 0.645871 loss)
I0428 20:35:12.713605  7412 solver.cpp:224] Iteration 40000 (2.95557 iter/s, 33.8344s/100 iters), loss = 0.258677
I0428 20:35:12.713605  7412 solver.cpp:243]     Train net output #0: loss = 0.258677 (* 1 = 0.258677 loss)
I0428 20:35:12.713605  7412 sgd_solver.cpp:46] MultiStep Status: Iteration 40000, step = 1
I0428 20:35:12.713605  7412 sgd_solver.cpp:137] Iteration 40000, lr = 0.001
I0428 20:35:12.721606  7412 sgd_solver.cpp:169] scale layer:0.445307 0.461598 0.520458 0.561663 0.518414 0.489418 0.499326 0.493981 0.493947 0.493356 0.504961 0.487007 0.464652 0.436187 0.424726 0.391476 0.355677 0.317404 0.310982 1.006506 
I0428 20:35:12.723605  7412 sgd_solver.cpp:200] weight diff/data:0.005541 0.008734 0.017858 0.011715 0.012312 0.009880 0.016331 0.014361 0.011181 0.021575 0.013387 0.026976 0.010606 0.022591 0.010270 0.012240 0.018380 0.011302 0.011646 0.012182 0.001094 
I0428 20:35:37.386016  7412 solver.cpp:224] Iteration 40100 (4.05295 iter/s, 24.6734s/100 iters), loss = 0.194815
I0428 20:35:37.387017  7412 solver.cpp:243]     Train net output #0: loss = 0.194815 (* 1 = 0.194815 loss)
I0428 20:35:37.387017  7412 sgd_solver.cpp:137] Iteration 40100, lr = 0.001
I0428 20:35:37.395017  7412 sgd_solver.cpp:169] scale layer:0.447214 0.462599 0.520500 0.561770 0.519393 0.489549 0.499026 0.494225 0.493464 0.493807 0.504662 0.487139 0.464593 0.436332 0.424607 0.391511 0.355915 0.317264 0.310981 1.006075 
I0428 20:35:37.397017  7412 sgd_solver.cpp:200] weight diff/data:0.002597 0.000810 0.002898 0.000888 0.001235 0.002302 0.001384 0.002442 0.001406 0.002569 0.002019 0.000937 0.002618 0.001123 0.001133 0.002746 0.001500 0.001440 0.001232 0.001065 0.000132 
I0428 20:36:02.028425  7412 solver.cpp:224] Iteration 40200 (4.05804 iter/s, 24.6424s/100 iters), loss = 0.14016
I0428 20:36:02.028425  7412 solver.cpp:243]     Train net output #0: loss = 0.14016 (* 1 = 0.14016 loss)
I0428 20:36:02.028425  7412 sgd_solver.cpp:137] Iteration 40200, lr = 0.001
I0428 20:36:02.037426  7412 sgd_solver.cpp:169] scale layer:0.446948 0.463206 0.520587 0.561896 0.519271 0.489442 0.499006 0.494152 0.493399 0.493536 0.504380 0.487136 0.464318 0.436249 0.424569 0.391388 0.355936 0.317156 0.310924 1.006682 
I0428 20:36:02.039427  7412 sgd_solver.cpp:200] weight diff/data:0.000425 0.001246 0.001125 0.000904 0.000946 0.000731 0.001058 0.001061 0.001011 0.001077 0.001787 0.001075 0.001176 0.001432 0.001098 0.000937 0.001092 0.001421 0.002589 0.004221 0.000101 
I0428 20:36:26.696836  7412 solver.cpp:224] Iteration 40300 (4.05371 iter/s, 24.6688s/100 iters), loss = 0.0965211
I0428 20:36:26.697836  7412 solver.cpp:243]     Train net output #0: loss = 0.096521 (* 1 = 0.096521 loss)
I0428 20:36:26.697836  7412 sgd_solver.cpp:137] Iteration 40300, lr = 0.001
I0428 20:36:26.705837  7412 sgd_solver.cpp:169] scale layer:0.446230 0.462926 0.520308 0.561632 0.518986 0.489180 0.498997 0.493900 0.493124 0.493432 0.504153 0.487029 0.464118 0.436164 0.424377 0.391295 0.355776 0.316960 0.310784 1.007543 
I0428 20:36:26.707837  7412 sgd_solver.cpp:200] weight diff/data:0.000593 0.000708 0.000671 0.000967 0.001352 0.001151 0.000806 0.001279 0.002182 0.001010 0.001758 0.001650 0.000885 0.001238 0.000716 0.001937 0.001286 0.001005 0.002524 0.002118 0.000117 
I0428 20:36:50.107175 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:36:51.330245  7412 solver.cpp:224] Iteration 40400 (4.05963 iter/s, 24.6328s/100 iters), loss = 0.124973
I0428 20:36:51.330245  7412 solver.cpp:243]     Train net output #0: loss = 0.124973 (* 1 = 0.124973 loss)
I0428 20:36:51.330245  7412 sgd_solver.cpp:137] Iteration 40400, lr = 0.001
I0428 20:36:51.339246  7412 sgd_solver.cpp:169] scale layer:0.445447 0.462356 0.520136 0.561627 0.518710 0.488930 0.498858 0.493722 0.492899 0.493121 0.503997 0.486863 0.463883 0.436042 0.424219 0.391153 0.355817 0.316825 0.310728 1.008724 
I0428 20:36:51.341246  7412 sgd_solver.cpp:200] weight diff/data:0.000476 0.000510 0.002807 0.000578 0.000875 0.000624 0.000782 0.001759 0.000746 0.000969 0.000980 0.000830 0.001136 0.000923 0.001057 0.001948 0.001230 0.001291 0.000770 0.001127 0.000064 
I0428 20:37:15.803645  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_40500.caffemodel
I0428 20:37:15.811646  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_40500.solverstate
I0428 20:37:15.814646  7412 solver.cpp:336] Iteration 40500, Testing net (#0)
I0428 20:37:24.591148 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:37:24.957170  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8348
I0428 20:37:24.957170  7412 solver.cpp:403]     Test net output #1: loss = 0.575733 (* 1 = 0.575733 loss)
I0428 20:37:25.197182  7412 solver.cpp:224] Iteration 40500 (2.95267 iter/s, 33.8677s/100 iters), loss = 0.120076
I0428 20:37:25.197182  7412 solver.cpp:243]     Train net output #0: loss = 0.120075 (* 1 = 0.120075 loss)
I0428 20:37:25.197182  7412 sgd_solver.cpp:137] Iteration 40500, lr = 0.001
I0428 20:37:25.205183  7412 sgd_solver.cpp:169] scale layer:0.445417 0.463141 0.520146 0.561653 0.518340 0.488780 0.498569 0.493391 0.492557 0.492838 0.503797 0.486643 0.463564 0.435908 0.424009 0.390964 0.355706 0.316729 0.310599 1.009733 
I0428 20:37:25.207183  7412 sgd_solver.cpp:200] weight diff/data:0.000347 0.000693 0.001278 0.000771 0.001146 0.001685 0.000941 0.003338 0.001277 0.000885 0.001025 0.001758 0.000861 0.001380 0.001499 0.001311 0.001735 0.000860 0.000879 0.001020 0.000088 
I0428 20:37:49.863593  7412 solver.cpp:224] Iteration 40600 (4.05398 iter/s, 24.6671s/100 iters), loss = 0.101843
I0428 20:37:49.864593  7412 solver.cpp:243]     Train net output #0: loss = 0.101843 (* 1 = 0.101843 loss)
I0428 20:37:49.864593  7412 sgd_solver.cpp:137] Iteration 40600, lr = 0.001
I0428 20:37:49.872594  7412 sgd_solver.cpp:169] scale layer:0.445447 0.463387 0.520033 0.561741 0.518137 0.488666 0.498319 0.493334 0.492321 0.492647 0.503639 0.486479 0.463310 0.435685 0.423887 0.390824 0.355507 0.316571 0.310501 1.010811 
I0428 20:37:49.874594  7412 sgd_solver.cpp:200] weight diff/data:0.000408 0.000731 0.001162 0.002113 0.001000 0.000940 0.001382 0.001163 0.001008 0.001404 0.000816 0.001212 0.001164 0.000953 0.001065 0.001197 0.001855 0.001774 0.001041 0.001694 0.000085 
I0428 20:38:14.538005  7412 solver.cpp:224] Iteration 40700 (4.05289 iter/s, 24.6738s/100 iters), loss = 0.0662436
I0428 20:38:14.538005  7412 solver.cpp:243]     Train net output #0: loss = 0.0662436 (* 1 = 0.0662436 loss)
I0428 20:38:14.538005  7412 sgd_solver.cpp:137] Iteration 40700, lr = 0.001
I0428 20:38:14.546005  7412 sgd_solver.cpp:169] scale layer:0.444687 0.462927 0.519757 0.561251 0.517978 0.488408 0.498070 0.493155 0.492215 0.492497 0.503465 0.486258 0.463100 0.435518 0.423741 0.390687 0.355239 0.316381 0.310338 1.011959 
I0428 20:38:14.548005  7412 sgd_solver.cpp:200] weight diff/data:0.000416 0.000529 0.001035 0.000650 0.000862 0.000828 0.000992 0.000930 0.000943 0.001247 0.000973 0.002986 0.000924 0.001579 0.000728 0.000845 0.001127 0.000847 0.000989 0.002684 0.000123 
I0428 20:38:37.992347 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:38:39.217417  7412 solver.cpp:224] Iteration 40800 (4.05189 iter/s, 24.6798s/100 iters), loss = 0.100809
I0428 20:38:39.217417  7412 solver.cpp:243]     Train net output #0: loss = 0.100809 (* 1 = 0.100809 loss)
I0428 20:38:39.217417  7412 sgd_solver.cpp:137] Iteration 40800, lr = 0.001
I0428 20:38:39.225417  7412 sgd_solver.cpp:169] scale layer:0.444382 0.462582 0.519341 0.561208 0.517714 0.488190 0.498003 0.492968 0.491940 0.492216 0.503241 0.486014 0.462890 0.435393 0.423629 0.390472 0.355117 0.316206 0.310207 1.013113 
I0428 20:38:39.227417  7412 sgd_solver.cpp:200] weight diff/data:0.001050 0.000677 0.001126 0.000702 0.001308 0.000675 0.001091 0.000939 0.002272 0.000839 0.001008 0.000860 0.001338 0.000918 0.000928 0.005663 0.000692 0.000830 0.000945 0.000643 0.000079 
I0428 20:39:03.899828  7412 solver.cpp:224] Iteration 40900 (4.05137 iter/s, 24.683s/100 iters), loss = 0.0767685
I0428 20:39:03.899828  7412 solver.cpp:243]     Train net output #0: loss = 0.0767685 (* 1 = 0.0767685 loss)
I0428 20:39:03.899828  7412 sgd_solver.cpp:137] Iteration 40900, lr = 0.001
I0428 20:39:03.907829  7412 sgd_solver.cpp:169] scale layer:0.444908 0.462982 0.519381 0.561343 0.517303 0.488007 0.497723 0.492745 0.491453 0.492047 0.502966 0.485845 0.462677 0.435249 0.423410 0.390405 0.355014 0.316105 0.310089 1.014101 
I0428 20:39:03.909829  7412 sgd_solver.cpp:200] weight diff/data:0.000493 0.003116 0.104403 0.000738 0.001824 0.000799 0.001113 0.001410 0.000995 0.000858 0.001031 0.000737 0.000816 0.000902 0.000924 0.001662 0.000943 0.001616 0.000819 0.000741 0.000076 
I0428 20:39:28.351227  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_41000.caffemodel
I0428 20:39:28.360227  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_41000.solverstate
I0428 20:39:28.363227  7412 solver.cpp:336] Iteration 41000, Testing net (#0)
I0428 20:39:37.144729 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:39:37.511750  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8353
I0428 20:39:37.511750  7412 solver.cpp:403]     Test net output #1: loss = 0.586375 (* 1 = 0.586375 loss)
I0428 20:39:37.745764  7412 solver.cpp:224] Iteration 41000 (2.95446 iter/s, 33.8471s/100 iters), loss = 0.0743334
I0428 20:39:37.745764  7412 solver.cpp:243]     Train net output #0: loss = 0.0743334 (* 1 = 0.0743334 loss)
I0428 20:39:37.745764  7412 sgd_solver.cpp:137] Iteration 41000, lr = 0.001
I0428 20:39:37.754765  7412 sgd_solver.cpp:169] scale layer:0.444627 0.462622 0.519205 0.561047 0.517491 0.487753 0.497414 0.492480 0.491148 0.491789 0.502704 0.485634 0.462425 0.434958 0.423249 0.390295 0.354916 0.315884 0.309958 1.015238 
I0428 20:39:37.755764  7412 sgd_solver.cpp:200] weight diff/data:0.000348 0.000538 0.001696 0.000856 0.000764 0.000767 0.002744 0.000936 0.001089 0.001283 0.001346 0.000924 0.000986 0.005264 0.001093 0.001621 0.000888 0.001078 0.000954 0.000866 0.000077 
I0428 20:40:02.431176  7412 solver.cpp:224] Iteration 41100 (4.05082 iter/s, 24.6864s/100 iters), loss = 0.0605975
I0428 20:40:02.432176  7412 solver.cpp:243]     Train net output #0: loss = 0.0605975 (* 1 = 0.0605975 loss)
I0428 20:40:02.432176  7412 sgd_solver.cpp:137] Iteration 41100, lr = 0.001
I0428 20:40:02.440176  7412 sgd_solver.cpp:169] scale layer:0.444205 0.462610 0.518980 0.560776 0.517317 0.487476 0.497205 0.492236 0.491008 0.491581 0.502508 0.485383 0.462213 0.434741 0.423095 0.390120 0.354661 0.315700 0.309768 1.016328 
I0428 20:40:02.442176  7412 sgd_solver.cpp:200] weight diff/data:0.000484 0.000980 0.000744 0.000933 0.001039 0.000662 0.001608 0.001434 0.001975 0.000923 0.000806 0.000952 0.000841 0.000928 0.000830 0.001074 0.000970 0.001108 0.000716 0.001326 0.000073 
I0428 20:40:25.881517 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:40:27.112587  7412 solver.cpp:224] Iteration 41200 (4.0516 iter/s, 24.6816s/100 iters), loss = 0.0711641
I0428 20:40:27.112587  7412 solver.cpp:243]     Train net output #0: loss = 0.0711641 (* 1 = 0.0711641 loss)
I0428 20:40:27.112587  7412 sgd_solver.cpp:137] Iteration 41200, lr = 0.001
I0428 20:40:27.122588  7412 sgd_solver.cpp:169] scale layer:0.443776 0.461930 0.518647 0.560510 0.517124 0.487315 0.496982 0.492021 0.490816 0.491346 0.502326 0.485195 0.462052 0.434640 0.422935 0.389844 0.354435 0.315457 0.309671 1.017598 
I0428 20:40:27.124588  7412 sgd_solver.cpp:200] weight diff/data:0.000270 0.000424 0.002720 0.000840 0.001003 0.000671 0.001105 0.001698 0.000746 0.001049 0.001546 0.000777 0.000866 0.001395 0.000823 0.000960 0.015288 0.002779 0.001460 0.000845 0.000069 
I0428 20:40:51.894006  7412 solver.cpp:224] Iteration 41300 (4.03531 iter/s, 24.7812s/100 iters), loss = 0.0755
I0428 20:40:51.894006  7412 solver.cpp:243]     Train net output #0: loss = 0.0755 (* 1 = 0.0755 loss)
I0428 20:40:51.894006  7412 sgd_solver.cpp:137] Iteration 41300, lr = 0.001
I0428 20:40:51.903005  7412 sgd_solver.cpp:169] scale layer:0.444219 0.462307 0.518757 0.560649 0.516734 0.487090 0.496657 0.491738 0.490445 0.491125 0.502052 0.484884 0.461766 0.434436 0.422722 0.389641 0.354220 0.315332 0.309544 1.018734 
I0428 20:40:51.905005  7412 sgd_solver.cpp:200] weight diff/data:0.000375 0.000977 0.000955 0.000868 0.003462 0.001277 0.000864 0.002664 0.000819 0.000972 0.000773 0.012143 0.000904 0.001075 0.001024 0.001145 0.001125 0.002768 0.000874 0.000716 0.000078 
I0428 20:41:16.663422  7412 solver.cpp:224] Iteration 41400 (4.03713 iter/s, 24.77s/100 iters), loss = 0.0536953
I0428 20:41:16.663422  7412 solver.cpp:243]     Train net output #0: loss = 0.0536952 (* 1 = 0.0536952 loss)
I0428 20:41:16.663422  7412 sgd_solver.cpp:137] Iteration 41400, lr = 0.001
I0428 20:41:16.672422  7412 sgd_solver.cpp:169] scale layer:0.443951 0.462286 0.518629 0.560464 0.516612 0.486879 0.496435 0.491512 0.490207 0.490866 0.501784 0.484629 0.461461 0.434123 0.422586 0.389516 0.353966 0.315148 0.309444 1.019742 
I0428 20:41:16.674422  7412 sgd_solver.cpp:200] weight diff/data:0.000269 0.000470 0.003726 0.000739 0.013573 0.001815 0.000641 0.000523 0.000570 0.001715 0.000720 0.001086 0.000692 0.000749 0.000620 0.001219 0.001609 0.001061 0.000658 0.000808 0.000078 
I0428 20:41:41.210826  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_41500.caffemodel
I0428 20:41:41.219826  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_41500.solverstate
I0428 20:41:41.222826  7412 solver.cpp:336] Iteration 41500, Testing net (#0)
I0428 20:41:50.048331 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:41:50.415352  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8329
I0428 20:41:50.415352  7412 solver.cpp:403]     Test net output #1: loss = 0.605324 (* 1 = 0.605324 loss)
I0428 20:41:50.650367  7412 solver.cpp:224] Iteration 41500 (2.94221 iter/s, 33.988s/100 iters), loss = 0.0595389
I0428 20:41:50.650367  7412 solver.cpp:243]     Train net output #0: loss = 0.0595389 (* 1 = 0.0595389 loss)
I0428 20:41:50.650367  7412 sgd_solver.cpp:137] Iteration 41500, lr = 0.001
I0428 20:41:50.659366  7412 sgd_solver.cpp:169] scale layer:0.443405 0.462242 0.518177 0.560057 0.516094 0.486638 0.496190 0.491341 0.489996 0.490666 0.501574 0.484429 0.461268 0.433917 0.422398 0.389264 0.353724 0.314954 0.309268 1.020918 
I0428 20:41:50.660367  7412 sgd_solver.cpp:200] weight diff/data:0.000370 0.001128 0.000592 0.000758 0.000545 0.001572 0.001164 0.010471 0.000813 0.000761 0.001070 0.000806 0.000589 0.001071 0.000781 0.001120 0.001217 0.001146 0.000830 0.001398 0.000080 
I0428 20:42:14.200713 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:42:15.434783  7412 solver.cpp:224] Iteration 41600 (4.03473 iter/s, 24.7848s/100 iters), loss = 0.0424381
I0428 20:42:15.434783  7412 solver.cpp:243]     Train net output #0: loss = 0.042438 (* 1 = 0.042438 loss)
I0428 20:42:15.434783  7412 sgd_solver.cpp:137] Iteration 41600, lr = 0.001
I0428 20:42:15.443784  7412 sgd_solver.cpp:169] scale layer:0.442734 0.461829 0.517920 0.559790 0.515907 0.486397 0.495984 0.491056 0.489838 0.490414 0.501353 0.484215 0.460999 0.433740 0.422253 0.389048 0.353563 0.314764 0.309151 1.022057 
I0428 20:42:15.445785  7412 sgd_solver.cpp:200] weight diff/data:0.000311 0.000870 0.000924 0.000912 0.000616 0.000593 0.000743 0.000661 0.000560 0.000690 0.002731 0.003969 0.000679 0.000669 0.000559 0.000598 0.000664 0.000816 0.000683 0.000612 0.000058 
I0428 20:42:40.223201  7412 solver.cpp:224] Iteration 41700 (4.034 iter/s, 24.7893s/100 iters), loss = 0.0691922
I0428 20:42:40.224201  7412 solver.cpp:243]     Train net output #0: loss = 0.0691921 (* 1 = 0.0691921 loss)
I0428 20:42:40.224201  7412 sgd_solver.cpp:137] Iteration 41700, lr = 0.001
I0428 20:42:40.232201  7412 sgd_solver.cpp:169] scale layer:0.442885 0.461747 0.517800 0.559734 0.515756 0.486178 0.495767 0.490753 0.489415 0.490179 0.501086 0.483941 0.460795 0.433504 0.422004 0.388811 0.353352 0.314656 0.309052 1.023311 
I0428 20:42:40.235203  7412 sgd_solver.cpp:200] weight diff/data:0.000475 0.000675 0.001237 0.000662 0.000746 0.000592 0.000799 0.000702 0.001208 0.000801 0.000863 0.001394 0.001035 0.000812 0.000716 0.000984 0.000755 0.000919 0.001031 0.000626 0.000073 
I0428 20:43:05.015619  7412 solver.cpp:224] Iteration 41800 (4.03348 iter/s, 24.7925s/100 iters), loss = 0.0864566
I0428 20:43:05.015619  7412 solver.cpp:243]     Train net output #0: loss = 0.0864565 (* 1 = 0.0864565 loss)
I0428 20:43:05.015619  7412 sgd_solver.cpp:137] Iteration 41800, lr = 0.001
I0428 20:43:05.025620  7412 sgd_solver.cpp:169] scale layer:0.442639 0.461796 0.517719 0.559664 0.515312 0.485960 0.495518 0.490506 0.489057 0.489944 0.500801 0.483792 0.460503 0.433228 0.421824 0.388609 0.353167 0.314467 0.308919 1.024416 
I0428 20:43:05.027621  7412 sgd_solver.cpp:200] weight diff/data:0.000503 0.000525 0.000797 0.000771 0.000767 0.000584 0.001002 0.000745 0.000955 0.001771 0.000724 0.000883 0.001095 0.000996 0.001089 0.001047 0.000800 0.001669 0.000695 0.001634 0.000052 
I0428 20:43:29.662029  7412 solver.cpp:224] Iteration 41900 (4.05744 iter/s, 24.6461s/100 iters), loss = 0.0489326
I0428 20:43:29.662029  7412 solver.cpp:243]     Train net output #0: loss = 0.0489325 (* 1 = 0.0489325 loss)
I0428 20:43:29.662029  7412 sgd_solver.cpp:137] Iteration 41900, lr = 0.001
I0428 20:43:29.670029  7412 sgd_solver.cpp:169] scale layer:0.442312 0.461512 0.517301 0.559304 0.514921 0.485728 0.495187 0.490253 0.488902 0.489769 0.500506 0.483514 0.460281 0.432995 0.421623 0.388356 0.352877 0.314240 0.308733 1.025519 
I0428 20:43:29.672029  7412 sgd_solver.cpp:200] weight diff/data:0.000401 0.000618 0.001467 0.001043 0.000769 0.000705 0.000709 0.001318 0.000623 0.001088 0.000949 0.000990 0.009477 0.000973 0.001278 0.001139 0.000975 0.008000 0.001169 0.002186 0.000086 
I0428 20:43:53.068367 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:43:54.053424  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_42000.caffemodel
I0428 20:43:54.061424  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_42000.solverstate
I0428 20:43:54.064424  7412 solver.cpp:336] Iteration 42000, Testing net (#0)
I0428 20:44:02.849927 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:44:03.209947  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8334
I0428 20:44:03.209947  7412 solver.cpp:403]     Test net output #1: loss = 0.619175 (* 1 = 0.619175 loss)
I0428 20:44:03.442961  7412 solver.cpp:224] Iteration 42000 (2.96011 iter/s, 33.7825s/100 iters), loss = 0.0616981
I0428 20:44:03.442961  7412 solver.cpp:243]     Train net output #0: loss = 0.0616981 (* 1 = 0.0616981 loss)
I0428 20:44:03.443961  7412 sgd_solver.cpp:137] Iteration 42000, lr = 0.001
I0428 20:44:03.452961  7412 sgd_solver.cpp:169] scale layer:0.441895 0.460740 0.516982 0.559159 0.514582 0.485533 0.494898 0.490034 0.488721 0.489507 0.500317 0.483258 0.460061 0.432832 0.421426 0.388113 0.352670 0.314048 0.308597 1.026654 
I0428 20:44:03.454962  7412 sgd_solver.cpp:200] weight diff/data:0.000359 0.000478 0.001050 0.000522 0.000860 0.000580 0.000564 0.000738 0.000480 0.000513 0.000858 0.001012 0.000731 0.000722 0.005013 0.000759 0.000838 0.000542 0.000885 0.000919 0.000058 
I0428 20:44:28.094372  7412 solver.cpp:224] Iteration 42100 (4.05661 iter/s, 24.6511s/100 iters), loss = 0.056166
I0428 20:44:28.094372  7412 solver.cpp:243]     Train net output #0: loss = 0.056166 (* 1 = 0.056166 loss)
I0428 20:44:28.094372  7412 sgd_solver.cpp:137] Iteration 42100, lr = 0.001
I0428 20:44:28.102371  7412 sgd_solver.cpp:169] scale layer:0.442264 0.461154 0.517057 0.559102 0.514424 0.485331 0.494649 0.489767 0.488400 0.489293 0.500091 0.483075 0.459850 0.432657 0.421214 0.387872 0.352480 0.313920 0.308452 1.027866 
I0428 20:44:28.105371  7412 sgd_solver.cpp:200] weight diff/data:0.000299 0.000537 0.000941 0.000555 0.007737 0.000681 0.001046 0.001359 0.000575 0.000691 0.000945 0.001373 0.001165 0.000873 0.001033 0.001143 0.000640 0.001537 0.000771 0.008397 0.000065 
I0428 20:44:52.682777  7412 solver.cpp:224] Iteration 42200 (4.0668 iter/s, 24.5894s/100 iters), loss = 0.0804197
I0428 20:44:52.682777  7412 solver.cpp:243]     Train net output #0: loss = 0.0804197 (* 1 = 0.0804197 loss)
I0428 20:44:52.682777  7412 sgd_solver.cpp:137] Iteration 42200, lr = 0.001
I0428 20:44:52.691778  7412 sgd_solver.cpp:169] scale layer:0.442335 0.461060 0.516960 0.558798 0.514258 0.485088 0.494430 0.489603 0.488101 0.489031 0.499852 0.482820 0.459594 0.432389 0.421044 0.387655 0.352283 0.313723 0.308314 1.028962 
I0428 20:44:52.693778  7412 sgd_solver.cpp:200] weight diff/data:0.000328 0.000491 0.000867 0.001337 0.002003 0.000768 0.000973 0.000974 0.001122 0.000805 0.001053 0.000812 0.000680 0.000709 0.002762 0.001074 0.001104 0.000725 0.003348 0.000895 0.000061 
I0428 20:45:17.373189  7412 solver.cpp:224] Iteration 42300 (4.05008 iter/s, 24.6909s/100 iters), loss = 0.0408358
I0428 20:45:17.373189  7412 solver.cpp:243]     Train net output #0: loss = 0.0408358 (* 1 = 0.0408358 loss)
I0428 20:45:17.373189  7412 sgd_solver.cpp:137] Iteration 42300, lr = 0.001
I0428 20:45:17.382190  7412 sgd_solver.cpp:169] scale layer:0.441760 0.460763 0.516562 0.558299 0.514174 0.484818 0.494239 0.489365 0.487903 0.488773 0.499624 0.482583 0.459405 0.432145 0.420841 0.387459 0.352001 0.313540 0.308138 1.030025 
I0428 20:45:17.384191  7412 sgd_solver.cpp:200] weight diff/data:0.000289 0.000915 0.001088 0.000979 0.000629 0.016285 0.000931 0.000670 0.000689 0.003699 0.000842 0.000910 0.001579 0.000655 0.000957 0.000694 0.000874 0.002181 0.001202 0.001657 0.000060 
I0428 20:45:40.844532 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:45:42.071602  7412 solver.cpp:224] Iteration 42400 (4.04879 iter/s, 24.6988s/100 iters), loss = 0.0440209
I0428 20:45:42.071602  7412 solver.cpp:243]     Train net output #0: loss = 0.0440208 (* 1 = 0.0440208 loss)
I0428 20:45:42.071602  7412 sgd_solver.cpp:137] Iteration 42400, lr = 0.001
I0428 20:45:42.081604  7412 sgd_solver.cpp:169] scale layer:0.441115 0.460368 0.516243 0.558101 0.513902 0.484563 0.494068 0.489113 0.487689 0.488523 0.499392 0.482253 0.459191 0.431972 0.420680 0.387294 0.351794 0.313358 0.308000 1.031126 
I0428 20:45:42.082603  7412 sgd_solver.cpp:200] weight diff/data:0.000263 0.009468 0.000814 0.001364 0.000907 0.000518 0.000744 0.000722 0.000730 0.001087 0.000682 0.001011 0.000943 0.000480 0.000778 0.000548 0.001188 0.000656 0.000680 0.000634 0.000051 
I0428 20:46:06.556002  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_42500.caffemodel
I0428 20:46:06.564003  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_42500.solverstate
I0428 20:46:06.567003  7412 solver.cpp:336] Iteration 42500, Testing net (#0)
I0428 20:46:15.376507 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:46:15.737529  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8341
I0428 20:46:15.737529  7412 solver.cpp:403]     Test net output #1: loss = 0.633606 (* 1 = 0.633606 loss)
I0428 20:46:15.976541  7412 solver.cpp:224] Iteration 42500 (2.94934 iter/s, 33.9058s/100 iters), loss = 0.0397401
I0428 20:46:15.976541  7412 solver.cpp:243]     Train net output #0: loss = 0.03974 (* 1 = 0.03974 loss)
I0428 20:46:15.976541  7412 sgd_solver.cpp:137] Iteration 42500, lr = 0.001
I0428 20:46:15.985543  7412 sgd_solver.cpp:169] scale layer:0.441404 0.460688 0.516162 0.557985 0.513487 0.484329 0.493700 0.488828 0.487327 0.488312 0.499137 0.481981 0.458953 0.431793 0.420455 0.387076 0.351547 0.313177 0.307855 1.032289 
I0428 20:46:15.987542  7412 sgd_solver.cpp:200] weight diff/data:0.000253 0.000745 0.000916 0.000501 0.000549 0.000516 0.001490 0.001801 0.015102 0.000614 0.000776 0.000640 0.000690 0.000727 0.000592 0.000891 0.001219 0.002201 0.000724 0.000630 0.000047 
I0428 20:46:40.686955  7412 solver.cpp:224] Iteration 42600 (4.04685 iter/s, 24.7106s/100 iters), loss = 0.0637487
I0428 20:46:40.686955  7412 solver.cpp:243]     Train net output #0: loss = 0.0637486 (* 1 = 0.0637486 loss)
I0428 20:46:40.686955  7412 sgd_solver.cpp:137] Iteration 42600, lr = 0.001
I0428 20:46:40.695955  7412 sgd_solver.cpp:169] scale layer:0.441191 0.460275 0.516089 0.557653 0.513361 0.484108 0.493422 0.488556 0.487065 0.488029 0.498924 0.481755 0.458714 0.431523 0.420277 0.386859 0.351412 0.313026 0.307722 1.033259 
I0428 20:46:40.697955  7412 sgd_solver.cpp:200] weight diff/data:0.000364 0.000982 0.000950 0.000727 0.001130 0.000680 0.000879 0.001409 0.000902 0.002163 0.000804 0.000925 0.000897 0.000813 0.000844 0.006016 0.000893 0.001742 0.001815 0.000714 0.000053 
I0428 20:47:05.302363  7412 solver.cpp:224] Iteration 42700 (4.06249 iter/s, 24.6155s/100 iters), loss = 0.0468932
I0428 20:47:05.302363  7412 solver.cpp:243]     Train net output #0: loss = 0.0468932 (* 1 = 0.0468932 loss)
I0428 20:47:05.302363  7412 sgd_solver.cpp:137] Iteration 42700, lr = 0.001
I0428 20:47:05.311363  7412 sgd_solver.cpp:169] scale layer:0.440570 0.460113 0.515734 0.557214 0.513210 0.483811 0.493164 0.488338 0.486910 0.487805 0.498686 0.481554 0.458465 0.431322 0.420089 0.386653 0.351093 0.312820 0.307534 1.034250 
I0428 20:47:05.314363  7412 sgd_solver.cpp:200] weight diff/data:0.000380 0.000642 0.001607 0.000547 0.001231 0.000928 0.001021 0.000751 0.000687 0.002956 0.000927 0.000617 0.001021 0.001423 0.000526 0.001830 0.001231 0.000708 0.000667 0.001239 0.000070 
I0428 20:47:28.642699 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:47:29.859767  7412 solver.cpp:224] Iteration 42800 (4.07207 iter/s, 24.5575s/100 iters), loss = 0.0889781
I0428 20:47:29.859767  7412 solver.cpp:243]     Train net output #0: loss = 0.088978 (* 1 = 0.088978 loss)
I0428 20:47:29.859767  7412 sgd_solver.cpp:137] Iteration 42800, lr = 0.001
I0428 20:47:29.868768  7412 sgd_solver.cpp:169] scale layer:0.440295 0.459664 0.515284 0.556885 0.512806 0.483601 0.492967 0.488199 0.486729 0.487555 0.498482 0.481302 0.458212 0.431168 0.419917 0.386446 0.350931 0.312604 0.307402 1.035283 
I0428 20:47:29.870769  7412 sgd_solver.cpp:200] weight diff/data:0.000221 0.000522 0.000593 0.001341 0.000764 0.000908 0.000695 0.000708 0.001664 0.001051 0.000714 0.000463 0.002190 0.000608 0.003606 0.000652 0.000692 0.000761 0.000525 0.000651 0.000046 
I0428 20:47:54.413172  7412 solver.cpp:224] Iteration 42900 (4.07281 iter/s, 24.5531s/100 iters), loss = 0.027289
I0428 20:47:54.413172  7412 solver.cpp:243]     Train net output #0: loss = 0.0272889 (* 1 = 0.0272889 loss)
I0428 20:47:54.413172  7412 sgd_solver.cpp:137] Iteration 42900, lr = 0.001
I0428 20:47:54.421172  7412 sgd_solver.cpp:169] scale layer:0.440472 0.459856 0.515196 0.556873 0.512671 0.483386 0.492649 0.487883 0.486352 0.487285 0.498212 0.481090 0.457939 0.430920 0.419660 0.386184 0.350693 0.312435 0.307258 1.036397 
I0428 20:47:54.423172  7412 sgd_solver.cpp:200] weight diff/data:0.000346 0.000636 0.000658 0.000560 0.000770 0.000590 0.000751 0.000681 0.000548 0.000975 0.000524 0.000459 0.000671 0.000624 0.000648 0.000710 0.000809 0.000829 0.001127 0.000494 0.000047 
I0428 20:48:18.779566  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_43000.caffemodel
I0428 20:48:18.788566  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_43000.solverstate
I0428 20:48:18.791566  7412 solver.cpp:336] Iteration 43000, Testing net (#0)
I0428 20:48:27.585069 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:48:27.948091  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8344
I0428 20:48:27.948091  7412 solver.cpp:403]     Test net output #1: loss = 0.644874 (* 1 = 0.644874 loss)
I0428 20:48:28.186103  7412 solver.cpp:224] Iteration 43000 (2.96084 iter/s, 33.7742s/100 iters), loss = 0.0479199
I0428 20:48:28.186103  7412 solver.cpp:243]     Train net output #0: loss = 0.0479198 (* 1 = 0.0479198 loss)
I0428 20:48:28.186103  7412 sgd_solver.cpp:137] Iteration 43000, lr = 0.001
I0428 20:48:28.195104  7412 sgd_solver.cpp:169] scale layer:0.440290 0.459761 0.514985 0.556618 0.512484 0.483155 0.492342 0.487673 0.486108 0.486992 0.497966 0.480790 0.457645 0.430629 0.419451 0.386010 0.350545 0.312251 0.307120 1.037429 
I0428 20:48:28.197104  7412 sgd_solver.cpp:200] weight diff/data:0.000319 0.000453 0.000509 0.002101 0.000635 0.000729 0.000937 0.001324 0.000664 0.001500 0.000682 0.000603 0.000867 0.000675 0.000952 0.000646 0.000698 0.000691 0.000865 0.000719 0.000062 
I0428 20:48:52.916518  7412 solver.cpp:224] Iteration 43100 (4.0436 iter/s, 24.7304s/100 iters), loss = 0.0380494
I0428 20:48:52.916518  7412 solver.cpp:243]     Train net output #0: loss = 0.0380493 (* 1 = 0.0380493 loss)
I0428 20:48:52.916518  7412 sgd_solver.cpp:137] Iteration 43100, lr = 0.001
I0428 20:48:52.925518  7412 sgd_solver.cpp:169] scale layer:0.439925 0.459558 0.514697 0.556198 0.512439 0.482902 0.492055 0.487498 0.485875 0.486815 0.497729 0.480516 0.457410 0.430399 0.419249 0.385805 0.350244 0.312018 0.306943 1.038453 
I0428 20:48:52.927518  7412 sgd_solver.cpp:200] weight diff/data:0.000327 0.002077 0.000525 0.000791 0.000939 0.000614 0.000820 0.001286 0.000646 0.000779 0.000593 0.000602 0.000911 0.000855 0.000786 0.001256 0.005731 0.002224 0.001155 0.001041 0.000059 
I0428 20:49:16.460865 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:49:17.697935  7412 solver.cpp:224] Iteration 43200 (4.03513 iter/s, 24.7823s/100 iters), loss = 0.0696707
I0428 20:49:17.697935  7412 solver.cpp:243]     Train net output #0: loss = 0.0696707 (* 1 = 0.0696707 loss)
I0428 20:49:17.698935  7412 sgd_solver.cpp:137] Iteration 43200, lr = 0.001
I0428 20:49:17.706936  7412 sgd_solver.cpp:169] scale layer:0.439452 0.459097 0.514152 0.555986 0.512020 0.482693 0.491800 0.487242 0.485656 0.486587 0.497490 0.480270 0.457212 0.430238 0.419061 0.385595 0.350001 0.311834 0.306824 1.039430 
I0428 20:49:17.709936  7412 sgd_solver.cpp:200] weight diff/data:0.000253 0.000345 0.000528 0.001398 0.000446 0.000455 0.000726 0.000486 0.000582 0.000569 0.000606 0.000511 0.000484 0.000604 0.000621 0.001103 0.000716 0.000728 0.010975 0.000975 0.000058 
I0428 20:49:42.436350  7412 solver.cpp:224] Iteration 43300 (4.04227 iter/s, 24.7386s/100 iters), loss = 0.0307797
I0428 20:49:42.436350  7412 solver.cpp:243]     Train net output #0: loss = 0.0307796 (* 1 = 0.0307796 loss)
I0428 20:49:42.436350  7412 sgd_solver.cpp:137] Iteration 43300, lr = 0.001
I0428 20:49:42.445351  7412 sgd_solver.cpp:169] scale layer:0.439772 0.459154 0.514118 0.555923 0.511569 0.482447 0.491532 0.486949 0.485316 0.486391 0.497218 0.479994 0.456976 0.430013 0.418818 0.385320 0.349769 0.311682 0.306678 1.040501 
I0428 20:49:42.447351  7412 sgd_solver.cpp:200] weight diff/data:0.000308 0.000815 0.000803 0.000706 0.000827 0.000784 0.000997 0.000664 0.000823 0.000668 0.000505 0.000529 0.000542 0.000927 0.000639 0.000646 0.001385 0.001215 0.000900 0.001108 0.000044 
I0428 20:50:07.024757  7412 solver.cpp:224] Iteration 43400 (4.0669 iter/s, 24.5888s/100 iters), loss = 0.0391693
I0428 20:50:07.024757  7412 solver.cpp:243]     Train net output #0: loss = 0.0391692 (* 1 = 0.0391692 loss)
I0428 20:50:07.024757  7412 sgd_solver.cpp:137] Iteration 43400, lr = 0.001
I0428 20:50:07.033757  7412 sgd_solver.cpp:169] scale layer:0.439541 0.459077 0.513962 0.555597 0.511325 0.482285 0.491286 0.486692 0.485097 0.486154 0.496979 0.479809 0.456721 0.429747 0.418673 0.385123 0.349570 0.311447 0.306529 1.041459 
I0428 20:50:07.035758  7412 sgd_solver.cpp:200] weight diff/data:0.000413 0.000716 0.000462 0.000497 0.000598 0.000497 0.000849 0.000815 0.000828 0.000649 0.000791 0.000590 0.000732 0.000621 0.001008 0.000728 0.001018 0.000736 0.086305 0.000774 0.000060 
I0428 20:50:31.375149  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_43500.caffemodel
I0428 20:50:31.383150  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_43500.solverstate
I0428 20:50:31.386150  7412 solver.cpp:336] Iteration 43500, Testing net (#0)
I0428 20:50:40.157652 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:50:40.520673  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8317
I0428 20:50:40.520673  7412 solver.cpp:403]     Test net output #1: loss = 0.659832 (* 1 = 0.659832 loss)
I0428 20:50:40.754686  7412 solver.cpp:224] Iteration 43500 (2.96468 iter/s, 33.7305s/100 iters), loss = 0.0401037
I0428 20:50:40.754686  7412 solver.cpp:243]     Train net output #0: loss = 0.0401037 (* 1 = 0.0401037 loss)
I0428 20:50:40.754686  7412 sgd_solver.cpp:137] Iteration 43500, lr = 0.001
I0428 20:50:40.763686  7412 sgd_solver.cpp:169] scale layer:0.439438 0.458704 0.513613 0.555181 0.511220 0.482024 0.491021 0.486425 0.484865 0.485908 0.496735 0.479584 0.456513 0.429529 0.418468 0.384929 0.349271 0.311222 0.306371 1.042431 
I0428 20:50:40.765686  7412 sgd_solver.cpp:200] weight diff/data:0.000288 0.000612 0.000821 0.000442 0.000460 0.000483 0.000887 0.001097 0.000617 0.000690 0.000833 0.000540 0.000666 0.000738 0.000699 0.000729 0.000736 0.000820 0.002151 0.000570 0.000071 
I0428 20:51:04.124022 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:51:05.343092  7412 solver.cpp:224] Iteration 43600 (4.06691 iter/s, 24.5887s/100 iters), loss = 0.0395293
I0428 20:51:05.343092  7412 solver.cpp:243]     Train net output #0: loss = 0.0395292 (* 1 = 0.0395292 loss)
I0428 20:51:05.343092  7412 sgd_solver.cpp:137] Iteration 43600, lr = 0.001
I0428 20:51:05.352093  7412 sgd_solver.cpp:169] scale layer:0.439017 0.458092 0.513322 0.554857 0.511059 0.481799 0.490798 0.486177 0.484636 0.485636 0.496522 0.479287 0.456275 0.429343 0.418292 0.384766 0.349081 0.310989 0.306214 1.043356 
I0428 20:51:05.354094  7412 sgd_solver.cpp:200] weight diff/data:0.000264 0.000429 0.000751 0.000392 0.000682 0.000410 0.002577 0.000593 0.000546 0.001016 0.000481 0.000920 0.000472 0.000565 0.000568 0.000660 0.000504 0.000559 0.000959 0.000525 0.000051 
I0428 20:51:29.925498  7412 solver.cpp:224] Iteration 43700 (4.06788 iter/s, 24.5829s/100 iters), loss = 0.0283673
I0428 20:51:29.925498  7412 solver.cpp:243]     Train net output #0: loss = 0.0283672 (* 1 = 0.0283672 loss)
I0428 20:51:29.925498  7412 sgd_solver.cpp:137] Iteration 43700, lr = 0.001
I0428 20:51:29.934499  7412 sgd_solver.cpp:169] scale layer:0.439113 0.458341 0.513066 0.554659 0.510639 0.481511 0.490490 0.485877 0.484291 0.485436 0.496279 0.479043 0.456048 0.429159 0.418046 0.384508 0.348789 0.310811 0.306062 1.044371 
I0428 20:51:29.936499  7412 sgd_solver.cpp:200] weight diff/data:0.000225 0.000577 0.000541 0.000488 0.000496 0.000467 0.000522 0.000571 0.000390 0.000665 0.000444 0.000455 0.000813 0.000585 0.000741 0.000725 0.000739 0.000478 0.000617 0.000660 0.000052 
I0428 20:51:54.510905  7412 solver.cpp:224] Iteration 43800 (4.06733 iter/s, 24.5862s/100 iters), loss = 0.0266177
I0428 20:51:54.510905  7412 solver.cpp:243]     Train net output #0: loss = 0.0266176 (* 1 = 0.0266176 loss)
I0428 20:51:54.510905  7412 sgd_solver.cpp:137] Iteration 43800, lr = 0.001
I0428 20:51:54.519906  7412 sgd_solver.cpp:169] scale layer:0.439011 0.458038 0.512944 0.554385 0.510334 0.481306 0.490241 0.485630 0.484031 0.485135 0.496067 0.478857 0.455781 0.428932 0.417892 0.384360 0.348620 0.310607 0.305941 1.045226 
I0428 20:51:54.521905  7412 sgd_solver.cpp:200] weight diff/data:0.000314 0.000424 0.000743 0.000687 0.000854 0.000582 0.001004 0.000742 0.000553 0.000570 0.000590 0.000656 0.000736 0.000585 0.000575 0.000627 0.000653 0.000856 0.000508 0.000540 0.000042 
I0428 20:52:19.112311  7412 solver.cpp:224] Iteration 43900 (4.06477 iter/s, 24.6016s/100 iters), loss = 0.0278876
I0428 20:52:19.112311  7412 solver.cpp:243]     Train net output #0: loss = 0.0278875 (* 1 = 0.0278875 loss)
I0428 20:52:19.112311  7412 sgd_solver.cpp:137] Iteration 43900, lr = 0.001
I0428 20:52:19.121312  7412 sgd_solver.cpp:169] scale layer:0.438553 0.457633 0.512603 0.554091 0.510194 0.480996 0.490041 0.485449 0.483872 0.484943 0.495837 0.478598 0.455570 0.428690 0.417710 0.384129 0.348379 0.310378 0.305746 1.046167 
I0428 20:52:19.123312  7412 sgd_solver.cpp:200] weight diff/data:0.000500 0.000622 0.000775 0.000548 0.002065 0.000418 0.001103 0.001274 0.000605 0.000534 0.000589 0.034315 0.001241 0.001013 0.000533 0.001133 0.000546 0.000854 0.000474 0.000572 0.000072 
I0428 20:52:42.570653 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:52:43.552709  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_44000.caffemodel
I0428 20:52:43.561710  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_44000.solverstate
I0428 20:52:43.563710  7412 solver.cpp:336] Iteration 44000, Testing net (#0)
I0428 20:52:52.410217 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:52:52.778237  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8338
I0428 20:52:52.779237  7412 solver.cpp:403]     Test net output #1: loss = 0.669036 (* 1 = 0.669036 loss)
I0428 20:52:53.015251  7412 solver.cpp:224] Iteration 44000 (2.94947 iter/s, 33.9044s/100 iters), loss = 0.0255814
I0428 20:52:53.015251  7412 solver.cpp:243]     Train net output #0: loss = 0.0255813 (* 1 = 0.0255813 loss)
I0428 20:52:53.016252  7412 sgd_solver.cpp:137] Iteration 44000, lr = 0.001
I0428 20:52:53.025251  7412 sgd_solver.cpp:169] scale layer:0.437932 0.457300 0.512226 0.553889 0.510015 0.480778 0.489807 0.485232 0.483631 0.484692 0.495588 0.478312 0.455315 0.428477 0.417507 0.383894 0.348133 0.310163 0.305591 1.047078 
I0428 20:52:53.026252  7412 sgd_solver.cpp:200] weight diff/data:0.000225 0.000357 0.000404 0.000560 0.000446 0.000400 0.000664 0.000833 0.000611 0.000468 0.000582 0.000478 0.000480 0.000635 0.000560 0.000760 0.000595 0.000585 0.000575 0.002237 0.000055 
I0428 20:53:17.643659  7412 solver.cpp:224] Iteration 44100 (4.0603 iter/s, 24.6287s/100 iters), loss = 0.0211622
I0428 20:53:17.644659  7412 solver.cpp:243]     Train net output #0: loss = 0.0211621 (* 1 = 0.0211621 loss)
I0428 20:53:17.644659  7412 sgd_solver.cpp:137] Iteration 44100, lr = 0.001
I0428 20:53:17.652660  7412 sgd_solver.cpp:169] scale layer:0.438212 0.457394 0.511954 0.553717 0.509825 0.480581 0.489545 0.484944 0.483321 0.484475 0.495281 0.478105 0.455092 0.428281 0.417290 0.383687 0.347898 0.310006 0.305448 1.048083 
I0428 20:53:17.654660  7412 sgd_solver.cpp:200] weight diff/data:0.000368 0.000380 0.000682 0.000715 0.000473 0.000576 0.000509 0.000802 0.000392 0.000624 0.000490 0.000458 0.000618 0.000395 0.000492 0.000528 0.001329 0.001048 0.001307 0.000708 0.000046 
I0428 20:53:42.255067  7412 solver.cpp:224] Iteration 44200 (4.06319 iter/s, 24.6112s/100 iters), loss = 0.0279362
I0428 20:53:42.255067  7412 solver.cpp:243]     Train net output #0: loss = 0.0279361 (* 1 = 0.0279361 loss)
I0428 20:53:42.255067  7412 sgd_solver.cpp:137] Iteration 44200, lr = 0.001
I0428 20:53:42.263067  7412 sgd_solver.cpp:169] scale layer:0.438101 0.457052 0.511842 0.553412 0.509460 0.480271 0.489282 0.484686 0.483050 0.484186 0.494970 0.477880 0.454815 0.428020 0.417078 0.383495 0.347772 0.309806 0.305305 1.048949 
I0428 20:53:42.265069  7412 sgd_solver.cpp:200] weight diff/data:0.000445 0.000478 0.000431 0.000650 0.000759 0.000431 0.000570 0.000577 0.000460 0.000731 0.000573 0.000473 0.001540 0.000516 0.000705 0.000678 0.000933 0.000551 0.000587 0.000463 0.000096 
I0428 20:54:06.867475  7412 solver.cpp:224] Iteration 44300 (4.06296 iter/s, 24.6126s/100 iters), loss = 0.022288
I0428 20:54:06.867475  7412 solver.cpp:243]     Train net output #0: loss = 0.022288 (* 1 = 0.022288 loss)
I0428 20:54:06.867475  7412 sgd_solver.cpp:137] Iteration 44300, lr = 0.001
I0428 20:54:06.875475  7412 sgd_solver.cpp:169] scale layer:0.437426 0.456748 0.511592 0.553016 0.509222 0.480013 0.489016 0.484476 0.482854 0.483974 0.494719 0.477638 0.454598 0.427821 0.416848 0.383331 0.347530 0.309629 0.305148 1.049802 
I0428 20:54:06.877475  7412 sgd_solver.cpp:200] weight diff/data:0.000212 0.000342 0.000402 0.000550 0.000622 0.000490 0.001075 0.000872 0.000503 0.000478 0.000488 0.000404 0.000572 0.000582 0.000494 0.000575 0.000599 0.000614 0.000643 0.001843 0.000046 
I0428 20:54:30.234812 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:54:31.456881  7412 solver.cpp:224] Iteration 44400 (4.06669 iter/s, 24.59s/100 iters), loss = 0.0589951
I0428 20:54:31.456881  7412 solver.cpp:243]     Train net output #0: loss = 0.0589951 (* 1 = 0.0589951 loss)
I0428 20:54:31.456881  7412 sgd_solver.cpp:137] Iteration 44400, lr = 0.001
I0428 20:54:31.464882  7412 sgd_solver.cpp:169] scale layer:0.437004 0.456397 0.511350 0.552773 0.508946 0.479771 0.488762 0.484241 0.482649 0.483742 0.494462 0.477383 0.454355 0.427670 0.416676 0.383141 0.347272 0.309449 0.305005 1.050653 
I0428 20:54:31.466882  7412 sgd_solver.cpp:200] weight diff/data:0.000357 0.000395 0.000475 0.000620 0.000656 0.001054 0.000766 0.000796 0.000471 0.000631 0.000705 0.000557 0.000717 0.070771 0.000851 0.000922 0.000606 0.000785 0.000994 0.000848 0.000091 
I0428 20:54:55.819275  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_44500.caffemodel
I0428 20:54:55.827275  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_44500.solverstate
I0428 20:54:55.830276  7412 solver.cpp:336] Iteration 44500, Testing net (#0)
I0428 20:55:04.592777 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:55:04.956797  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8327
I0428 20:55:04.957798  7412 solver.cpp:403]     Test net output #1: loss = 0.681727 (* 1 = 0.681727 loss)
I0428 20:55:05.191812  7412 solver.cpp:224] Iteration 44500 (2.96415 iter/s, 33.7365s/100 iters), loss = 0.0221641
I0428 20:55:05.191812  7412 solver.cpp:243]     Train net output #0: loss = 0.022164 (* 1 = 0.022164 loss)
I0428 20:55:05.191812  7412 sgd_solver.cpp:137] Iteration 44500, lr = 0.001
I0428 20:55:05.201812  7412 sgd_solver.cpp:169] scale layer:0.437066 0.456438 0.511128 0.552597 0.508687 0.479512 0.488503 0.483957 0.482308 0.483494 0.494213 0.477127 0.454103 0.427461 0.416449 0.382879 0.347006 0.309289 0.304850 1.051537 
I0428 20:55:05.202811  7412 sgd_solver.cpp:200] weight diff/data:0.000242 0.000642 0.000959 0.000960 0.000855 0.000303 0.000460 0.000529 0.000464 0.000512 0.000482 0.000418 0.000505 0.000492 0.001692 0.000597 0.001740 0.000748 0.000535 0.004715 0.000116 
I0428 20:55:29.805219  7412 solver.cpp:224] Iteration 44600 (4.06279 iter/s, 24.6137s/100 iters), loss = 0.0328739
I0428 20:55:29.805219  7412 solver.cpp:243]     Train net output #0: loss = 0.0328738 (* 1 = 0.0328738 loss)
I0428 20:55:29.805219  7412 sgd_solver.cpp:137] Iteration 44600, lr = 0.001
I0428 20:55:29.814219  7412 sgd_solver.cpp:169] scale layer:0.436747 0.456331 0.510948 0.552362 0.508319 0.479302 0.488245 0.483703 0.482037 0.483204 0.494000 0.476959 0.453847 0.427213 0.416249 0.382691 0.346802 0.309108 0.304702 1.052407 
I0428 20:55:29.816220  7412 sgd_solver.cpp:200] weight diff/data:0.000290 0.000692 0.000632 0.000525 0.001152 0.000756 0.001380 0.001168 0.001077 0.000782 0.052693 0.001272 0.000795 0.000693 0.000861 0.000676 0.000582 0.000848 0.009094 0.000963 0.000225 
I0428 20:55:54.416626  7412 solver.cpp:224] Iteration 44700 (4.06309 iter/s, 24.6118s/100 iters), loss = 0.0352176
I0428 20:55:54.416626  7412 solver.cpp:243]     Train net output #0: loss = 0.0352175 (* 1 = 0.0352175 loss)
I0428 20:55:54.416626  7412 sgd_solver.cpp:137] Iteration 44700, lr = 0.001
I0428 20:55:54.425627  7412 sgd_solver.cpp:169] scale layer:0.436337 0.456020 0.510661 0.551980 0.508140 0.479055 0.487974 0.483495 0.481833 0.482990 0.493776 0.476700 0.453625 0.427018 0.416021 0.382483 0.346539 0.308899 0.304524 1.053190 
I0428 20:55:54.427628  7412 sgd_solver.cpp:200] weight diff/data:0.000313 0.000423 0.000457 0.000423 0.000681 0.003603 0.001128 0.001148 0.000409 0.001569 0.000601 0.000498 0.000631 0.000514 0.000490 0.000691 0.000557 0.001174 0.000475 0.000979 0.000585 
I0428 20:56:17.927971 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:56:19.149041  7412 solver.cpp:224] Iteration 44800 (4.04322 iter/s, 24.7327s/100 iters), loss = 0.0519253
I0428 20:56:19.149041  7412 solver.cpp:243]     Train net output #0: loss = 0.0519252 (* 1 = 0.0519252 loss)
I0428 20:56:19.149041  7412 sgd_solver.cpp:137] Iteration 44800, lr = 0.001
I0428 20:56:19.158041  7412 sgd_solver.cpp:169] scale layer:0.436099 0.455709 0.510429 0.551788 0.507916 0.478817 0.487771 0.483257 0.481653 0.482734 0.493571 0.476438 0.453430 0.426795 0.415871 0.382255 0.346293 0.308698 0.304346 1.053997 
I0428 20:56:19.160042  7412 sgd_solver.cpp:200] weight diff/data:0.000463 0.000446 0.000442 0.000639 0.000793 0.000676 0.000570 0.000782 0.000575 0.001069 0.000539 0.000851 0.001656 0.000465 0.000473 0.000680 0.001132 0.000723 0.000613 0.000402 0.000135 
I0428 20:56:43.762449  7412 solver.cpp:224] Iteration 44900 (4.06283 iter/s, 24.6134s/100 iters), loss = 0.0224229
I0428 20:56:43.762449  7412 solver.cpp:243]     Train net output #0: loss = 0.0224228 (* 1 = 0.0224228 loss)
I0428 20:56:43.762449  7412 sgd_solver.cpp:137] Iteration 44900, lr = 0.001
I0428 20:56:43.771450  7412 sgd_solver.cpp:169] scale layer:0.436281 0.455669 0.510177 0.551681 0.507584 0.478583 0.487505 0.482948 0.481390 0.482488 0.493347 0.476225 0.453174 0.426553 0.415667 0.382019 0.346048 0.308491 0.304186 1.054866 
I0428 20:56:43.773449  7412 sgd_solver.cpp:200] weight diff/data:0.000306 0.000479 0.000829 0.000469 0.000490 0.000348 0.000562 0.000990 0.000840 0.001323 0.000800 0.000539 0.000821 0.001645 0.000696 0.000668 0.000586 0.000638 0.000512 0.000594 0.000045 
I0428 20:57:08.123842  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_45000.caffemodel
I0428 20:57:08.131842  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_45000.solverstate
I0428 20:57:08.134843  7412 solver.cpp:336] Iteration 45000, Testing net (#0)
I0428 20:57:16.899344 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:57:17.264365  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8321
I0428 20:57:17.265365  7412 solver.cpp:403]     Test net output #1: loss = 0.69361 (* 1 = 0.69361 loss)
I0428 20:57:17.500378  7412 solver.cpp:224] Iteration 45000 (2.96398 iter/s, 33.7385s/100 iters), loss = 0.0196086
I0428 20:57:17.500378  7412 solver.cpp:243]     Train net output #0: loss = 0.0196085 (* 1 = 0.0196085 loss)
I0428 20:57:17.500378  7412 sgd_solver.cpp:137] Iteration 45000, lr = 0.001
I0428 20:57:17.509379  7412 sgd_solver.cpp:169] scale layer:0.435957 0.455471 0.509998 0.551341 0.507385 0.478335 0.487198 0.482668 0.481123 0.482184 0.493066 0.476048 0.452964 0.426272 0.415495 0.381831 0.345822 0.308304 0.304035 1.055686 
I0428 20:57:17.512379  7412 sgd_solver.cpp:200] weight diff/data:0.000219 0.000518 0.000392 0.000516 0.001242 0.000389 0.000443 0.001168 0.000599 0.000714 0.000930 0.000563 0.000669 0.000548 0.000575 0.000842 0.000552 0.016469 0.000533 0.000493 0.000071 
I0428 20:57:42.129787  7412 solver.cpp:224] Iteration 45100 (4.06018 iter/s, 24.6294s/100 iters), loss = 0.0170551
I0428 20:57:42.129787  7412 solver.cpp:243]     Train net output #0: loss = 0.017055 (* 1 = 0.017055 loss)
I0428 20:57:42.129787  7412 sgd_solver.cpp:137] Iteration 45100, lr = 0.001
I0428 20:57:42.138788  7412 sgd_solver.cpp:169] scale layer:0.435848 0.455279 0.509805 0.550963 0.507257 0.478029 0.486924 0.482468 0.480923 0.481966 0.492837 0.475842 0.452743 0.426051 0.415267 0.381604 0.345573 0.308098 0.303880 1.056439 
I0428 20:57:42.140789  7412 sgd_solver.cpp:200] weight diff/data:0.000224 0.000376 0.000401 0.000527 0.000662 0.000537 0.000704 0.000617 0.001060 0.000731 0.000505 0.000527 0.000529 0.000842 0.000439 0.000574 0.000762 0.000590 0.000411 0.000523 0.000041 
I0428 20:58:05.525125 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:58:06.747195  7412 solver.cpp:224] Iteration 45200 (4.06211 iter/s, 24.6178s/100 iters), loss = 0.0499062
I0428 20:58:06.748195  7412 solver.cpp:243]     Train net output #0: loss = 0.0499061 (* 1 = 0.0499061 loss)
I0428 20:58:06.748195  7412 sgd_solver.cpp:137] Iteration 45200, lr = 0.001
I0428 20:58:06.756196  7412 sgd_solver.cpp:169] scale layer:0.435527 0.454830 0.509451 0.550723 0.507170 0.477820 0.486661 0.482222 0.480716 0.481729 0.492616 0.475566 0.452500 0.425838 0.415071 0.381406 0.345354 0.307908 0.303732 1.057209 
I0428 20:58:06.758196  7412 sgd_solver.cpp:200] weight diff/data:0.001168 0.000314 0.001629 0.000353 0.000856 0.000536 0.001059 0.000649 0.000553 0.000805 0.000522 0.000773 0.000496 0.000597 0.000492 0.000442 0.000639 0.000938 0.000505 0.000443 0.000037 
I0428 20:58:31.457608  7412 solver.cpp:224] Iteration 45300 (4.04685 iter/s, 24.7106s/100 iters), loss = 0.020606
I0428 20:58:31.458608  7412 solver.cpp:243]     Train net output #0: loss = 0.0206059 (* 1 = 0.0206059 loss)
I0428 20:58:31.458608  7412 sgd_solver.cpp:137] Iteration 45300, lr = 0.001
I0428 20:58:31.466609  7412 sgd_solver.cpp:169] scale layer:0.435621 0.454770 0.509347 0.550533 0.506753 0.477583 0.486434 0.481964 0.480475 0.481469 0.492345 0.475350 0.452252 0.425635 0.414844 0.381166 0.345155 0.307754 0.303572 1.058044 
I0428 20:58:31.468610  7412 sgd_solver.cpp:200] weight diff/data:0.000321 0.000380 0.000494 0.000698 0.000639 0.000364 0.000630 0.000575 0.000621 0.000726 0.000627 0.000731 0.001986 0.000577 0.000462 0.000526 0.000459 0.000539 0.000699 0.000596 0.000070 
I0428 20:58:56.214025  7412 solver.cpp:224] Iteration 45400 (4.0393 iter/s, 24.7567s/100 iters), loss = 0.0240659
I0428 20:58:56.215024  7412 solver.cpp:243]     Train net output #0: loss = 0.0240658 (* 1 = 0.0240658 loss)
I0428 20:58:56.215024  7412 sgd_solver.cpp:137] Iteration 45400, lr = 0.001
I0428 20:58:56.224025  7412 sgd_solver.cpp:169] scale layer:0.435484 0.454630 0.509210 0.550299 0.506530 0.477358 0.486149 0.481733 0.480187 0.481198 0.492107 0.475125 0.451997 0.425420 0.414642 0.380960 0.344989 0.307530 0.303414 1.058833 
I0428 20:58:56.226025  7412 sgd_solver.cpp:200] weight diff/data:0.000195 0.000417 0.000496 0.000345 0.000572 0.000736 0.001042 0.000560 0.000576 0.000795 0.000573 0.001302 0.000993 0.000681 0.000472 0.000582 0.000645 0.000799 0.000712 0.000622 0.000063 
I0428 20:59:20.730427  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_45500.caffemodel
I0428 20:59:20.739428  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_45500.solverstate
I0428 20:59:20.742427  7412 solver.cpp:336] Iteration 45500, Testing net (#0)
I0428 20:59:29.525930 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:59:29.886950  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8307
I0428 20:59:29.886950  7412 solver.cpp:403]     Test net output #1: loss = 0.707333 (* 1 = 0.707333 loss)
I0428 20:59:30.121964  7412 solver.cpp:224] Iteration 45500 (2.94916 iter/s, 33.908s/100 iters), loss = 0.0189945
I0428 20:59:30.121964  7412 solver.cpp:243]     Train net output #0: loss = 0.0189944 (* 1 = 0.0189944 loss)
I0428 20:59:30.121964  7412 sgd_solver.cpp:137] Iteration 45500, lr = 0.001
I0428 20:59:30.130964  7412 sgd_solver.cpp:169] scale layer:0.435059 0.454400 0.508933 0.549978 0.506359 0.477096 0.485898 0.481508 0.479959 0.480957 0.491863 0.474882 0.451767 0.425189 0.414437 0.380746 0.344737 0.307303 0.303251 1.059564 
I0428 20:59:30.132964  7412 sgd_solver.cpp:200] weight diff/data:0.000206 0.000265 0.000301 0.000330 0.000521 0.000500 0.000361 0.000383 0.000448 0.000373 0.000729 0.000853 0.000674 0.000685 0.001648 0.001831 0.000347 0.000563 0.000691 0.000562 0.000040 
I0428 20:59:53.527303 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:59:54.751374  7412 solver.cpp:224] Iteration 45600 (4.0601 iter/s, 24.6299s/100 iters), loss = 0.0216633
I0428 20:59:54.751374  7412 solver.cpp:243]     Train net output #0: loss = 0.0216632 (* 1 = 0.0216632 loss)
I0428 20:59:54.751374  7412 sgd_solver.cpp:137] Iteration 45600, lr = 0.001
I0428 20:59:54.760373  7412 sgd_solver.cpp:169] scale layer:0.434577 0.454122 0.508627 0.549752 0.506160 0.476855 0.485676 0.481239 0.479775 0.480695 0.491605 0.474593 0.451502 0.425030 0.414262 0.380570 0.344542 0.307105 0.303096 1.060243 
I0428 20:59:54.762373  7412 sgd_solver.cpp:200] weight diff/data:0.000211 0.000278 0.003735 0.000295 0.000409 0.000425 0.000522 0.000516 0.000396 0.000685 0.000528 0.000413 0.001616 0.000382 0.000906 0.000377 0.000497 0.000657 0.000736 0.000539 0.000042 
I0428 21:00:19.366780  7412 solver.cpp:224] Iteration 45700 (4.06234 iter/s, 24.6163s/100 iters), loss = 0.0147957
I0428 21:00:19.367781  7412 solver.cpp:243]     Train net output #0: loss = 0.0147956 (* 1 = 0.0147956 loss)
I0428 21:00:19.367781  7412 sgd_solver.cpp:137] Iteration 45700, lr = 0.001
I0428 21:00:19.376781  7412 sgd_solver.cpp:169] scale layer:0.434522 0.453977 0.508382 0.549577 0.505966 0.476634 0.485407 0.481009 0.479465 0.480488 0.491361 0.474415 0.451301 0.424821 0.414025 0.380309 0.344304 0.306930 0.302941 1.061030 
I0428 21:00:19.378782  7412 sgd_solver.cpp:200] weight diff/data:0.000156 0.000411 0.000799 0.000272 0.000349 0.000462 0.000781 0.000366 0.000395 0.000367 0.000425 0.000370 0.000359 0.000344 0.000409 0.001209 0.000410 0.000427 0.000556 0.000405 0.000044 
I0428 21:00:43.952188  7412 solver.cpp:224] Iteration 45800 (4.06752 iter/s, 24.585s/100 iters), loss = 0.0180804
I0428 21:00:43.952188  7412 solver.cpp:243]     Train net output #0: loss = 0.0180802 (* 1 = 0.0180802 loss)
I0428 21:00:43.952188  7412 sgd_solver.cpp:137] Iteration 45800, lr = 0.001
I0428 21:00:43.961187  7412 sgd_solver.cpp:169] scale layer:0.434324 0.453771 0.508135 0.549347 0.505728 0.476402 0.485125 0.480762 0.479239 0.480209 0.491107 0.474200 0.451033 0.424625 0.413808 0.380091 0.344155 0.306697 0.302786 1.061744 
I0428 21:00:43.963187  7412 sgd_solver.cpp:200] weight diff/data:0.000198 0.000569 0.000310 0.000873 0.000593 0.001053 0.000391 0.000448 0.001347 0.000396 0.000511 0.000417 0.000821 0.000450 0.000476 0.000461 0.000408 0.000601 0.000502 0.000367 0.000040 
I0428 21:01:08.555594  7412 solver.cpp:224] Iteration 45900 (4.06442 iter/s, 24.6038s/100 iters), loss = 0.0164866
I0428 21:01:08.555594  7412 solver.cpp:243]     Train net output #0: loss = 0.0164864 (* 1 = 0.0164864 loss)
I0428 21:01:08.555594  7412 sgd_solver.cpp:137] Iteration 45900, lr = 0.001
I0428 21:01:08.564594  7412 sgd_solver.cpp:169] scale layer:0.434077 0.453416 0.507930 0.549005 0.505659 0.476154 0.484820 0.480514 0.479043 0.479977 0.490891 0.473934 0.450827 0.424422 0.413625 0.379871 0.343919 0.306479 0.302621 1.062409 
I0428 21:01:08.566596  7412 sgd_solver.cpp:200] weight diff/data:0.000160 0.000931 0.002151 0.000342 0.000607 0.000412 0.000579 0.000400 0.000515 0.000411 0.000538 0.000784 0.000363 0.000437 0.000362 0.000791 0.000727 0.000979 0.000528 0.000366 0.000033 
I0428 21:01:31.945932 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:01:32.925988  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_46000.caffemodel
I0428 21:01:32.934988  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_46000.solverstate
I0428 21:01:32.936988  7412 solver.cpp:336] Iteration 46000, Testing net (#0)
I0428 21:01:41.700490 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:01:42.064512  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8303
I0428 21:01:42.064512  7412 solver.cpp:403]     Test net output #1: loss = 0.714732 (* 1 = 0.714732 loss)
I0428 21:01:42.298524  7412 solver.cpp:224] Iteration 46000 (2.9635 iter/s, 33.7438s/100 iters), loss = 0.0445236
I0428 21:01:42.298524  7412 solver.cpp:243]     Train net output #0: loss = 0.0445234 (* 1 = 0.0445234 loss)
I0428 21:01:42.298524  7412 sgd_solver.cpp:137] Iteration 46000, lr = 0.001
I0428 21:01:42.307524  7412 sgd_solver.cpp:169] scale layer:0.433577 0.453010 0.507555 0.548668 0.505360 0.475942 0.484562 0.480270 0.478836 0.479737 0.490650 0.473666 0.450626 0.424234 0.413441 0.379687 0.343707 0.306294 0.302479 1.063010 
I0428 21:01:42.309525  7412 sgd_solver.cpp:200] weight diff/data:0.000209 0.000279 0.000357 0.000349 0.000388 0.000529 0.000473 0.000412 0.000346 0.001309 0.000481 0.000438 0.000446 0.001014 0.000603 0.001111 0.001222 0.001218 0.000450 0.000411 0.000033 
I0428 21:02:06.917932  7412 solver.cpp:224] Iteration 46100 (4.06168 iter/s, 24.6203s/100 iters), loss = 0.0162312
I0428 21:02:06.918932  7412 solver.cpp:243]     Train net output #0: loss = 0.016231 (* 1 = 0.016231 loss)
I0428 21:02:06.918932  7412 sgd_solver.cpp:137] Iteration 46100, lr = 0.001
I0428 21:02:06.926934  7412 sgd_solver.cpp:169] scale layer:0.433852 0.452985 0.507439 0.548429 0.504938 0.475680 0.484327 0.479998 0.478562 0.479550 0.490372 0.473433 0.450332 0.424010 0.413199 0.379466 0.343482 0.306166 0.302334 1.063746 
I0428 21:02:06.928933  7412 sgd_solver.cpp:200] weight diff/data:0.000258 0.000361 0.000536 0.000424 0.000550 0.000415 0.001168 0.000571 0.000518 0.000761 0.000701 0.000436 0.000513 0.000463 0.000638 0.000455 0.000456 0.000604 0.000916 0.000541 0.000046 
I0428 21:02:31.548341  7412 solver.cpp:224] Iteration 46200 (4.06005 iter/s, 24.6302s/100 iters), loss = 0.0219586
I0428 21:02:31.548341  7412 solver.cpp:243]     Train net output #0: loss = 0.0219584 (* 1 = 0.0219584 loss)
I0428 21:02:31.548341  7412 sgd_solver.cpp:137] Iteration 46200, lr = 0.001
I0428 21:02:31.556341  7412 sgd_solver.cpp:169] scale layer:0.433534 0.452925 0.507347 0.548157 0.504745 0.475473 0.484108 0.479747 0.478292 0.479256 0.490116 0.473162 0.450101 0.423745 0.413026 0.379282 0.343261 0.305953 0.302169 1.064418 
I0428 21:02:31.558341  7412 sgd_solver.cpp:200] weight diff/data:0.000192 0.000327 0.003082 0.000382 0.000450 0.000539 0.000395 0.000818 0.000546 0.000544 0.000472 0.000546 0.000373 0.000425 0.000952 0.000570 0.000544 0.000770 0.000631 0.002169 0.000034 
I0428 21:02:56.137748  7412 solver.cpp:224] Iteration 46300 (4.06662 iter/s, 24.5905s/100 iters), loss = 0.0181819
I0428 21:02:56.138747  7412 solver.cpp:243]     Train net output #0: loss = 0.0181818 (* 1 = 0.0181818 loss)
I0428 21:02:56.138747  7412 sgd_solver.cpp:137] Iteration 46300, lr = 0.001
I0428 21:02:56.147748  7412 sgd_solver.cpp:169] scale layer:0.433207 0.452819 0.507015 0.547845 0.504606 0.475218 0.483872 0.479508 0.478050 0.479022 0.489890 0.472906 0.449892 0.423527 0.412817 0.379088 0.343023 0.305760 0.301996 1.065021 
I0428 21:02:56.149749  7412 sgd_solver.cpp:200] weight diff/data:0.000236 0.000271 0.000994 0.000310 0.000330 0.000665 0.000320 0.000387 0.000479 0.000343 0.000374 0.000372 0.000436 0.000494 0.000366 0.001066 0.000812 0.000646 0.000362 0.000405 0.000038 
I0428 21:03:19.544086 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:03:20.765156  7412 solver.cpp:224] Iteration 46400 (4.06054 iter/s, 24.6272s/100 iters), loss = 0.0191006
I0428 21:03:20.765156  7412 solver.cpp:243]     Train net output #0: loss = 0.0191005 (* 1 = 0.0191005 loss)
I0428 21:03:20.765156  7412 sgd_solver.cpp:137] Iteration 46400, lr = 0.001
I0428 21:03:20.773156  7412 sgd_solver.cpp:169] scale layer:0.432761 0.452394 0.506698 0.547542 0.504313 0.474973 0.483556 0.479248 0.477788 0.478799 0.489636 0.472647 0.449643 0.423324 0.412633 0.378926 0.342784 0.305553 0.301844 1.065626 
I0428 21:03:20.775156  7412 sgd_solver.cpp:200] weight diff/data:0.000157 0.000292 0.000765 0.000321 0.000588 0.000338 0.000594 0.000365 0.000513 0.000536 0.000592 0.000363 0.000783 0.000508 0.000600 0.000430 0.000500 0.001016 0.000414 0.000607 0.000030 
I0428 21:03:45.127549  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_46500.caffemodel
I0428 21:03:45.136550  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_46500.solverstate
I0428 21:03:45.139550  7412 solver.cpp:336] Iteration 46500, Testing net (#0)
I0428 21:03:53.901051 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:03:54.264072  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8324
I0428 21:03:54.264072  7412 solver.cpp:403]     Test net output #1: loss = 0.728027 (* 1 = 0.728027 loss)
I0428 21:03:54.498085  7412 solver.cpp:224] Iteration 46500 (2.96437 iter/s, 33.734s/100 iters), loss = 0.0169019
I0428 21:03:54.498085  7412 solver.cpp:243]     Train net output #0: loss = 0.0169018 (* 1 = 0.0169018 loss)
I0428 21:03:54.498085  7412 sgd_solver.cpp:137] Iteration 46500, lr = 0.001
I0428 21:03:54.507086  7412 sgd_solver.cpp:169] scale layer:0.432766 0.452333 0.506474 0.547316 0.504108 0.474767 0.483330 0.479012 0.477516 0.478574 0.489387 0.472432 0.449416 0.423095 0.412385 0.378710 0.342567 0.305390 0.301686 1.066298 
I0428 21:03:54.508086  7412 sgd_solver.cpp:200] weight diff/data:0.000189 0.000510 0.000377 0.000329 0.000861 0.000474 0.000398 0.000446 0.000379 0.000603 0.000524 0.000386 0.000484 0.000822 0.001054 0.000547 0.000482 0.000871 0.000606 0.001015 0.000041 
I0428 21:04:19.099493  7412 solver.cpp:224] Iteration 46600 (4.06469 iter/s, 24.6021s/100 iters), loss = 0.0162825
I0428 21:04:19.100492  7412 solver.cpp:243]     Train net output #0: loss = 0.0162823 (* 1 = 0.0162823 loss)
I0428 21:04:19.100492  7412 sgd_solver.cpp:137] Iteration 46600, lr = 0.001
I0428 21:04:19.109493  7412 sgd_solver.cpp:169] scale layer:0.432471 0.452146 0.506214 0.547131 0.503732 0.474502 0.483080 0.478786 0.477270 0.478296 0.489142 0.472213 0.449177 0.422848 0.412169 0.378489 0.342394 0.305156 0.301533 1.066940 
I0428 21:04:19.111493  7412 sgd_solver.cpp:200] weight diff/data:0.000223 0.000385 0.000320 0.000403 0.000384 0.000348 0.000590 0.000738 0.000456 0.000512 0.000449 0.000554 0.000365 0.000431 0.000605 0.000527 0.000399 0.001097 0.000603 0.000559 0.000036 
I0428 21:04:43.754904  7412 solver.cpp:224] Iteration 46700 (4.05597 iter/s, 24.655s/100 iters), loss = 0.010636
I0428 21:04:43.754904  7412 solver.cpp:243]     Train net output #0: loss = 0.0106359 (* 1 = 0.0106359 loss)
I0428 21:04:43.754904  7412 sgd_solver.cpp:137] Iteration 46700, lr = 0.001
I0428 21:04:43.763903  7412 sgd_solver.cpp:169] scale layer:0.432050 0.451817 0.505930 0.546783 0.503467 0.474256 0.482810 0.478573 0.477062 0.478084 0.488923 0.471977 0.448976 0.422622 0.411973 0.378309 0.342143 0.304958 0.301361 1.067500 
I0428 21:04:43.765903  7412 sgd_solver.cpp:200] weight diff/data:0.000226 0.000294 0.000344 0.000343 0.000646 0.000310 0.001014 0.000556 0.000378 0.001095 0.000394 0.000545 0.000395 0.000389 0.000510 0.000441 0.000383 0.000506 0.000501 0.000766 0.000025 
I0428 21:05:07.137240 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:05:08.353310  7412 solver.cpp:224] Iteration 46800 (4.06525 iter/s, 24.5987s/100 iters), loss = 0.0187491
I0428 21:05:08.353310  7412 solver.cpp:243]     Train net output #0: loss = 0.018749 (* 1 = 0.018749 loss)
I0428 21:05:08.354310  7412 sgd_solver.cpp:137] Iteration 46800, lr = 0.001
I0428 21:05:08.362310  7412 sgd_solver.cpp:169] scale layer:0.431852 0.451483 0.505689 0.546491 0.503206 0.473974 0.482549 0.478310 0.476820 0.477820 0.488724 0.471678 0.448735 0.422434 0.411782 0.378082 0.341905 0.304758 0.301199 1.068029 
I0428 21:05:08.364310  7412 sgd_solver.cpp:200] weight diff/data:0.000194 0.000308 0.000338 0.000530 0.000586 0.000358 0.000494 0.000340 0.000519 0.000966 0.000455 0.001037 0.000652 0.000380 0.000568 0.000500 0.000807 0.000645 0.000615 0.000389 0.000036 
I0428 21:05:32.964717  7412 solver.cpp:224] Iteration 46900 (4.06317 iter/s, 24.6113s/100 iters), loss = 0.0134497
I0428 21:05:32.964717  7412 solver.cpp:243]     Train net output #0: loss = 0.0134496 (* 1 = 0.0134496 loss)
I0428 21:05:32.964717  7412 sgd_solver.cpp:137] Iteration 46900, lr = 0.001
I0428 21:05:32.972718  7412 sgd_solver.cpp:169] scale layer:0.431720 0.451444 0.505439 0.546373 0.502998 0.473760 0.482282 0.478043 0.476551 0.477593 0.488408 0.471464 0.448458 0.422198 0.411573 0.377844 0.341651 0.304610 0.301047 1.068679 
I0428 21:05:32.974719  7412 sgd_solver.cpp:200] weight diff/data:0.000217 0.000273 0.000260 0.000518 0.000551 0.000337 0.000357 0.000508 0.000651 0.000336 0.001154 0.000570 0.000355 0.000345 0.000393 0.000322 0.000344 0.004381 0.000361 0.000314 0.000029 
I0428 21:05:57.360113  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_47000.caffemodel
I0428 21:05:57.369113  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_47000.solverstate
I0428 21:05:57.372113  7412 solver.cpp:336] Iteration 47000, Testing net (#0)
I0428 21:06:06.131614 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:06:06.492635  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8322
I0428 21:06:06.492635  7412 solver.cpp:403]     Test net output #1: loss = 0.735592 (* 1 = 0.735592 loss)
I0428 21:06:06.728648  7412 solver.cpp:224] Iteration 47000 (2.96169 iter/s, 33.7645s/100 iters), loss = 0.0173456
I0428 21:06:06.728648  7412 solver.cpp:243]     Train net output #0: loss = 0.0173454 (* 1 = 0.0173454 loss)
I0428 21:06:06.728648  7412 sgd_solver.cpp:137] Iteration 47000, lr = 0.001
I0428 21:06:06.737649  7412 sgd_solver.cpp:169] scale layer:0.431541 0.451077 0.505253 0.546086 0.502734 0.473518 0.482015 0.477764 0.476304 0.477340 0.488180 0.471238 0.448221 0.421991 0.411372 0.377649 0.341477 0.304431 0.300893 1.069249 
I0428 21:06:06.738649  7412 sgd_solver.cpp:200] weight diff/data:0.000228 0.000318 0.000262 0.000314 0.000440 0.000327 0.000475 0.000359 0.000515 0.000439 0.000507 0.001176 0.000842 0.000392 0.000445 0.000427 0.000390 0.000417 0.000509 0.000561 0.000039 
I0428 21:06:31.333056  7412 solver.cpp:224] Iteration 47100 (4.06432 iter/s, 24.6044s/100 iters), loss = 0.0153279
I0428 21:06:31.333056  7412 solver.cpp:243]     Train net output #0: loss = 0.0153278 (* 1 = 0.0153278 loss)
I0428 21:06:31.333056  7412 sgd_solver.cpp:137] Iteration 47100, lr = 0.001
I0428 21:06:31.341056  7412 sgd_solver.cpp:169] scale layer:0.431412 0.450785 0.505009 0.545749 0.502552 0.473285 0.481796 0.477546 0.476080 0.477116 0.487927 0.470991 0.448035 0.421770 0.411163 0.377480 0.341236 0.304241 0.300725 1.069785 
I0428 21:06:31.344056  7412 sgd_solver.cpp:200] weight diff/data:0.000217 0.000289 0.000280 0.000382 0.000469 0.000288 0.000381 0.000326 0.000425 0.000447 0.000352 0.000384 0.000384 0.000416 0.000314 0.000850 0.000391 0.000364 0.000394 0.000654 0.000034 
I0428 21:06:54.741395 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:06:55.967465  7412 solver.cpp:224] Iteration 47200 (4.05935 iter/s, 24.6345s/100 iters), loss = 0.0417914
I0428 21:06:55.967465  7412 solver.cpp:243]     Train net output #0: loss = 0.0417913 (* 1 = 0.0417913 loss)
I0428 21:06:55.967465  7412 sgd_solver.cpp:137] Iteration 47200, lr = 0.001
I0428 21:06:55.975466  7412 sgd_solver.cpp:169] scale layer:0.431089 0.450386 0.504748 0.545397 0.502280 0.473043 0.481568 0.477305 0.475837 0.476847 0.487696 0.470728 0.447810 0.421579 0.411000 0.377295 0.340994 0.304050 0.300567 1.070277 
I0428 21:06:55.977465  7412 sgd_solver.cpp:200] weight diff/data:0.000178 0.000504 0.000319 0.001144 0.000950 0.000355 0.000535 0.000415 0.000588 0.000413 0.000586 0.000913 0.000670 0.000703 0.000650 0.000626 0.000562 0.001230 0.000419 0.000672 0.000045 
I0428 21:07:20.575872  7412 solver.cpp:224] Iteration 47300 (4.06349 iter/s, 24.6094s/100 iters), loss = 0.0161644
I0428 21:07:20.576872  7412 solver.cpp:243]     Train net output #0: loss = 0.0161642 (* 1 = 0.0161642 loss)
I0428 21:07:20.576872  7412 sgd_solver.cpp:137] Iteration 47300, lr = 0.001
I0428 21:07:20.584873  7412 sgd_solver.cpp:169] scale layer:0.431079 0.450241 0.504545 0.545264 0.501919 0.472792 0.481375 0.477064 0.475558 0.476623 0.487473 0.470526 0.447588 0.421346 0.410798 0.377082 0.340837 0.303915 0.300407 1.070862 
I0428 21:07:20.587873  7412 sgd_solver.cpp:200] weight diff/data:0.000177 0.000273 0.000325 0.000540 0.016024 0.172610 0.000538 0.000520 0.000463 0.000348 0.000377 0.000366 0.000827 0.000531 0.000396 0.000539 0.000448 0.000538 0.005642 0.000851 0.000050 
I0428 21:07:45.225282  7412 solver.cpp:224] Iteration 47400 (4.05692 iter/s, 24.6492s/100 iters), loss = 0.0113016
I0428 21:07:45.225282  7412 solver.cpp:243]     Train net output #0: loss = 0.0113015 (* 1 = 0.0113015 loss)
I0428 21:07:45.225282  7412 sgd_solver.cpp:137] Iteration 47400, lr = 0.001
I0428 21:07:45.233283  7412 sgd_solver.cpp:169] scale layer:0.430948 0.450148 0.504300 0.545033 0.501662 0.472589 0.481112 0.476813 0.475316 0.476353 0.487258 0.470389 0.447363 0.421139 0.410585 0.376888 0.340594 0.303710 0.300248 1.071390 
I0428 21:07:45.235283  7412 sgd_solver.cpp:200] weight diff/data:0.000137 0.000219 0.000423 0.001059 0.000795 0.000345 0.000528 0.000292 0.000462 0.000341 0.001034 0.000377 0.000377 0.000440 0.000425 0.000421 0.000579 0.000499 0.000360 0.001483 0.000046 
I0428 21:08:09.602676  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_47500.caffemodel
I0428 21:08:09.611677  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_47500.solverstate
I0428 21:08:09.614677  7412 solver.cpp:336] Iteration 47500, Testing net (#0)
I0428 21:08:18.380178 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:08:18.742199  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8314
I0428 21:08:18.742199  7412 solver.cpp:403]     Test net output #1: loss = 0.745126 (* 1 = 0.745126 loss)
I0428 21:08:18.976213  7412 solver.cpp:224] Iteration 47500 (2.96287 iter/s, 33.751s/100 iters), loss = 0.0126704
I0428 21:08:18.976213  7412 solver.cpp:243]     Train net output #0: loss = 0.0126703 (* 1 = 0.0126703 loss)
I0428 21:08:18.976213  7412 sgd_solver.cpp:137] Iteration 47500, lr = 0.001
I0428 21:08:18.984213  7412 sgd_solver.cpp:169] scale layer:0.430562 0.449791 0.504032 0.544701 0.501440 0.472322 0.480862 0.476573 0.475113 0.476094 0.487020 0.470161 0.447143 0.420929 0.410365 0.376709 0.340399 0.303538 0.300088 1.071886 
I0428 21:08:18.986213  7412 sgd_solver.cpp:200] weight diff/data:0.000148 0.000223 0.000213 0.000311 0.000333 0.000190 0.000279 0.000370 0.000302 0.000257 0.000251 0.000329 0.000335 0.000700 0.000313 0.000366 0.000256 0.000303 0.000289 0.000460 0.000030 
I0428 21:08:42.377552 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:08:43.597621  7412 solver.cpp:224] Iteration 47600 (4.06142 iter/s, 24.622s/100 iters), loss = 0.0142883
I0428 21:08:43.597621  7412 solver.cpp:243]     Train net output #0: loss = 0.0142882 (* 1 = 0.0142882 loss)
I0428 21:08:43.597621  7412 sgd_solver.cpp:137] Iteration 47600, lr = 0.001
I0428 21:08:43.605621  7412 sgd_solver.cpp:169] scale layer:0.430258 0.449490 0.503783 0.544415 0.501214 0.472069 0.480618 0.476332 0.474899 0.475859 0.486801 0.469902 0.446912 0.420736 0.410191 0.376515 0.340215 0.303362 0.299928 1.072357 
I0428 21:08:43.607621  7412 sgd_solver.cpp:200] weight diff/data:0.000153 0.000265 0.000232 0.000620 0.000535 0.000631 0.000554 0.000315 0.000387 0.000278 0.000351 0.000331 0.000955 0.000432 0.000281 0.000391 0.000423 0.000686 0.000706 0.000249 0.000037 
I0428 21:09:08.215029  7412 solver.cpp:224] Iteration 47700 (4.06206 iter/s, 24.618s/100 iters), loss = 0.011856
I0428 21:09:08.215029  7412 solver.cpp:243]     Train net output #0: loss = 0.0118559 (* 1 = 0.0118559 loss)
I0428 21:09:08.215029  7412 sgd_solver.cpp:137] Iteration 47700, lr = 0.001
I0428 21:09:08.224030  7412 sgd_solver.cpp:169] scale layer:0.430227 0.449378 0.503548 0.544191 0.500908 0.471860 0.480376 0.476081 0.474615 0.475636 0.486551 0.469665 0.446696 0.420503 0.409943 0.376299 0.340010 0.303191 0.299754 1.072906 
I0428 21:09:08.225029  7412 sgd_solver.cpp:200] weight diff/data:0.000188 0.000252 0.000267 0.000310 0.000319 0.000461 0.000547 0.000332 0.000622 0.000529 0.000366 0.000558 0.000469 0.000330 0.000593 0.000681 0.000513 0.000655 0.000483 0.000370 0.000065 
I0428 21:09:32.828438  7412 solver.cpp:224] Iteration 47800 (4.06271 iter/s, 24.6141s/100 iters), loss = 0.0143244
I0428 21:09:32.829437  7412 solver.cpp:243]     Train net output #0: loss = 0.0143243 (* 1 = 0.0143243 loss)
I0428 21:09:32.829437  7412 sgd_solver.cpp:137] Iteration 47800, lr = 0.001
I0428 21:09:32.838438  7412 sgd_solver.cpp:169] scale layer:0.430032 0.449182 0.503366 0.543961 0.500540 0.471623 0.480129 0.475850 0.474359 0.475406 0.486277 0.469430 0.446433 0.420285 0.409763 0.376076 0.339820 0.302983 0.299611 1.073418 
I0428 21:09:32.840437  7412 sgd_solver.cpp:200] weight diff/data:0.000187 0.000289 0.000231 0.000352 0.001385 0.000465 0.000311 0.000321 0.000462 0.000419 0.000356 0.000346 0.000396 0.000390 0.000536 0.000501 0.000483 0.000377 0.000725 0.000335 0.000134 
I0428 21:09:57.436844  7412 solver.cpp:224] Iteration 47900 (4.0637 iter/s, 24.6081s/100 iters), loss = 0.0148709
I0428 21:09:57.436844  7412 solver.cpp:243]     Train net output #0: loss = 0.0148708 (* 1 = 0.0148708 loss)
I0428 21:09:57.436844  7412 sgd_solver.cpp:137] Iteration 47900, lr = 0.001
I0428 21:09:57.444845  7412 sgd_solver.cpp:169] scale layer:0.429792 0.449003 0.503096 0.543613 0.500427 0.471366 0.479900 0.475627 0.474092 0.475186 0.486038 0.469173 0.446235 0.420080 0.409536 0.375862 0.339566 0.302757 0.299437 1.073867 
I0428 21:09:57.446846  7412 sgd_solver.cpp:200] weight diff/data:0.000180 0.000410 0.000264 0.000438 0.000596 0.000341 0.000377 0.000441 0.000640 0.001438 0.000423 0.000390 0.000410 0.000325 0.000705 0.000472 0.000520 0.001087 0.000432 0.000313 0.000110 
I0428 21:10:20.828182 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:10:21.807238  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_48000.caffemodel
I0428 21:10:21.816239  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_48000.solverstate
I0428 21:10:21.819239  7412 solver.cpp:336] Iteration 48000, Testing net (#0)
I0428 21:10:30.565739 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:10:30.928761  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8313
I0428 21:10:30.928761  7412 solver.cpp:403]     Test net output #1: loss = 0.753941 (* 1 = 0.753941 loss)
I0428 21:10:31.163774  7412 solver.cpp:224] Iteration 48000 (2.9649 iter/s, 33.7279s/100 iters), loss = 0.0141717
I0428 21:10:31.163774  7412 solver.cpp:243]     Train net output #0: loss = 0.0141716 (* 1 = 0.0141716 loss)
I0428 21:10:31.163774  7412 sgd_solver.cpp:137] Iteration 48000, lr = 0.001
I0428 21:10:31.172775  7412 sgd_solver.cpp:169] scale layer:0.429352 0.448540 0.502874 0.543311 0.500268 0.471140 0.479666 0.475372 0.473858 0.474925 0.485814 0.468932 0.446005 0.419887 0.409361 0.375652 0.339346 0.302566 0.299286 1.074280 
I0428 21:10:31.173774  7412 sgd_solver.cpp:200] weight diff/data:0.000173 0.000601 0.000211 0.000339 0.000365 0.000251 0.000246 0.000288 0.003304 0.000276 0.000342 0.000233 0.000340 0.000231 0.000319 0.000370 0.000276 0.000319 0.000389 0.000249 0.000025 
I0428 21:10:55.780181  7412 solver.cpp:224] Iteration 48100 (4.06221 iter/s, 24.6171s/100 iters), loss = 0.0082573
I0428 21:10:55.780181  7412 solver.cpp:243]     Train net output #0: loss = 0.0082572 (* 1 = 0.0082572 loss)
I0428 21:10:55.780181  7412 sgd_solver.cpp:137] Iteration 48100, lr = 0.001
I0428 21:10:55.789182  7412 sgd_solver.cpp:169] scale layer:0.429425 0.448502 0.502569 0.543108 0.499935 0.470917 0.479415 0.475115 0.473540 0.474690 0.485564 0.468710 0.445799 0.419654 0.409150 0.375458 0.339134 0.302410 0.299122 1.074801 
I0428 21:10:55.791182  7412 sgd_solver.cpp:200] weight diff/data:0.000139 0.000301 0.000213 0.000264 0.000460 0.000254 0.000294 0.000334 0.000417 0.000295 0.000420 0.000390 0.000485 0.000319 0.000305 0.000412 0.000273 0.000345 0.000346 0.000363 0.000060 
I0428 21:11:20.403590  7412 solver.cpp:224] Iteration 48200 (4.06118 iter/s, 24.6234s/100 iters), loss = 0.0164269
I0428 21:11:20.403590  7412 solver.cpp:243]     Train net output #0: loss = 0.0164268 (* 1 = 0.0164268 loss)
I0428 21:11:20.403590  7412 sgd_solver.cpp:137] Iteration 48200, lr = 0.001
I0428 21:11:20.412590  7412 sgd_solver.cpp:169] scale layer:0.429224 0.448358 0.502382 0.542869 0.499601 0.470701 0.479159 0.474861 0.473285 0.474438 0.485312 0.468508 0.445568 0.419430 0.408952 0.375250 0.338924 0.302209 0.298965 1.075286 
I0428 21:11:20.414590  7412 sgd_solver.cpp:200] weight diff/data:0.000215 0.000572 0.000297 0.001255 0.000470 0.000410 0.000377 0.000290 0.000420 0.000470 0.000398 0.001009 0.000555 0.000357 0.000443 0.000529 0.000385 0.000561 0.000438 0.000474 0.000141 
I0428 21:11:44.994997  7412 solver.cpp:224] Iteration 48300 (4.06632 iter/s, 24.5923s/100 iters), loss = 0.0100475
I0428 21:11:44.994997  7412 solver.cpp:243]     Train net output #0: loss = 0.0100474 (* 1 = 0.0100474 loss)
I0428 21:11:44.994997  7412 sgd_solver.cpp:137] Iteration 48300, lr = 0.001
I0428 21:11:45.002997  7412 sgd_solver.cpp:169] scale layer:0.428930 0.448084 0.502057 0.542607 0.499475 0.470454 0.478903 0.474653 0.473066 0.474197 0.485047 0.468271 0.445342 0.419221 0.408754 0.375074 0.338698 0.302026 0.298795 1.075722 
I0428 21:11:45.005997  7412 sgd_solver.cpp:200] weight diff/data:0.000149 0.000280 0.000195 0.000251 0.000397 0.000221 0.001127 0.000260 0.000684 0.000364 0.000334 0.000433 0.000448 0.000321 0.000233 0.001030 0.000231 0.000243 0.000887 0.000212 0.000066 
I0428 21:12:08.393334 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:12:09.617404  7412 solver.cpp:224] Iteration 48400 (4.06131 iter/s, 24.6226s/100 iters), loss = 0.0118011
I0428 21:12:09.617404  7412 solver.cpp:243]     Train net output #0: loss = 0.011801 (* 1 = 0.011801 loss)
I0428 21:12:09.617404  7412 sgd_solver.cpp:137] Iteration 48400, lr = 0.001
I0428 21:12:09.626405  7412 sgd_solver.cpp:169] scale layer:0.428570 0.447722 0.501750 0.542261 0.499223 0.470220 0.478660 0.474416 0.472893 0.473961 0.484832 0.468029 0.445110 0.419026 0.408566 0.374891 0.338483 0.301835 0.298648 1.076107 
I0428 21:12:09.628406  7412 sgd_solver.cpp:200] weight diff/data:0.000228 0.000395 0.000315 0.000388 0.000774 0.000437 0.000472 0.000284 0.566085 0.000313 0.000520 0.000298 0.000318 0.000276 0.000354 0.000750 0.000310 0.000338 0.000355 0.000460 0.000053 
I0428 21:12:34.000799  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_48500.caffemodel
I0428 21:12:34.008800  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_48500.solverstate
I0428 21:12:34.011801  7412 solver.cpp:336] Iteration 48500, Testing net (#0)
I0428 21:12:42.775301 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:12:43.136322  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8306
I0428 21:12:43.136322  7412 solver.cpp:403]     Test net output #1: loss = 0.759288 (* 1 = 0.759288 loss)
I0428 21:12:43.370335  7412 solver.cpp:224] Iteration 48500 (2.96257 iter/s, 33.7545s/100 iters), loss = 0.00981607
I0428 21:12:43.370335  7412 solver.cpp:243]     Train net output #0: loss = 0.00981597 (* 1 = 0.00981597 loss)
I0428 21:12:43.370335  7412 sgd_solver.cpp:137] Iteration 48500, lr = 0.001
I0428 21:12:43.379336  7412 sgd_solver.cpp:169] scale layer:0.428431 0.447634 0.501479 0.542067 0.498986 0.470021 0.478427 0.474166 0.472605 0.473731 0.484605 0.467778 0.444863 0.418811 0.408336 0.374653 0.338261 0.301665 0.298486 1.076587 
I0428 21:12:43.380336  7412 sgd_solver.cpp:200] weight diff/data:0.000137 0.000288 0.000227 0.000407 0.001862 0.000322 0.000290 0.000507 0.000357 0.000261 0.000511 0.000672 0.000356 0.000295 0.000347 0.000364 0.000293 0.000399 0.000542 0.000361 0.000140 
I0428 21:13:07.951741  7412 solver.cpp:224] Iteration 48600 (4.06808 iter/s, 24.5816s/100 iters), loss = 0.0144125
I0428 21:13:07.951741  7412 solver.cpp:243]     Train net output #0: loss = 0.0144124 (* 1 = 0.0144124 loss)
I0428 21:13:07.951741  7412 sgd_solver.cpp:137] Iteration 48600, lr = 0.001
I0428 21:13:07.959741  7412 sgd_solver.cpp:169] scale layer:0.428107 0.447466 0.501262 0.541831 0.498745 0.469794 0.478147 0.473939 0.472354 0.473492 0.484380 0.467569 0.444608 0.418595 0.408126 0.374454 0.338013 0.301489 0.298328 1.076998 
I0428 21:13:07.961742  7412 sgd_solver.cpp:200] weight diff/data:0.000142 0.000470 0.000251 0.000363 0.000444 0.000442 0.000326 0.000393 0.000333 0.000431 0.000293 0.000398 0.000372 0.000352 0.000702 0.000685 0.000339 0.000528 0.000674 0.000478 0.000217 
I0428 21:13:32.537147  7412 solver.cpp:224] Iteration 48700 (4.06736 iter/s, 24.586s/100 iters), loss = 0.00966441
I0428 21:13:32.537147  7412 solver.cpp:243]     Train net output #0: loss = 0.00966432 (* 1 = 0.00966432 loss)
I0428 21:13:32.537147  7412 sgd_solver.cpp:137] Iteration 48700, lr = 0.001
I0428 21:13:32.545148  7412 sgd_solver.cpp:169] scale layer:0.427846 0.447205 0.501047 0.541523 0.498558 0.469551 0.477898 0.473719 0.472097 0.473270 0.484140 0.467343 0.444390 0.418379 0.407906 0.374261 0.337777 0.301271 0.298155 1.077383 
I0428 21:13:32.547148  7412 sgd_solver.cpp:200] weight diff/data:0.000116 0.000154 0.000178 0.000230 0.000267 0.000978 0.000189 0.000222 0.000214 0.000196 0.000428 0.000786 0.000266 0.000358 0.000219 0.000239 0.000235 0.000498 0.000232 0.000704 0.000070 
I0428 21:13:55.973489 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:13:57.194558  7412 solver.cpp:224] Iteration 48800 (4.0555 iter/s, 24.6579s/100 iters), loss = 0.00975285
I0428 21:13:57.194558  7412 solver.cpp:243]     Train net output #0: loss = 0.00975277 (* 1 = 0.00975277 loss)
I0428 21:13:57.194558  7412 sgd_solver.cpp:137] Iteration 48800, lr = 0.001
I0428 21:13:57.203558  7412 sgd_solver.cpp:169] scale layer:0.427695 0.446869 0.500795 0.541175 0.498315 0.469334 0.477653 0.473496 0.471868 0.473043 0.483895 0.467065 0.444177 0.418172 0.407713 0.374066 0.337566 0.301073 0.297995 1.077785 
I0428 21:13:57.205559  7412 sgd_solver.cpp:200] weight diff/data:0.000157 0.000519 0.000206 0.000340 0.000355 0.000924 0.000222 0.000379 0.000480 0.000342 0.000247 0.000440 0.000542 0.000401 0.000296 0.000313 0.000720 0.000572 0.000276 0.000248 0.000021 
I0428 21:14:21.804965  7412 solver.cpp:224] Iteration 48900 (4.06316 iter/s, 24.6114s/100 iters), loss = 0.00956144
I0428 21:14:21.805965  7412 solver.cpp:243]     Train net output #0: loss = 0.00956136 (* 1 = 0.00956136 loss)
I0428 21:14:21.805965  7412 sgd_solver.cpp:137] Iteration 48900, lr = 0.001
I0428 21:14:21.813966  7412 sgd_solver.cpp:169] scale layer:0.427622 0.446793 0.500561 0.540955 0.498020 0.469103 0.477428 0.473266 0.471591 0.472833 0.483624 0.466838 0.443953 0.417925 0.407493 0.373855 0.337368 0.300898 0.297834 1.078227 
I0428 21:14:21.815966  7412 sgd_solver.cpp:200] weight diff/data:0.000121 0.000207 0.000250 0.000295 0.000283 0.000488 0.000350 0.000286 0.000285 0.000299 0.000273 0.000289 0.000444 0.000301 0.000332 0.000606 0.000395 0.000576 0.000538 0.000370 0.000070 
I0428 21:14:46.177359  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_49000.caffemodel
I0428 21:14:46.186360  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_49000.solverstate
I0428 21:14:46.189360  7412 solver.cpp:336] Iteration 49000, Testing net (#0)
I0428 21:14:54.964862 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:14:55.330883  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8305
I0428 21:14:55.330883  7412 solver.cpp:403]     Test net output #1: loss = 0.76858 (* 1 = 0.76858 loss)
I0428 21:14:55.567896  7412 solver.cpp:224] Iteration 49000 (2.96179 iter/s, 33.7634s/100 iters), loss = 0.0104094
I0428 21:14:55.567896  7412 solver.cpp:243]     Train net output #0: loss = 0.0104093 (* 1 = 0.0104093 loss)
I0428 21:14:55.567896  7412 sgd_solver.cpp:137] Iteration 49000, lr = 0.001
I0428 21:14:55.576897  7412 sgd_solver.cpp:169] scale layer:0.427355 0.446491 0.500347 0.540715 0.497719 0.468855 0.477196 0.473027 0.471339 0.472585 0.483343 0.466632 0.443696 0.417688 0.407299 0.373654 0.337160 0.300690 0.297682 1.078657 
I0428 21:14:55.577898  7412 sgd_solver.cpp:200] weight diff/data:0.000178 0.000451 0.000356 0.000345 0.000407 0.000334 0.000821 0.000334 0.000485 0.000419 0.000453 0.000413 0.000342 0.000373 0.000463 0.000486 0.000540 0.000882 0.003962 0.000374 0.000057 
I0428 21:15:20.167304  7412 solver.cpp:224] Iteration 49100 (4.06515 iter/s, 24.5993s/100 iters), loss = 0.0076166
I0428 21:15:20.167304  7412 solver.cpp:243]     Train net output #0: loss = 0.00761651 (* 1 = 0.00761651 loss)
I0428 21:15:20.167304  7412 sgd_solver.cpp:137] Iteration 49100, lr = 0.001
I0428 21:15:20.176304  7412 sgd_solver.cpp:169] scale layer:0.426893 0.446213 0.500091 0.540399 0.497635 0.468613 0.476961 0.472817 0.471119 0.472351 0.483123 0.466389 0.443473 0.417456 0.407097 0.373459 0.336948 0.300480 0.297512 1.079020 
I0428 21:15:20.178304  7412 sgd_solver.cpp:200] weight diff/data:0.000164 0.000197 0.000230 0.000425 0.000278 0.000358 0.000267 0.000374 0.000275 0.000328 0.000322 0.000318 0.000595 0.000308 0.000270 0.000297 0.000305 0.000547 0.000642 0.000238 0.000046 
I0428 21:15:43.588644 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:15:44.803712  7412 solver.cpp:224] Iteration 49200 (4.05888 iter/s, 24.6374s/100 iters), loss = 0.013081
I0428 21:15:44.803712  7412 solver.cpp:243]     Train net output #0: loss = 0.0130809 (* 1 = 0.0130809 loss)
I0428 21:15:44.804713  7412 sgd_solver.cpp:137] Iteration 49200, lr = 0.001
I0428 21:15:44.813714  7412 sgd_solver.cpp:169] scale layer:0.426591 0.445887 0.499873 0.540126 0.497325 0.468364 0.476749 0.472577 0.470902 0.472110 0.482876 0.466131 0.443264 0.417271 0.406903 0.373276 0.336741 0.300310 0.297350 1.079381 
I0428 21:15:44.815713  7412 sgd_solver.cpp:200] weight diff/data:0.000160 0.000190 0.000200 0.000280 0.000356 0.000297 0.000303 0.000805 0.000329 0.000491 0.000452 0.000265 0.000326 0.000801 0.000268 0.000362 0.000256 0.000419 0.000363 0.000405 0.000022 
I0428 21:16:09.425122  7412 solver.cpp:224] Iteration 49300 (4.06154 iter/s, 24.6212s/100 iters), loss = 0.00668578
I0428 21:16:09.425122  7412 solver.cpp:243]     Train net output #0: loss = 0.00668569 (* 1 = 0.00668569 loss)
I0428 21:16:09.425122  7412 sgd_solver.cpp:137] Iteration 49300, lr = 0.001
I0428 21:16:09.434121  7412 sgd_solver.cpp:169] scale layer:0.426632 0.445778 0.499673 0.539909 0.497073 0.468144 0.476520 0.472303 0.470650 0.471876 0.482618 0.465903 0.443038 0.417067 0.406697 0.373064 0.336541 0.300137 0.297194 1.079793 
I0428 21:16:09.436121  7412 sgd_solver.cpp:200] weight diff/data:0.000137 0.000188 0.001021 0.001176 0.000564 0.000210 0.000237 0.000260 0.000294 0.000285 0.000254 0.000304 0.000321 0.000319 0.001036 0.000869 0.000451 0.000704 0.000410 0.000319 0.000049 
I0428 21:16:34.034528  7412 solver.cpp:224] Iteration 49400 (4.06343 iter/s, 24.6098s/100 iters), loss = 0.014754
I0428 21:16:34.034528  7412 solver.cpp:243]     Train net output #0: loss = 0.0147539 (* 1 = 0.0147539 loss)
I0428 21:16:34.034528  7412 sgd_solver.cpp:137] Iteration 49400, lr = 0.001
I0428 21:16:34.043529  7412 sgd_solver.cpp:169] scale layer:0.426371 0.445577 0.499394 0.539639 0.496827 0.467922 0.476297 0.472030 0.470412 0.471623 0.482364 0.465696 0.442807 0.416864 0.406478 0.372861 0.336325 0.299945 0.297041 1.080156 
I0428 21:16:34.046530  7412 sgd_solver.cpp:200] weight diff/data:0.000159 0.000255 0.000217 0.000417 0.000466 0.000880 0.000393 0.000369 0.000246 0.000382 0.000265 0.000306 0.000254 0.000287 0.000263 0.000735 0.000330 0.000301 0.000265 0.000343 0.000132 
I0428 21:16:58.420923  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_49500.caffemodel
I0428 21:16:58.428925  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_49500.solverstate
I0428 21:16:58.431924  7412 solver.cpp:336] Iteration 49500, Testing net (#0)
I0428 21:17:07.197425 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:17:07.561446  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8309
I0428 21:17:07.561446  7412 solver.cpp:403]     Test net output #1: loss = 0.776404 (* 1 = 0.776404 loss)
I0428 21:17:07.795459  7412 solver.cpp:224] Iteration 49500 (2.96196 iter/s, 33.7614s/100 iters), loss = 0.0077551
I0428 21:17:07.795459  7412 solver.cpp:243]     Train net output #0: loss = 0.00775502 (* 1 = 0.00775502 loss)
I0428 21:17:07.795459  7412 sgd_solver.cpp:137] Iteration 49500, lr = 0.001
I0428 21:17:07.804461  7412 sgd_solver.cpp:169] scale layer:0.426105 0.445373 0.499084 0.539310 0.496653 0.467682 0.476058 0.471812 0.470181 0.471376 0.482129 0.465454 0.442587 0.416659 0.406277 0.372663 0.336109 0.299753 0.296864 1.080493 
I0428 21:17:07.806460  7412 sgd_solver.cpp:200] weight diff/data:0.000098 0.000152 0.000178 0.000227 0.001075 0.000193 0.000262 0.000337 0.000239 0.000225 0.000283 0.000268 0.000345 0.024909 0.000208 0.000640 0.000205 0.000237 0.000315 0.000148 0.000042 
I0428 21:17:31.176797 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:17:32.394866  7412 solver.cpp:224] Iteration 49600 (4.06512 iter/s, 24.5995s/100 iters), loss = 0.0319703
I0428 21:17:32.394866  7412 solver.cpp:243]     Train net output #0: loss = 0.0319702 (* 1 = 0.0319702 loss)
I0428 21:17:32.394866  7412 sgd_solver.cpp:137] Iteration 49600, lr = 0.001
I0428 21:17:32.403867  7412 sgd_solver.cpp:169] scale layer:0.425848 0.445157 0.498808 0.539002 0.496424 0.467440 0.475818 0.471578 0.469960 0.471144 0.481883 0.465203 0.442364 0.416459 0.406091 0.372466 0.335903 0.299564 0.296716 1.080804 
I0428 21:17:32.405867  7412 sgd_solver.cpp:200] weight diff/data:0.000148 0.000275 0.000257 0.000496 0.000406 0.000310 0.000305 0.000435 0.010050 0.000474 0.000715 0.004702 0.000371 0.000415 0.000413 0.000498 0.000309 0.000448 0.002959 0.001017 0.000027 
I0428 21:17:56.999274  7412 solver.cpp:224] Iteration 49700 (4.06428 iter/s, 24.6046s/100 iters), loss = 0.00733408
I0428 21:17:56.999274  7412 solver.cpp:243]     Train net output #0: loss = 0.00733399 (* 1 = 0.00733399 loss)
I0428 21:17:56.999274  7412 sgd_solver.cpp:137] Iteration 49700, lr = 0.001
I0428 21:17:57.008275  7412 sgd_solver.cpp:169] scale layer:0.425731 0.444964 0.498610 0.538772 0.496168 0.467199 0.475583 0.471329 0.469664 0.470919 0.481637 0.464983 0.442099 0.416229 0.405876 0.372243 0.335706 0.299409 0.296564 1.081167 
I0428 21:17:57.010274  7412 sgd_solver.cpp:200] weight diff/data:0.000148 0.000210 0.000175 0.000378 0.000266 0.000169 0.000236 0.000340 0.000568 0.000297 0.000307 0.000230 0.000360 0.000235 0.000224 0.000306 0.000229 0.000701 0.000453 0.000363 0.000032 
I0428 21:18:21.590680  7412 solver.cpp:224] Iteration 49800 (4.06643 iter/s, 24.5916s/100 iters), loss = 0.0110778
I0428 21:18:21.590680  7412 solver.cpp:243]     Train net output #0: loss = 0.0110777 (* 1 = 0.0110777 loss)
I0428 21:18:21.590680  7412 sgd_solver.cpp:137] Iteration 49800, lr = 0.001
I0428 21:18:21.598681  7412 sgd_solver.cpp:169] scale layer:0.425440 0.444772 0.498364 0.538543 0.495943 0.466971 0.475356 0.471088 0.469440 0.470657 0.481371 0.464769 0.441860 0.416010 0.405687 0.372047 0.335459 0.299224 0.296414 1.081513 
I0428 21:18:21.600682  7412 sgd_solver.cpp:200] weight diff/data:0.000104 0.000178 0.000531 0.000379 0.000203 0.000232 0.000323 0.000226 0.000322 0.000224 0.000269 0.000599 0.000386 0.001073 0.000309 0.000345 0.000230 0.001457 0.000272 0.000269 0.000085 
I0428 21:18:46.199088  7412 solver.cpp:224] Iteration 49900 (4.06342 iter/s, 24.6098s/100 iters), loss = 0.00887009
I0428 21:18:46.200088  7412 solver.cpp:243]     Train net output #0: loss = 0.00886999 (* 1 = 0.00886999 loss)
I0428 21:18:46.200088  7412 sgd_solver.cpp:137] Iteration 49900, lr = 0.001
I0428 21:18:46.209089  7412 sgd_solver.cpp:169] scale layer:0.425121 0.444454 0.498095 0.538260 0.495761 0.466699 0.475098 0.470868 0.469205 0.470415 0.481134 0.464554 0.441630 0.415804 0.405499 0.371852 0.335236 0.299031 0.296243 1.081815 
I0428 21:18:46.211088  7412 sgd_solver.cpp:200] weight diff/data:0.000109 0.000319 0.000187 0.000222 0.000255 0.000279 0.000265 0.000371 0.000247 0.000294 0.000200 0.000246 0.000266 0.000385 0.000237 0.000320 0.000224 0.000280 0.000218 0.000182 0.000049 
I0428 21:19:09.641428 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:19:10.621485  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_50000.caffemodel
I0428 21:19:10.630486  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_50000.solverstate
I0428 21:19:10.633486  7412 solver.cpp:336] Iteration 50000, Testing net (#0)
I0428 21:19:19.419988 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:19:19.783010  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8298
I0428 21:19:19.783010  7412 solver.cpp:403]     Test net output #1: loss = 0.782791 (* 1 = 0.782791 loss)
I0428 21:19:20.016022  7412 solver.cpp:224] Iteration 50000 (2.95709 iter/s, 33.817s/100 iters), loss = 0.00907802
I0428 21:19:20.016022  7412 solver.cpp:243]     Train net output #0: loss = 0.00907792 (* 1 = 0.00907792 loss)
I0428 21:19:20.016022  7412 sgd_solver.cpp:137] Iteration 50000, lr = 0.001
I0428 21:19:20.025022  7412 sgd_solver.cpp:169] scale layer:0.424732 0.444155 0.497797 0.537959 0.495530 0.466477 0.474844 0.470659 0.468977 0.470179 0.480883 0.464318 0.441415 0.415599 0.405310 0.371672 0.335039 0.298844 0.296087 1.082108 
I0428 21:19:20.026022  7412 sgd_solver.cpp:200] weight diff/data:0.000069 0.000142 0.000549 0.000203 0.000214 0.000217 0.000200 0.000164 0.000189 0.000278 0.000222 0.000221 0.001338 0.000155 0.000694 0.000272 0.000210 0.000257 0.000262 0.000165 0.000015 
I0428 21:19:44.611429  7412 solver.cpp:224] Iteration 50100 (4.0656 iter/s, 24.5966s/100 iters), loss = 0.00888797
I0428 21:19:44.612429  7412 solver.cpp:243]     Train net output #0: loss = 0.00888787 (* 1 = 0.00888787 loss)
I0428 21:19:44.612429  7412 sgd_solver.cpp:137] Iteration 50100, lr = 0.001
I0428 21:19:44.621429  7412 sgd_solver.cpp:169] scale layer:0.424757 0.444118 0.497568 0.537751 0.495262 0.466258 0.474624 0.470410 0.468732 0.469955 0.480630 0.464076 0.441203 0.415378 0.405099 0.371465 0.334852 0.298720 0.295928 1.082453 
I0428 21:19:44.623430  7412 sgd_solver.cpp:200] weight diff/data:0.000104 0.000217 0.000484 0.000237 0.000907 0.000263 0.000368 0.000632 0.000264 0.000309 0.000325 0.000331 0.000290 0.000306 0.000520 0.000348 0.000424 0.000343 0.000283 0.000275 0.000046 
I0428 21:20:09.234838  7412 solver.cpp:224] Iteration 50200 (4.06111 iter/s, 24.6238s/100 iters), loss = 0.0103221
I0428 21:20:09.234838  7412 solver.cpp:243]     Train net output #0: loss = 0.010322 (* 1 = 0.010322 loss)
I0428 21:20:09.235837  7412 sgd_solver.cpp:137] Iteration 50200, lr = 0.001
I0428 21:20:09.243839  7412 sgd_solver.cpp:169] scale layer:0.424602 0.443909 0.497337 0.537469 0.494944 0.466009 0.474404 0.470149 0.468472 0.469696 0.480392 0.463871 0.440971 0.415148 0.404890 0.371269 0.334630 0.298537 0.295766 1.082782 
I0428 21:20:09.245838  7412 sgd_solver.cpp:200] weight diff/data:0.000104 0.000157 0.000196 0.000263 0.000275 0.000183 0.000295 0.000389 0.000272 0.000276 0.000326 0.000252 0.000345 0.000513 0.000294 0.000315 0.000240 0.000389 0.000348 0.000241 0.000036 
I0428 21:20:33.848245  7412 solver.cpp:224] Iteration 50300 (4.06275 iter/s, 24.6139s/100 iters), loss = 0.0102377
I0428 21:20:33.849246  7412 solver.cpp:243]     Train net output #0: loss = 0.0102376 (* 1 = 0.0102376 loss)
I0428 21:20:33.849246  7412 sgd_solver.cpp:137] Iteration 50300, lr = 0.001
I0428 21:20:33.858245  7412 sgd_solver.cpp:169] scale layer:0.424323 0.443676 0.497121 0.537197 0.494666 0.465760 0.474169 0.469928 0.468245 0.469465 0.480156 0.463649 0.440741 0.414941 0.404705 0.371081 0.334397 0.298358 0.295593 1.083046 
I0428 21:20:33.860246  7412 sgd_solver.cpp:200] weight diff/data:0.000084 0.000201 0.000199 0.000453 0.000267 0.000196 0.000244 0.000236 0.000270 0.000291 0.000534 0.000195 0.000615 0.000271 0.000523 0.000257 0.000192 0.000246 0.000338 0.000423 0.000020 
I0428 21:20:57.254585 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:20:58.465653  7412 solver.cpp:224] Iteration 50400 (4.06208 iter/s, 24.6179s/100 iters), loss = 0.0194594
I0428 21:20:58.466653  7412 solver.cpp:243]     Train net output #0: loss = 0.0194593 (* 1 = 0.0194593 loss)
I0428 21:20:58.466653  7412 sgd_solver.cpp:137] Iteration 50400, lr = 0.001
I0428 21:20:58.474653  7412 sgd_solver.cpp:169] scale layer:0.423986 0.443267 0.496875 0.536905 0.494462 0.465514 0.473903 0.469671 0.468045 0.469208 0.479900 0.463378 0.440518 0.414767 0.404510 0.370902 0.334207 0.298167 0.295449 1.083314 
I0428 21:20:58.476655  7412 sgd_solver.cpp:200] weight diff/data:0.000087 0.000186 0.000226 0.000340 0.000235 0.000219 0.000290 0.001986 0.000422 0.000518 0.000968 0.000335 0.000481 0.000359 0.000435 0.000356 0.000290 0.000917 0.000464 0.000298 0.000024 
I0428 21:21:22.809046  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_50500.caffemodel
I0428 21:21:22.819046  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_50500.solverstate
I0428 21:21:22.822046  7412 solver.cpp:336] Iteration 50500, Testing net (#0)
I0428 21:21:31.590548 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:21:31.950568  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8307
I0428 21:21:31.950568  7412 solver.cpp:403]     Test net output #1: loss = 0.788928 (* 1 = 0.788928 loss)
I0428 21:21:32.184581  7412 solver.cpp:224] Iteration 50500 (2.96564 iter/s, 33.7195s/100 iters), loss = 0.00990206
I0428 21:21:32.184581  7412 solver.cpp:243]     Train net output #0: loss = 0.00990196 (* 1 = 0.00990196 loss)
I0428 21:21:32.184581  7412 sgd_solver.cpp:137] Iteration 50500, lr = 0.001
I0428 21:21:32.192582  7412 sgd_solver.cpp:169] scale layer:0.423930 0.443221 0.496633 0.536624 0.494129 0.465305 0.473657 0.469416 0.467778 0.468988 0.479639 0.463138 0.440300 0.414550 0.404289 0.370687 0.334019 0.297975 0.295289 1.083606 
I0428 21:21:32.194582  7412 sgd_solver.cpp:200] weight diff/data:0.000112 0.000257 0.000342 0.000398 0.000603 0.000258 0.000679 0.000356 0.000824 0.000531 0.000717 0.000276 0.000900 0.000339 0.000428 0.000453 0.000278 0.000326 0.000518 0.000442 0.000028 
I0428 21:21:56.780988  7412 solver.cpp:224] Iteration 50600 (4.06554 iter/s, 24.597s/100 iters), loss = 0.00812252
I0428 21:21:56.780988  7412 solver.cpp:243]     Train net output #0: loss = 0.00812241 (* 1 = 0.00812241 loss)
I0428 21:21:56.780988  7412 sgd_solver.cpp:137] Iteration 50600, lr = 0.001
I0428 21:21:56.789989  7412 sgd_solver.cpp:169] scale layer:0.423792 0.443041 0.496413 0.536405 0.493805 0.465061 0.473405 0.469155 0.467517 0.468755 0.479387 0.462903 0.440076 0.414330 0.404071 0.370476 0.333805 0.297781 0.295133 1.083913 
I0428 21:21:56.791990  7412 sgd_solver.cpp:200] weight diff/data:0.000120 0.000299 0.000244 0.000254 0.000242 0.000195 0.000307 0.000261 0.000461 0.000390 0.000949 0.000246 0.000321 0.000413 0.000283 0.001041 0.000313 0.000403 0.000448 0.000288 0.000032 
I0428 21:22:21.395396  7412 solver.cpp:224] Iteration 50700 (4.06257 iter/s, 24.615s/100 iters), loss = 0.00601505
I0428 21:22:21.395396  7412 solver.cpp:243]     Train net output #0: loss = 0.00601494 (* 1 = 0.00601494 loss)
I0428 21:22:21.395396  7412 sgd_solver.cpp:137] Iteration 50700, lr = 0.001
I0428 21:22:21.404397  7412 sgd_solver.cpp:169] scale layer:0.423524 0.442845 0.496184 0.536149 0.493579 0.464841 0.473194 0.468938 0.467305 0.468536 0.479156 0.462665 0.439849 0.414128 0.403873 0.370300 0.333589 0.297592 0.294957 1.084170 
I0428 21:22:21.406397  7412 sgd_solver.cpp:200] weight diff/data:0.000087 0.000200 0.000234 0.000274 0.000262 0.000197 0.000306 0.000343 0.000951 0.000840 0.000274 0.000206 0.000281 0.000988 0.000233 0.000219 0.000203 0.000210 0.000201 0.000579 0.000020 
I0428 21:22:44.782734 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:22:45.995803  7412 solver.cpp:224] Iteration 50800 (4.06494 iter/s, 24.6006s/100 iters), loss = 0.00684567
I0428 21:22:45.995803  7412 solver.cpp:243]     Train net output #0: loss = 0.00684555 (* 1 = 0.00684555 loss)
I0428 21:22:45.995803  7412 sgd_solver.cpp:137] Iteration 50800, lr = 0.001
I0428 21:22:46.004804  7412 sgd_solver.cpp:169] scale layer:0.423258 0.442435 0.495974 0.535860 0.493388 0.464579 0.472931 0.468705 0.467090 0.468297 0.478929 0.462433 0.439627 0.413927 0.403678 0.370114 0.333417 0.297406 0.294802 1.084409 
I0428 21:22:46.006804  7412 sgd_solver.cpp:200] weight diff/data:0.000092 0.000120 0.000145 0.000400 0.000206 0.000170 0.001344 0.000202 0.000359 0.000541 0.000195 0.000997 0.000227 0.000182 0.000311 0.000268 0.000335 0.000611 0.000391 0.024291 0.000017 
I0428 21:23:10.669214  7412 solver.cpp:224] Iteration 50900 (4.05291 iter/s, 24.6736s/100 iters), loss = 0.00617681
I0428 21:23:10.669214  7412 solver.cpp:243]     Train net output #0: loss = 0.0061767 (* 1 = 0.0061767 loss)
I0428 21:23:10.669214  7412 sgd_solver.cpp:137] Iteration 50900, lr = 0.001
I0428 21:23:10.677215  7412 sgd_solver.cpp:169] scale layer:0.423075 0.442313 0.495694 0.535595 0.493148 0.464373 0.472723 0.468459 0.466823 0.468075 0.478690 0.462225 0.439412 0.413699 0.403465 0.369902 0.333244 0.297257 0.294655 1.084702 
I0428 21:23:10.679215  7412 sgd_solver.cpp:200] weight diff/data:0.000116 0.000177 0.000188 0.000226 0.000257 0.000185 0.000562 0.000477 0.000256 0.000434 0.000239 0.000341 0.000346 0.000403 0.000261 0.000329 0.000259 0.000414 0.000287 0.000832 0.000023 
I0428 21:23:35.030608  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_51000.caffemodel
I0428 21:23:35.038609  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_51000.solverstate
I0428 21:23:35.041610  7412 solver.cpp:336] Iteration 51000, Testing net (#0)
I0428 21:23:43.805110 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:23:44.169131  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8304
I0428 21:23:44.169131  7412 solver.cpp:403]     Test net output #1: loss = 0.794508 (* 1 = 0.794508 loss)
I0428 21:23:44.402144  7412 solver.cpp:224] Iteration 51000 (2.96431 iter/s, 33.7346s/100 iters), loss = 0.00836107
I0428 21:23:44.402144  7412 solver.cpp:243]     Train net output #0: loss = 0.00836096 (* 1 = 0.00836096 loss)
I0428 21:23:44.403144  7412 sgd_solver.cpp:137] Iteration 51000, lr = 0.001
I0428 21:23:44.411144  7412 sgd_solver.cpp:169] scale layer:0.422820 0.442094 0.495439 0.535352 0.492879 0.464146 0.472486 0.468204 0.466591 0.467836 0.478450 0.461977 0.439175 0.413497 0.403262 0.369694 0.333059 0.297080 0.294505 1.084954 
I0428 21:23:44.412144  7412 sgd_solver.cpp:200] weight diff/data:0.000098 0.000318 0.000176 0.000306 0.000337 0.000235 0.000298 0.000256 0.000360 0.000308 0.000667 0.000247 0.000320 0.000288 0.001350 0.000737 0.000393 0.000546 0.000296 0.000380 0.000032 
I0428 21:24:09.004551  7412 solver.cpp:224] Iteration 51100 (4.06459 iter/s, 24.6027s/100 iters), loss = 0.00781201
I0428 21:24:09.004551  7412 solver.cpp:243]     Train net output #0: loss = 0.0078119 (* 1 = 0.0078119 loss)
I0428 21:24:09.004551  7412 sgd_solver.cpp:137] Iteration 51100, lr = 0.001
I0428 21:24:09.013552  7412 sgd_solver.cpp:169] scale layer:0.422605 0.441790 0.495190 0.535052 0.492673 0.463900 0.472232 0.467988 0.466359 0.467611 0.478232 0.461754 0.438981 0.413291 0.403052 0.369516 0.332840 0.296887 0.294336 1.085168 
I0428 21:24:09.014552  7412 sgd_solver.cpp:200] weight diff/data:0.000154 0.000234 0.000239 0.000196 0.000307 0.000391 0.000256 0.000267 0.000297 0.000411 0.000309 0.000387 0.000296 0.000316 0.000249 0.000208 0.000256 0.000236 0.000210 0.001967 0.000030 
I0428 21:24:32.399889 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:24:33.614959  7412 solver.cpp:224] Iteration 51200 (4.06321 iter/s, 24.6111s/100 iters), loss = 0.0165628
I0428 21:24:33.614959  7412 solver.cpp:243]     Train net output #0: loss = 0.0165626 (* 1 = 0.0165626 loss)
I0428 21:24:33.615959  7412 sgd_solver.cpp:137] Iteration 51200, lr = 0.001
I0428 21:24:33.623960  7412 sgd_solver.cpp:169] scale layer:0.422315 0.441527 0.494942 0.534774 0.492425 0.463677 0.472004 0.467744 0.466157 0.467376 0.477988 0.461506 0.438754 0.413095 0.402843 0.369347 0.332656 0.296698 0.294182 1.085385 
I0428 21:24:33.625959  7412 sgd_solver.cpp:200] weight diff/data:0.000088 0.000226 0.000227 0.000262 0.000364 0.000463 0.000329 0.000264 0.000261 0.000291 0.000367 0.000577 0.000392 0.003726 0.000423 0.000243 0.000311 0.000342 0.000331 0.000278 0.000028 
I0428 21:24:58.192364  7412 solver.cpp:224] Iteration 51300 (4.06876 iter/s, 24.5775s/100 iters), loss = 0.00665513
I0428 21:24:58.192364  7412 solver.cpp:243]     Train net output #0: loss = 0.00665501 (* 1 = 0.00665501 loss)
I0428 21:24:58.192364  7412 sgd_solver.cpp:137] Iteration 51300, lr = 0.001
I0428 21:24:58.201365  7412 sgd_solver.cpp:169] scale layer:0.422257 0.441422 0.494745 0.534559 0.492191 0.463435 0.471776 0.467503 0.465883 0.467142 0.477742 0.461278 0.438540 0.412872 0.402633 0.369140 0.332478 0.296557 0.294025 1.085650 
I0428 21:24:58.203366  7412 sgd_solver.cpp:200] weight diff/data:0.000087 0.000215 0.000220 0.000193 0.000718 0.000151 0.000613 0.000182 0.000311 0.000181 0.000334 0.000182 0.000286 0.000374 0.000273 0.000407 0.000461 0.000235 0.000312 0.001692 0.000023 
I0428 21:25:22.788771  7412 solver.cpp:224] Iteration 51400 (4.06559 iter/s, 24.5967s/100 iters), loss = 0.00890943
I0428 21:25:22.788771  7412 solver.cpp:243]     Train net output #0: loss = 0.00890931 (* 1 = 0.00890931 loss)
I0428 21:25:22.788771  7412 sgd_solver.cpp:137] Iteration 51400, lr = 0.001
I0428 21:25:22.796772  7412 sgd_solver.cpp:169] scale layer:0.422062 0.441258 0.494512 0.534304 0.491945 0.463204 0.471534 0.467251 0.465664 0.466889 0.477493 0.461032 0.438322 0.412641 0.402433 0.368939 0.332270 0.296366 0.293868 1.085877 
I0428 21:25:22.799772  7412 sgd_solver.cpp:200] weight diff/data:0.000097 0.000248 0.001204 0.000186 0.000287 0.000164 0.000196 0.000255 0.000242 0.000185 0.000218 0.000308 0.000273 0.000240 0.001459 0.000219 0.000246 0.000188 0.000378 0.000365 0.000023 
I0428 21:25:47.162165  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_51500.caffemodel
I0428 21:25:47.170166  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_51500.solverstate
I0428 21:25:47.173166  7412 solver.cpp:336] Iteration 51500, Testing net (#0)
I0428 21:25:55.949668 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:25:56.317689  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8297
I0428 21:25:56.317689  7412 solver.cpp:403]     Test net output #1: loss = 0.801286 (* 1 = 0.801286 loss)
I0428 21:25:56.552702  7412 solver.cpp:224] Iteration 51500 (2.96166 iter/s, 33.7648s/100 iters), loss = 0.00797245
I0428 21:25:56.552702  7412 solver.cpp:243]     Train net output #0: loss = 0.00797233 (* 1 = 0.00797233 loss)
I0428 21:25:56.552702  7412 sgd_solver.cpp:137] Iteration 51500, lr = 0.001
I0428 21:25:56.561703  7412 sgd_solver.cpp:169] scale layer:0.421731 0.440997 0.494203 0.534000 0.491818 0.462979 0.471294 0.467022 0.465433 0.466648 0.477276 0.460805 0.438096 0.412434 0.402225 0.368752 0.332040 0.296184 0.293698 1.086087 
I0428 21:25:56.562703  7412 sgd_solver.cpp:200] weight diff/data:0.000074 0.000191 0.000577 0.000306 0.000241 0.000220 0.000212 0.000905 0.000380 0.000268 0.000249 0.000228 0.000242 0.000244 0.000453 0.000200 0.000434 0.000287 0.000499 0.000196 0.000025 
I0428 21:26:19.946041 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:26:21.175112  7412 solver.cpp:224] Iteration 51600 (4.06117 iter/s, 24.6235s/100 iters), loss = 0.0104438
I0428 21:26:21.176111  7412 solver.cpp:243]     Train net output #0: loss = 0.0104437 (* 1 = 0.0104437 loss)
I0428 21:26:21.176111  7412 sgd_solver.cpp:137] Iteration 51600, lr = 0.001
I0428 21:26:21.185111  7412 sgd_solver.cpp:169] scale layer:0.421529 0.440674 0.493987 0.533688 0.491590 0.462749 0.471073 0.466779 0.465196 0.466419 0.477019 0.460533 0.437862 0.412249 0.402039 0.368566 0.331832 0.296008 0.293549 1.086273 
I0428 21:26:21.187111  7412 sgd_solver.cpp:200] weight diff/data:0.000072 0.000159 0.000191 0.000182 0.000223 0.000182 0.000261 0.000330 0.000284 0.000202 0.000298 0.000270 0.000300 0.000234 0.000280 0.000245 0.000452 0.000196 0.000270 0.000284 0.000014 
I0428 21:26:45.764518  7412 solver.cpp:224] Iteration 51700 (4.06684 iter/s, 24.5891s/100 iters), loss = 0.00584164
I0428 21:26:45.764518  7412 solver.cpp:243]     Train net output #0: loss = 0.00584151 (* 1 = 0.00584151 loss)
I0428 21:26:45.764518  7412 sgd_solver.cpp:137] Iteration 51700, lr = 0.001
I0428 21:26:45.773519  7412 sgd_solver.cpp:169] scale layer:0.421516 0.440529 0.493750 0.533487 0.491346 0.462515 0.470830 0.466534 0.464906 0.466196 0.476759 0.460303 0.437635 0.412027 0.401835 0.368364 0.331635 0.295825 0.293387 1.086519 
I0428 21:26:45.776518  7412 sgd_solver.cpp:200] weight diff/data:0.000101 0.000158 0.000220 0.000204 0.001591 0.000274 0.000368 0.000407 0.000443 0.000324 0.000233 0.000409 0.000331 0.000313 0.000285 0.000244 0.000497 0.000283 0.000319 0.000249 0.000021 
I0428 21:27:10.376925  7412 solver.cpp:224] Iteration 51800 (4.06286 iter/s, 24.6132s/100 iters), loss = 0.00760887
I0428 21:27:10.376925  7412 solver.cpp:243]     Train net output #0: loss = 0.00760874 (* 1 = 0.00760874 loss)
I0428 21:27:10.376925  7412 sgd_solver.cpp:137] Iteration 51800, lr = 0.001
I0428 21:27:10.385926  7412 sgd_solver.cpp:169] scale layer:0.421268 0.440383 0.493490 0.533257 0.491038 0.462282 0.470580 0.466307 0.464666 0.465950 0.476523 0.460089 0.437420 0.411815 0.401634 0.368194 0.331471 0.295642 0.293237 1.086746 
I0428 21:27:10.387926  7412 sgd_solver.cpp:200] weight diff/data:0.000098 0.000183 0.000266 0.000199 0.000274 0.000330 0.000260 0.000259 0.003179 0.000247 0.000288 0.000276 0.000337 0.000486 0.000279 0.000267 0.000304 0.000392 0.000439 0.000433 0.000037 
I0428 21:27:35.000334  7412 solver.cpp:224] Iteration 51900 (4.06115 iter/s, 24.6236s/100 iters), loss = 0.00692229
I0428 21:27:35.000334  7412 solver.cpp:243]     Train net output #0: loss = 0.00692216 (* 1 = 0.00692216 loss)
I0428 21:27:35.000334  7412 sgd_solver.cpp:137] Iteration 51900, lr = 0.001
I0428 21:27:35.009335  7412 sgd_solver.cpp:169] scale layer:0.420974 0.440114 0.493287 0.532934 0.490895 0.462028 0.470332 0.466095 0.464433 0.465717 0.476288 0.459863 0.437207 0.411600 0.401430 0.368017 0.331280 0.295457 0.293063 1.086925 
I0428 21:27:35.011334  7412 sgd_solver.cpp:200] weight diff/data:0.000079 0.000141 0.000296 0.000140 0.000220 0.000248 0.000200 0.000341 0.000188 0.000204 0.000229 0.000494 0.000496 0.000348 0.000201 0.000195 0.000282 0.000912 0.000356 0.000142 0.000026 
I0428 21:27:58.396672 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:27:59.378728  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_52000.caffemodel
I0428 21:27:59.386729  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_52000.solverstate
I0428 21:27:59.389729  7412 solver.cpp:336] Iteration 52000, Testing net (#0)
I0428 21:28:08.156230 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:28:08.516250  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8293
I0428 21:28:08.516250  7412 solver.cpp:403]     Test net output #1: loss = 0.807187 (* 1 = 0.807187 loss)
I0428 21:28:08.750264  7412 solver.cpp:224] Iteration 52000 (2.96288 iter/s, 33.751s/100 iters), loss = 0.0122227
I0428 21:28:08.750264  7412 solver.cpp:243]     Train net output #0: loss = 0.0122226 (* 1 = 0.0122226 loss)
I0428 21:28:08.750264  7412 sgd_solver.cpp:137] Iteration 52000, lr = 0.001
I0428 21:28:08.759264  7412 sgd_solver.cpp:169] scale layer:0.420752 0.439789 0.493054 0.532634 0.490680 0.461811 0.470083 0.465846 0.464247 0.465476 0.476075 0.459626 0.436991 0.411402 0.401236 0.367830 0.331093 0.295278 0.292906 1.087091 
I0428 21:28:08.760264  7412 sgd_solver.cpp:200] weight diff/data:0.000114 0.000191 0.000271 0.000184 0.000420 0.000317 0.000448 0.000236 0.000264 0.001787 0.000312 0.000257 0.000260 0.000287 0.000294 0.000315 0.000250 0.000286 0.000534 0.000235 0.000020 
I0428 21:28:33.342670  7412 solver.cpp:224] Iteration 52100 (4.06626 iter/s, 24.5926s/100 iters), loss = 0.00703006
I0428 21:28:33.342670  7412 solver.cpp:243]     Train net output #0: loss = 0.00702993 (* 1 = 0.00702993 loss)
I0428 21:28:33.342670  7412 sgd_solver.cpp:137] Iteration 52100, lr = 0.001
I0428 21:28:33.351671  7412 sgd_solver.cpp:169] scale layer:0.420618 0.439635 0.492821 0.532378 0.490394 0.461584 0.469867 0.465599 0.463994 0.465249 0.475818 0.459397 0.436780 0.411179 0.401035 0.367633 0.330909 0.295131 0.292764 1.087307 
I0428 21:28:33.353672  7412 sgd_solver.cpp:200] weight diff/data:0.000124 0.000752 0.000255 0.000297 0.000407 0.000293 0.000278 0.000358 0.000255 0.000520 0.000265 0.000323 0.000613 0.000424 0.000494 0.000418 0.000291 0.000471 0.000526 0.000321 0.000021 
I0428 21:28:57.947078  7412 solver.cpp:224] Iteration 52200 (4.06417 iter/s, 24.6053s/100 iters), loss = 0.0101943
I0428 21:28:57.947078  7412 solver.cpp:243]     Train net output #0: loss = 0.0101942 (* 1 = 0.0101942 loss)
I0428 21:28:57.947078  7412 sgd_solver.cpp:137] Iteration 52200, lr = 0.001
I0428 21:28:57.956079  7412 sgd_solver.cpp:169] scale layer:0.420363 0.439484 0.492608 0.532133 0.490107 0.461348 0.469617 0.465351 0.463754 0.465005 0.475584 0.459139 0.436548 0.410962 0.400825 0.367434 0.330691 0.294926 0.292604 1.087520 
I0428 21:28:57.958078  7412 sgd_solver.cpp:200] weight diff/data:0.000109 0.000343 0.000226 0.000405 0.000393 0.000647 0.000312 0.000229 0.000307 0.003472 0.000226 0.000363 0.000426 0.000281 0.000280 0.000598 0.000326 0.000419 0.001039 0.000432 0.000026 
I0428 21:29:22.560485  7412 solver.cpp:224] Iteration 52300 (4.06284 iter/s, 24.6133s/100 iters), loss = 0.0064691
I0428 21:29:22.560485  7412 solver.cpp:243]     Train net output #0: loss = 0.00646897 (* 1 = 0.00646897 loss)
I0428 21:29:22.560485  7412 sgd_solver.cpp:137] Iteration 52300, lr = 0.001
I0428 21:29:22.569486  7412 sgd_solver.cpp:169] scale layer:0.420066 0.439188 0.492322 0.531808 0.489874 0.461110 0.469363 0.465122 0.463537 0.464774 0.475358 0.458930 0.436336 0.410755 0.400618 0.367247 0.330470 0.294726 0.292433 1.087692 
I0428 21:29:22.571486  7412 sgd_solver.cpp:200] weight diff/data:0.000105 0.000177 0.000252 0.000174 0.000642 0.000159 0.000225 0.000187 0.000214 0.000194 0.000496 0.000225 0.000199 0.000229 0.000200 0.000177 0.000213 0.000191 0.000239 0.000171 0.000017 
I0428 21:29:45.952823 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:29:47.174893  7412 solver.cpp:224] Iteration 52400 (4.06253 iter/s, 24.6152s/100 iters), loss = 0.00610774
I0428 21:29:47.174893  7412 solver.cpp:243]     Train net output #0: loss = 0.00610762 (* 1 = 0.00610762 loss)
I0428 21:29:47.175894  7412 sgd_solver.cpp:137] Iteration 52400, lr = 0.001
I0428 21:29:47.184895  7412 sgd_solver.cpp:169] scale layer:0.419870 0.438904 0.492066 0.531548 0.489679 0.460887 0.469143 0.464881 0.463319 0.464534 0.475123 0.458682 0.436123 0.410572 0.400426 0.367056 0.330276 0.294542 0.292279 1.087849 
I0428 21:29:47.186894  7412 sgd_solver.cpp:200] weight diff/data:0.000101 0.000153 0.000204 0.000200 0.000393 0.000279 0.000229 0.000187 0.000226 0.000260 0.000252 0.000294 0.000377 0.000272 0.000195 0.000270 0.000279 0.000520 0.000389 0.000192 0.000020 
I0428 21:30:11.530287  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_52500.caffemodel
I0428 21:30:11.539288  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_52500.solverstate
I0428 21:30:11.541287  7412 solver.cpp:336] Iteration 52500, Testing net (#0)
I0428 21:30:20.283787 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:30:20.645808  7412 solver.cpp:403]     Test net output #0: accuracy = 0.8296
I0428 21:30:20.645808  7412 solver.cpp:403]     Test net output #1: loss = 0.80887 (* 1 = 0.80887 loss)
I0428 21:30:20.879822  7412 solver.cpp:224] Iteration 52500 (2.9669 iter/s, 33.7052s/100 iters), loss = 0.00665624
I0428 21:30:20.879822  7412 solver.cpp:243]     Train net output #0: loss = 0.00665611 (* 1 = 0.00665611 loss)
I0428 21:30:20.879822  7412 sgd_solver.cpp:137] Iteration 52500, lr = 0.001
I0428 21:30:20.888823  7412 sgd_solver.cpp:169] scale layer:0.419767 0.438737 0.491838 0.531318 0.489429 0.460649 0.468925 0.464630 0.463068 0.464307 0.474862 0.458461 0.435901 0.410354 0.400213 0.366841 0.330081 0.294395 0.292119 1.088042 
I0428 21:30:20.890822  7412 sgd_solver.cpp:200] weight diff/data:0.000116 0.000183 0.000308 0.000212 0.000376 0.000225 0.000304 0.000417 0.000247 0.000202 0.000667 0.000299 0.000297 0.000302 0.000356 0.000405 0.000276 0.000300 0.000288 0.000293 0.000017 
I0428 21:30:45.484228  7412 solver.cpp:224] Iteration 52600 (4.06426 iter/s, 24.6047s/100 iters), loss = 0.00686048
I0428 21:30:45.484228  7412 solver.cpp:243]     Train net output #0: loss = 0.00686036 (* 1 = 0.00686036 loss)
I0428 21:30:45.484228  7412 sgd_solver.cpp:137] Iteration 52600, lr = 0.001
I0428 21:30:45.493229  7412 sgd_solver.cpp:169] scale layer:0.419582 0.438545 0.491622 0.531097 0.489187 0.460439 0.468696 0.464417 0.462815 0.464077 0.474633 0.458257 0.435666 0.410143 0.400024 0.366633 0.329857 0.294211 0.291960 1.088234 
I0428 21:30:45.495229  7412 sgd_solver.cpp:200] weight diff/data:0.000126 0.000177 0.000144 0.000177 0.000207 0.000183 0.000161 0.000193 0.000190 0.000187 0.000188 0.001483 0.000295 0.000201 0.000201 0.001570 0.000296 0.000293 0.002793 0.000207 0.000022 
I0428 21:31:10.102638  7412 solver.cpp:224] Iteration 52700 (4.0618 iter/s, 24.6196s/100 iters), loss = 0.00696695
I0428 21:31:10.102638  7412 solver.cpp:243]     Train net output #0: loss = 0.00696683 (* 1 = 0.00696683 loss)
I0428 21:31:10.103637  7412 sgd_solver.cpp:137] Iteration 52700, lr = 0.001
I0428 21:31:10.111637  7412 sgd_solver.cpp:169] scale layer:0.419203 0.438242 0.491368 0.530826 0.488972 0.460193 0.468455 0.464205 0.462595 0.463845 0.474393 0.458031 0.435468 0.409938 0.399819 0.366452 0.329665 0.294024 0.291791 1.088388 
I0428 21:31:10.113637  7412 sgd_solver.cpp:200] weight diff/data:0.000084 0.000220 0.000297 0.000226 0.000257 0.000257 0.000332 0.000237 0.000332 0.000162 0.000751 0.000333 0.000631 0.000202 0.000336 0.000296 0.000155 0.000191 0.000228 0.000154 0.000023 
I0428 21:31:33.472973 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:31:34.690043  7412 solver.cpp:224] Iteration 52800 (4.06708 iter/s, 24.5877s/100 iters), loss = 0.00877435
I0428 21:31:34.690043  7412 solver.cpp:243]     Train net output #0: loss = 0.00877423 (* 1 = 0.00877423 loss)
I0428 21:31:34.690043  7412 sgd_solver.cpp:137] Iteration 52800, lr = 0.001
I0428 21:31:34.699043  7412 sgd_solver.cpp:169] scale layer:0.418950 0.437935 0.491118 0.530560 0.488784 0.459957 0.468218 0.463964 0.462385 0.463603 0.474161 0.457792 0.435240 0.409734 0.399634 0.366268 0.329467 0.293858 0.291637 1.088540 
I0428 21:31:34.701043  7412 sgd_solver.cpp:200] weight diff/data:0.000087 0.000122 0.008500 0.021278 0.000318 0.000181 0.000209 0.000201 0.000298 0.000225 0.000240 0.000246 0.000211 0.000261 0.000187 0.000247 0.000289 0.000402 0.000203 0.000175 0.000017 
I0428 21:31:59.292450  7412 solver.cpp:224] Iteration 52900 (4.06465 iter/s, 24.6024s/100 iters), loss = 0.00700569
I0428 21:31:59.292450  7412 solver.cpp:243]     Train net output #0: loss = 0.00700558 (* 1 = 0.00700558 loss)
I0428 21:31:59.292450  7412 sgd_solver.cpp:137] Iteration 52900, lr = 0.001
I0428 21:31:59.301451  7412 sgd_solver.cpp:169] scale layer:0.418794 0.437765 0.490899 0.530353 0.488538 0.459733 0.467990 0.463710 0.462133 0.463381 0.473910 0.457567 0.435017 0.409521 0.399433 0.366046 0.329302 0.293704 0.291484 1.088735 
I0428 21:31:59.303452  7412 sgd_solver.cpp:200] weight diff/data:0.000072 0.000156 0.000850 0.000136 0.000292 0.000251 0.000406 0.000173 0.000191 0.000166 0.000187 0.000221 0.000236 0.000219 0.000261 0.000196 0.000196 0.000774 0.000327 0.000318 0.000021 
I0428 21:32:23.678845  7412 solver.cpp:453] Snapshotting to binary proto file D:/_test/train/caffenet_train/_iter_53000.caffemodel
I0428 21:32:23.687845  7412 sgd_solver.cpp:361] Snapshotting solver state to binary proto file D:/_test/train/caffenet_train/_iter_53000.solverstate
I0428 21:32:23.690846  7412 solver.cpp:336] Iteration 53000, Testing net (#0)
I0428 21:32:32.469348 10348 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:32:32.832368  7412 solver.cpp:403]     Test net output #0: accuracy = 0.83
I0428 21:32:32.832368  7412 solver.cpp:403]     Test net output #1: loss = 0.812854 (* 1 = 0.812854 loss)
I0428 21:32:33.067382  7412 solver.cpp:224] Iteration 53000 (2.96068 iter/s, 33.776s/100 iters), loss = 0.00620486
I0428 21:32:33.067382  7412 solver.cpp:243]     Train net output #0: loss = 0.00620475 (* 1 = 0.00620475 loss)
I0428 21:32:33.067382  7412 sgd_solver.cpp:137] Iteration 53000, lr = 0.001
I0428 21:32:33.076382  7412 sgd_solver.cpp:169] scale layer:0.418623 0.437557 0.490668 0.530102 0.488254 0.459493 0.467762 0.463461 0.461879 0.463149 0.473660 0.457331 0.434805 0.409308 0.399223 0.365848 0.329126 0.293501 0.291324 1.088886 
I0428 21:32:33.077383  7412 sgd_solver.cpp:200] weight diff/data:0.000116 0.000180 0.000258 0.000207 0.000459 0.001707 0.000511 0.000395 0.000451 0.000241 0.000314 0.002461 0.000308 0.003728 0.000275 0.000715 0.000241 0.000276 0.000390 0.000474 0.000027 
I0428 21:32:57.698791  7412 solver.cpp:224] Iteration 53100 (4.05986 iter/s, 24.6314s/100 iters), loss = 0.00525665
I0428 21:32:57.698791  7412 solver.cpp:243]     Train net output #0: loss = 0.00525654 (* 1 = 0.00525654 loss)
I0428 21:32:57.698791  7412 sgd_solver.cpp:137] Iteration 53100, lr = 0.001
I0428 21:32:57.706791  7412 sgd_solver.cpp:169] scale layer:0.418333 0.437363 0.490413 0.529841 0.488123 0.459260 0.467544 0.463248 0.461656 0.462928 0.473429 0.457120 0.434590 0.409107 0.399018 0.365658 0.328929 0.293326 0.291160 1.089032 
I0428 21:32:57.708791  7412 sgd_solver.cpp:200] weight diff/data:0.000088 0.000141 0.000372 0.000160 0.000158 0.001459 0.000130 0.000441 0.000279 0.000159 0.000240 0.000561 0.000217 0.000634 0.000208 0.000185 0.000152 0.000201 0.000172 0.000158 0.000016 
I0428 21:33:21.206135 13608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 21:33:22.430205  7412 solver.cpp:224] Iteration 53200 (4.04324 iter/s, 24.7327s/100 iters), loss = 0.00589797
I0428 21:33:22.431205  7412 solver.cpp:243]     Train net output #0: loss = 0.00589786 (* 1 = 0.00589786 loss)
I0428 21:33:22.431205  7412 sgd_solver.cpp:137] Iteration 53200, lr = 0.001
I0428 21:33:22.440207  7412 sgd_solver.cpp:169] scale layer:0.418156 0.437011 0.490164 0.529536 0.487899 0.459024 0.467302 0.463010 0.461455 0.462688 0.473183 0.456876 0.434352 0.408909 0.398826 0.365481 0.328750 0.293163 0.291002 1.089120 
I0428 21:33:22.442206  7412 sgd_solver.cpp:200] weight diff/data:0.000066 0.000114 0.000606 0.000173 0.000245 0.000535 0.000245 0.000282 0.000208 0.000227 0.000804 0.000272 0.000209 0.000149 0.000304 0.000937 0.000163 0.001199 0.000279 0.000198 0.000020 
I0428 21:33:47.104617  7412 solver.cpp:224] Iteration 53300 (4.05295 iter/s, 24.6734s/100 iters), loss = 0.00569226
